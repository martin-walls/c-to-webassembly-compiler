\documentclass[00-main.tex]{subfiles}

\begin{document}

\chapter{Evaluation}

\begin{Comment}
Word budget: \textasciitilde 2000--2400 words
\end{Comment}

\begin{Comment}
"Signs of success, evidence of thorough and systematic evaluation"

- How many of the original goals were achieved?

- Were they proved to have been achieved?

- Did the program really work?

Answer questions posed in the introduction

use appropriate techniques for evaluation, eg. confidence intervals
\end{Comment}

\begin{Comment}
Talk about how I wrote my test script to automatically compare with GCC.

Talk about the test programs I used.
\end{Comment}


\section{Success Criteria}

The success criteria for my project, as defined in my project proposal, are:
\begin{itemize}
\item The program generates an abstract syntax tree from C source code.
\item The program transforms the abstract syntax tree into an intermediate representation.
\item The program uses the Relooper algorithm to transform unstructured to structured control
flow.
\item The program generates WebAssembly binary code from the intermediate representation.
\item The compiler generates binary code that produces the same output as the source program.
\end{itemize}

All of my success criteria have been met.
The first four criteria correspond to the main stages of the compiler pipeline, respectively.
The correctness of this pipeline is verified by the correctness of the generated binary code.
The generated binary would not be correct if any of the stages had not been successful.

I used a variety of test programs to verify the success of the fifth criteria. This is described in \secref{sec:eval:testing}.


\section{Testing} \label{sec:eval:testing}

I wrote a suite of test programs in C to evaluate the correctness and performance of my compiler.
There were two types of program:
\begin{itemize}
\item `Unit test' programs, which test a specific construct in the C language;
\item Full programs, which represent real workloads that the compiler would be used for.
\end{itemize}

The first type of test program are not strictly unit tests in the standard sense of the word. However, since they verify the functionality of the compiler on individual parts of the C language, they fulfil a very similar purpose.

Examples of `full programs' include Conway's Game of Life \cite{conways-game-of-life}, calculating the Fibonacci numbers (recursively), and finding occurrences of a substring in a string.

For all my tests, I used GCC\footnote{GCC version 11.3.1} as the reference for correctness.
I deemed a program to be correct if it produced the same output as when compiled with GCC.
To facilitate this, I made liberal use of \texttt{printf}, to output the results of computations.

I wrote a test runner script to ensure that I maintained correctness as I continued to develop the compiler, fix bugs, and implement optimisations.
This script read a directory of \texttt{.yaml} files, which described the path to each test program, and the arguments to run it with.
It then compiled the program with both my compiler and with GCC, and compared the outputs.
A test passed if the outputs were identical, and failed otherwise.
The script reported which tests, if any, failed.

I also implemented some convenience features into the test script.
The command-line interface takes an optional `filter' argument, which can be used to run a subset of the tests whose name matches the filter.
The script can also be used to run one of the test programs without comparing to GCC, printing to the standard output. This allows easier manual testing.

To prevent bugs from accidentally creeping in to my compiler, I set up the test suite to run automatically as a commit hook. This would prevent a commit from succeeding if any of the tests failed, allowing me to make corrections first.
This ensured that the version of my project in source control was always correct and functioning.




\section{Performance Impacts of Optimisations}

\subsection{Unreachable Procedure Elimination}


In the context of this project, unreachable procedure elimination mainly has benefits in removing unused standard library functions from the compiled binary.
When a standard library header is included in the program, the preprocessing stage inserts the entire code of that library\footnote{That is, the entirety of my skeleton implementation of that library, rather than the actual standard library code.}.
If the program only uses one or two of the functions, most of them will be dead code.
Unreachable procedure elimination is able to safely remove these.

I only implemented enough of the standard library to allow my test programs to run, so the impact of this optimisation is limited by the number of functions imported.
If I were to implement more of the standard library, this optimisation would become more important.

The standard library header with the most functions that I implemented was \Filename{ctype.h}.
This header contains 13 functions, of which a program might normally use two or three.
This is where I saw the biggest improvement from the optimisation.
Programs that used the \CInlineCode{ctype} library saw an average file-size reduction of \SI{4.7}{\kilo\byte}.

The other standard library headers I implemented only contained a few functions, so the impact of this optimisation was much more limited. However, as mentioned above, if I implemented more of the standard library, I would see much more of an improvement.

\subsection{Stack Usage Profiling}

The other two optimisations I implemented both reduced the amount of stack memory used by programs.
To evaluate the effectiveness of the optimisations, I inserted profiling code when compiling the programs, to measure the size of the stack throughout program execution.

When generating instructions that move the stack pointer, the compiler additionally inserts a call to \JSInlineCode{log_stack_ptr}, which is a function imported from the JavaScript runtime.
\JSInlineCode{log_stack_ptr} reads the current stack pointer value from memory and appends it to a log file.

To visualise the resulting data, I wrote a Python script to plot the stack usage log.
All the figures below are generated using this profiling method.



\newcommand{\IncludeStackPlot}[1]{\fbox{\resizebox{0.98\textwidth}{!}{\input{21-stack-profile-plots/#1}}}}

\subsection{Tail-Call Optimisation}

My implementation of tail-call optimisation\index{Tail-call optimisation} was successful in reusing the existing function stack frame for tail-recursive calls. The recursion is converted into iteration within the function, eliminating the need for new stack frame allocations.
Therefore, the stack memory usage remains constant rather than growing linearly with the number of recursive calls.

One of the functions I used to evaluate this optimisation was the function in \lstref{lst:tail-recursive sum} below, that uses tail-recursion to compute the sum of the first $n$ integers.

\begin{listing}[ht]
  \CInputListing[firstline=4, lastline=9]{../02-test-programs/15-tail-call-optimisation/02-tail-call-optimised.c}
  \caption{Tail-recursive function to sum the integers 1 to $n$}
  \label{lst:tail-recursive sum}
\end{listing}


\figref{fig:plot:tail-call optimisation stack use} compares the stack memory usage with tail-call optimisation disabled and enabled.
Without the optimisation, the stack size grows linearly with $n$.
When running the program with $n=500$, a stack size of \SI{46.3}{\kilo\byte} is reached.
When the same program is compiled with tail-call optimisation enabled, only 298 bytes of stack space are used.
This is a \SI{99.36}{\percent} reduction in memory usage.
Of course, the reduction depends on how many iterations of the function are run.
In the non-optimised case, the stack usage is \bigo{n} in the number of iterations, whereas in the optimised case it is \bigo{1}.

\begin{figure}[ht]
  \centering
  \IncludeStackPlot{22-tailcall-sum-compare-tailcallopt-without-stackopt.pgf}
  \caption{Stack usage for calling \texttt{sum(500, 0)} (see \lstref{lst:tail-recursive sum})}
  \label{fig:plot:tail-call optimisation stack use}
\end{figure}

When testing with large $n$, the non-optimised version ran out of memory space, and failed to complete.
In contrast, the optimised version has no memory constraint on how many iterations can be run.
It successfully runs \num{1000000} levels of recursion without needing any more memory; even GCC fails to run that many.


\subsection{Stack Allocation Policy}

The stack allocation policy that I implemented was successful in reducing the amount of stack memory used.

From my test programs, the highest gain was xxx and the average gain was xxx.
The amount of different made depended on how many temp vars there were, and how much they clashed with each other.

\begin{figure}[ht]
  \centering
  \IncludeStackPlot{01-case-compare.pgf}
  \caption{Stack profile}
  \label{fig:}
\end{figure}

\section{Summary}

\end{document}
