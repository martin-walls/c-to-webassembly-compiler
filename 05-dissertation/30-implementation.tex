\documentclass[00-main.tex]{subfiles}

\begin{document}

\chapter{Implementation}

\begin{mrwComment}
%TC:ignore
Word budget: \textasciitilde 4500--5400 words
%TC:endignore
\end{mrwComment}

\begin{mrwComment}
%TC:ignore
Describe what was actually produced.

Describe any design strategies that looked ahead to the testing phase, to demonstrate professional approach
%TC:endignore
\end{mrwComment}
\begin{mrwComment}
%TC:ignore
Describe high-level structure of codebase.

Say that I wrote it from scratch.

-> mention LALRPOP parser generator used for .lalrpop files
%TC:endignore
\end{mrwComment}

\begin{mrwComment}
Add some intro text here between chapter heading and section title.
\end{mrwComment}

\section{Repository Overview}

I developed my project in a GitHub repository, ensuring to regularly push to the cloud for backup purposes.
This repository is a monorepo containing both my research and documentation along with my source code.

The high-level structure of the codebase is shown below.
All the code for the compiler is in the \Dirname{src} directory.
The other directories contain the runtime environment code, skeleton standard library implementation, and tests and other tools.

\newlength\IndentWidth
\setlength\IndentWidth{1em}
\NewDocumentCommand{\Indent}{m}{\hspace{#1\IndentWidth}}
\definecolor{TableHlineColour}{HTML}{ababab}

\begin{xltabular}{\textwidth}{lX}
\arrayrulecolor{TableHlineColour}
\hline
\Dirname{src} & Compiler source code. \\\hline
\Indent{1}\Dirname{program_config} & Compiler constants and runtime options data structures. \\\hline
\Indent{1}\Dirname{front_end} & Lexer, parser grammar, \gls{ast} data structure. \\\hline
\Indent{1}\Dirname{middle_end} & \gls{ir} data structures, definition of intermediate instructions. Converting \gls{ast} to \gls{ir}. \\\hline
\Indent{2}\Dirname{middle_end_optimiser} & Tail-call optimisation and unreachable procedure elimination. \\\hline
\Indent{1}\Dirname{relooper} & Relooper algorithm. \\\hline
\Indent{1}\Dirname{back_end} & Target code generation stage. \\\hline
\Indent{2}\Dirname{wasm_module} & Data structures to represent a WebAssembly module. \\\hline
\Indent{2}\Dirname{dataflow_analysis} & Flowgraph generation, dead code analysis, live variable analysis, clash graph. \\\hline
\Indent{2}\Dirname{stack_allocation} & Different stack allocation policies. \\\hline
\Indent{1}\Dirname{data_structures} & Interval tree implementation that I ended up not using. \\\hline
\Indent{1}\Filename{preprocessor.rs} & C preprocessor. \\\hline
\Indent{1}\Filename{id.rs} & Trait for generating IDs used across the compiler. \\\hline
\Indent{1}\Filename{lib.rs} & Contains the main \texttt{run} function. \\\hline
\Dirname{runtime} & NodeJS runtime environment. \\\hline
\Dirname{headers} & Header files for the parts of the standard library I implemented. \\\hline
\Dirname{tools} &  \\\hline
\Indent{1}\Filename{profiler.py} & Plot stack usage profiles. \\\hline
\Indent{1}\Filename{testsuite.py} & Test runner script. \\\hline
\Dirname{tests} & Automated test specifications. \\\hline
\end{xltabular}


\section{System Architecture}

\Ccref{fig:project flowchart} describes the high-level structure of the project. The \textcolor{frontendcolor}{front end}, \textcolor{middleendcolor}{middle end}, and \textcolor{backendcolor}{back end} are denoted by colour.

\begin{figure}[!ht]
  \centering
  \scalebox{0.8}{\tikzfig{70-figures/01-overview-flowchart}}
  \caption{Project structure, highlighting the \textcolor{frontendcolor}{front end}, \textcolor{middleendcolor}{middle end}, and \textcolor{backendcolor}{back end}.}
  \label{fig:project flowchart}
\end{figure}

Each solid box represents a module of the project, transforming the input data representation into the output representation.
The data representations are shown as dashed boxes.

I created my own \gls{ast} representation and \gls{ir}, which are used as the main data representations in the compiler.

% The optimisations I implemented in the middle end were unreachable procedure elimination, and tail-call optimisation.
% As an extension to the project, I implemented a more optimal stack allocation policy, as part of the target code generation module.

\section{Front End}

% \begin{figure}[H]
%   \centering
%   \tikzfig{70-figures/02-front-end-overview-flowchart}
%   \caption{Structure of the compiler front end.}
%   \label{fig:front end flowchart}
% \end{figure}

The front end of the compiler consists of the lexer and the parser; it takes C source code as input and outputs an abstract syntax tree.
I wrote a custom lexer, because this is necessary to support \CInline{typedef} definitions in C\@.
I used the LALRPOP parser generator \ccite{lalrpop-docs} to convert the tokens emitted by the lexer into an \gls{ast}\@.

\subsection{Preprocessor}

I used the GNU C preprocessor (\mintinline{bash}{cpp}) \ccite{gnu-c-preprocessor} to handle any preprocessor directives in the source code, for example macro definitions.
However, since I do not support linking, I removed any \CInline{#include} directives before running the preprocessor and handled them myself.

For each \CInline{#include <name.h>} directive that is removed, if it is one of the standard library headers that I support, the appropriate library code is copied into the source code from \Filename{headers/<name>.h}.
One exception is when the name of the header file matches the name of the source program, in which case the contents of the program's header file are inserted into the source code, rather than finding a matching library.

After processing \CInline{#include} directives, the compiler spawns \mintinline{bash}{cpp} as a child process, writes the source code to its stdin, and reads the processed source code back from its stdout.

\subsection{Lexer}

The grammar of the C language is mostly context-free, however the presence of \CInline{typedef} definitions makes it context-sensitive \ccite[Section 5.10.3]{c-reference-manual}.
For example, the statement \CInline{foo (bar);} can be interpreted in two ways:
\begin{itemize}
\item As a variable declaration, if \CInline{foo} has previously been defined as a type name\footnote{The brackets will be ignored.}; or
\item As a function call, if \CInline{foo} is the name of a function.
\end{itemize}

This ambiguity is impossible to resolve with the language grammar. The solution is to preprocess the \CInline{typedef} names during the lexing stage, and emit distinct type name and identifier tokens to the parser.
Therefore, I implemented a custom lexer that is aware of the type names that have already been defined at the current point in the program.

The lexer is implemented as a finite state machine. \Ccref{fig:lexing numbers fsm,fig:lexing identifiers fsm} highlight portions of the machine; the remaining state transition diagrams can be found in \ccref{app:lexer fsm}.
The diagrams show the input character as a regular expression along each transition arrow. (Note: in a slight abuse of regular expression notation, the dot character `\texttt{.}' represents a literal full stop character and the backslash character `\texttt{\char`\\}' represents a literal backslash.)
It is assumed that when no state transition is shown for a particular input, the end of the current token has been reached.
Transition arrows without a prior state are the initial transitions for the first input character.
Node labels represent the token that will be emitted when we finish in that state.

The finite state machine consumes the source code one character at a time, until the end of the token is reached (i.e.\ there is no transition for the next input character).
Then, the token corresponding to the current state is emitted to the parser.
Some states have no corresponding token to emit because they occur part-way through parsing a token; if the machine finishes in one of these states, this raises a lex error.
(In other words, every state labelled with a token is an accepting state of the machine.)
For tokens such as number literals and identifiers, the lexer appends the input character to a string buffer on each transition and when the token is complete, the string is stored inside the emitted token.
This gives the parser access to the necessary data about the token, for example the name of the literal.

If, when starting to lex a new token, there is no initial transition corresponding to the input character, then there is no valid token for this input. This raises a lex error and the compiler will exit.

\Ccref{fig:lexing numbers fsm} shows the finite state machine for lexing number literals. This handles all the different forms of number that C supports: decimal, binary, octal, hexadecimal, and floating point.
(Note: the states leading to the ellipsis token are shown for completeness, even though the token is not a number literal, since they share the starting dot state.)

\begin{figure}[!htb]
  \centering
  \tikzfig{70-figures/10-lexer-fsm/00-numbers}
  \caption{Finite state machine for lexing number literals.}
  \label{fig:lexing numbers fsm}
\end{figure}

\Ccref{fig:lexing identifiers fsm} shows the finite state machine for lexing identifiers and \CInline{typedef} names.
This is where we handle the ambiguity introduced into the language.
  Every time we consume another character of an identifier, we check whether the current name (which we have stored in the string buffer) matches either a keyword of the language or a \CInline{typedef} name we have encountered so far.
(Keywords are given a higher priority of matching.)
  If a match is found, we move to the corresponding state, represented by the $\epsilon$ transitions (since no input is consumed along these transitions).
When we reach the end of the token, the three states will emit the corresponding token, either an identifier, keyword, or \CInline{typedef} name token respectively.
The lexer stores the \CInline{typedef} names that have been declared so far so that it can emit the correct type of token for future names.

\begin{figure}[!htb]
  \centering
  \tikzfig{70-figures/10-lexer-fsm/01-identifiers}
  \caption{Finite state machine for lexing identifiers.}
  \label{fig:lexing identifiers fsm}
\end{figure}


\subsection{Parser}

I used the LALRPOP parser generator \ccite{lalrpop-docs} to generate parsing code from the input grammar I wrote.
It generates an \gls{ast} that I defined the structure of.
Microsoft's C Language Syntax Summary \ccite{c-language-grammar-microsoft} and C: A Reference Manual \ccite{c-reference-manual} were very useful references to ensure I captured the subtleties of C's syntax when writing my grammar.
My grammar is able to parse all of the core features of the C language, omitting some of the recent language additions. I chose to make my parser handle a larger subset of C than the compiler as a whole supports; the middle end rejects or ignores nodes of the \gls{ast} that it doesn't handle. For example, the parser handles storage class specifiers (e.g.\ \CInline{static}) and type qualifiers (e.g.\ \CInline{const}), and the middle end simply ignores them.

\subsubsection{Dangling Else Ambiguity}

A naive grammar for C (\ccref{lst:ambiguous if-else grammar}) contains an ambiguity around \CInline{if}/\CInline{else} statements \ccite[Section 8.5.2]{c-reference-manual}.
C permits the bodies of conditional statements to be written without curly brackets if the body is a single statement.
If we have nested \CInline{if}/\CInline{else} statements that do not use curly brackets, it can be ambiguous which \CInline{if} an \CInline{else} belongs to. This is known as the dangling else problem \ccite{dangling-else-wiki}.

\begin{listing}[!ht]
  \begin{GrammarListing}
    if-stmt      ::= "if" "(" expr ")" stmt
    if-else-stmt ::= "if" "(" expr ")" stmt "else" stmt
  \end{GrammarListing}
  \caption{Ambigious \CInline{if}/\CInline{else} grammar.}
  \label{lst:ambiguous if-else grammar}
\end{listing}

An example of the dangling else problem is shown in \ccref{lst:dangling else}.
According to the grammar in \ccref{lst:ambiguous if-else grammar}, there are two possible parses of this program.
Either the \CInline{else} belongs to the inner or the outer \CInline{if} (\ccref{lst:dangling else possible parsings}).

\begin{listing}[!ht]
  \begin{CListing}
    if (x)
      if (y)
        stmt1;
    else
      stmt2;
  \end{CListing}
  \caption{Example of the dangling else problem.}
  \label{lst:dangling else}
\end{listing}

\begin{listing}[!ht]
  \begin{mrwComment}
  Make this an AST diagram instead?
  \end{mrwComment}
  \begin{subfigure}[t]{0.49\textwidth}
    \begin{CListing}
      if (x) {
        if (y) {
          stmt1;
        } else {
          stmt2;
        }
      }
    \end{CListing}
    \caption{\CInline{else} belongs to inner \CInline{if}.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\textwidth}
    \begin{CListing}
      if (x) {
        if (y) {
          stmt1;
        }
      } else {
        stmt2;
      }
    \end{CListing}
    \caption{\CInline{else} belongs to outer \CInline{if}.}
  \end{subfigure}
  \caption{Possible parsings of \ccref{lst:dangling else}.}
  \label{lst:dangling else possible parsings}
\end{listing}

C resolves the ambiguity by always associating the \CInline{else} with the closest possible \CInline{if}.
We can encode this into the grammar with the concept of `open' and `closed' statements \ccite{final-solution-to-dangling-else}.
\Ccref{lst:open/closed statement grammar} shows how I introduce this into my grammar for \CInline{if}/\CInline{else} statements.
All other forms of statement must also be converted to the new structure.
Any basic statements, i.e.\ statements that have no sub-statements, are added to \GrammarInline{closed-stmt}.
All other statements that have sub-statements, such as \CInline{while}, \CInline{for}, and \CInline{switch} statements, must be duplicated to both \GrammarInline{open-stmt} and \GrammarInline{closed-stmt}.


\begin{listing}[!ht]
  \begin{GrammarListing}
    stmt        ::= open-stmt | closed-stmt

    open-stmt   ::= "if" "(" expr ")" stmt
                  | "if" "(" expr ")" closed-stmt "else" open-stmt
                  | ...

    closed-stmt ::= "if" "(" expr ")" closed-stmt "else" closed-stmt
                  | ...
  \end{GrammarListing}
  \caption{Using open and closed statements to solve the dangling else problem}
  \label{lst:open/closed statement grammar}
\end{listing}

A closed statement always has the same number of \CInline{if} and \CInline{else} keywords (excluding anything between a pair of brackets, because bracket matching is unambiguous).
Thus, in the second alternative of an \GrammarInline{open-stmt}, the \CInline{else} terminal can be found by counting the number of \CInline{if}s and \CInline{else}s we encounter since the start of the \GrammarInline{closed-stmt}; the \CInline{else} belonging to the \GrammarInline{open-stmt} is the first \CInline{else} once we have more \CInline{else}s than \CInline{if}s.

If we allowed open statements inside the \CInline{if} clause of an \GrammarInline{open-stmt}, then \GrammarInline{open-stmt} and \GrammarInline{closed-stmt} would no longer be disjoint, and the grammar would be ambiguous. This is because we wouldn't be able to use the above method for finding the \CInline{else} that belongs to the outer \GrammarInline{open-stmt}.

\subsubsection{LALRPOP Parser Generator}

I chose the LALRPOP parser generator because it builds up the \gls{ast} as it parses the grammar.
This is in contrast to some of the other available libraries, which separate the grammar code and the code that generates the \gls{ast}\@.
LALRPOP provides an intuitive and powerful approach.
Each grammar rule contains both the grammar specification and code to generate the corresponding \gls{ast} node.

\Ccref{lst:AST generation code example} is an example of the LALRPOP syntax for addition expressions.
The left-hand side of the~\RustInline{=>} describes the grammar rule, and the right-hand side is the code to generate an \RustInline{Expression} node.
Terminals are represented in double quotes; these are defined to map to the tokens emitted by the lexer.
Non-terminals are represented inside angle brackets, with their type and a variable name to use in the \gls{ast} generation code.

\begin{listing}[!ht]
  \begin{subfigure}[t]{\textwidth}
    \begin{GrammarListing}
      additive-expression ::= additive-expression "+" multiplicative-expression
    \end{GrammarListing}
    \caption{The grammar rule for addition expressions.}
  \end{subfigure}
  \par\medskip % vertical space between subfigures
  \begin{subfigure}[t]{\textwidth}
    \begin{RustListing}
      AdditiveExpression: ast::Expression = {
          <e1:AdditiveExpression> "+" <e2:MultiplicativeExpression>
              => ast::Expression::BinaryOp(
                  ast::BinaryOperator::Add,
                  Box::new(e1),
                  Box::new(e2)
              ),
          ...
      };
    \end{RustListing}
    \caption{The LALRPOP syntax for the addition grammar rule.}
  \end{subfigure}
  \caption{In LALRPOP, the \gls{ast} generation and grammar code are combined.}
  \label{lst:AST generation code example}
\end{listing}

LALRPOP also allows macros to be defined, which allow the grammar to be written in a more intuitive way.
For example, I defined a macro to represent a comma-separated list of non-terminals (\ccref{lst:parser macro for comma-separated list}).
The macro has a generic type \RustInline{T}, and automatically collects the list items into a \RustInline{Vec<T>}, which can be used by the rules that use the macro.

\begin{listing}[!ht]
  \begin{RustListing}
    CommaSepList<T>: Vec<T> = {
        <mut v:(<T> ",")*> <e:T> => {
            v.push(e);
            v
        }
    };
  \end{RustListing}
  \caption{LALRPOP macro to parse a comma-separated list of non-terminals.}
  \label{lst:parser macro for comma-separated list}
\end{listing}

\subsubsection{String Escape Sequences}

The parser had to handle escape sequences in strings.
I implemented this by first creating an iterator over the characters of a string, which replaces escape sequences by the character they represent as it emits each character.
When the current character is a backslash, instead of emitting it straight away, the iterator consumes the next character and emits the character corresponding to the escape sequence.
I wrapped this in an \RustInline{interpret_string} function that internally creates an instance of the iterator and collects the emitted characters back to a string.

\subsubsection{Parsing Type Specifiers}

Another feature of the C language is that type specifiers (\CInline{int}, \CInline{signed}, etc.) can appear in any order before a declaration.
For example, \CInline{signed int x} and \CInline{int signed x} are equivalent declarations.
To handle this, my parser first consumes all type specifier tokens of a declaration, then constructs an \RustInline{ArithmeticType} \gls{ast} node from them.
It uses a bitfield where each bit represents the presence of one of the type specifiers in the type.
The bitfield is the normalised representation of a type; every possible declaration that is equivalent to a type will have the same bitfield.
The declarations above would construct the bitfield \RustInline{0b00010100}, where the two bits set represent \CInline{signed} and \CInline{int} respectively.
For each type specifier, the corresponding bit is set.
Then, the bitfield is matched against the possible valid types, to assign the type to the \gls{ast} node.


\section{Middle End}

\begin{mrwComment}
%TC:ignore
Give an overview of the middle end
%TC:endignore
\end{mrwComment}

\subsection{Intermediate Code Generation}

I defined a custom three-address code \gls{ir} (the instructions are listed in \ccref{app:intermediate code}).
The \gls{ir} contains both the program instructions and necessary metadata, such as variable type information, the mapping of variable and function names to their IDs, etc.
The instructions and metadata are contained in separate sub-structs within the main \gls{ir} struct, which enables the metadata to be carried forwards through stages of the compiler pipeline while the instructions are transformed at each stage.
In the stage of converting the \gls{ast} to \gls{ir} code, the instructions and metadata are encapsulated in the \RustInline{Program} struct.

Many objects in the \gls{ir} require unique IDs, such as variables and labels.
I created a \RustInline{Id} trait to abstract this concept, together with a generic \RustInline{IdGenerator} struct (\ccref{lst:Id and IdGenerator implementation}).
The ID generator internally tracks the highest ID that has been generated so far, so that the \gls{ir} can create as many IDs as necessary without needing to know anything about their implementation.
IDs are generated inductively: each ID knows how to generate the next one.

\begin{listing}[!ht]
  \begin{RustListing}
    pub trait Id {
        fn initial_id() -> Self;
        fn next_id(&self) -> Self;
    }

    pub struct IdGenerator<T: Id + Clone> {
        max_id: Option<T>,
    }

    impl<T: Id + Clone> IdGenerator<T> {
        pub fn new() -> Self {
            IdGenerator { max_id: None }
        }

        pub fn new_id(&mut self) -> T {
            let new_id = match &self.max_id {
                None => T::initial_id(),
                Some(id) => id.next_id(),
            };
            self.max_id = Some(new_id.to_owned());
            new_id
        }
    }
  \end{RustListing}
  \caption{Implementation of the \RustInline{Id} trait and \RustInline{IdGenerator}.}
  \label{lst:Id and IdGenerator implementation}
\end{listing}


To convert the \gls{ast} to \gls{ir} code, the compiler recursively traverses the tree, generating three-address code instructions and metadata as it does so.
At the highest level, the \gls{ast} contains a list of statements.
For each of these, the compiler calls \RustInline{convert_statement_to_ir(stmt, program, context)}.
\RustInline{program} is the mutable intermediate representation, to which instructions and metadata are added as the \gls{ast} is traversed.
\RustInline{context} is the context object described in \ccref{sec:impl:context object design pattern}, which passes relevant contextual information through the functions recursively.

The core of converting a statement or expressions to \gls{ir} code is pattern matching the \gls{ast} node, and generating \gls{ir} instructions according to its structure, recursing into sub-statements and expressions.
The case for a \CInline{while} statement is shown in \ccref{lst:convert while stmt to IR pseudocode}; the labels and branches to execute a while loop are added, and the instructions to evaluate the condition and body are generated recursively.
Other control-flow structures are similar.

\begin{listing}[!ht]
  \begin{PseudocodeListing}
    \Function{ConvertStatementToIR}{\textit{stmt}, \textit{program}, \textit{context}}
      \State \textit{instrs} $\gets []$
      \Match {\textit{stmt}}
        \MatchCase {While(\textit{condition}, \textit{body})}
          \State \textit{startLabel}, \textit{endLabel} $\gets$ create new labels for start and end of loop
          \State Push new loop context to Context object
          \State \textit{instrs} += \texttt{label <\textrm{\textit{startLabel}}>}
          \State \textit{instrs} += \Call{ConvertExpressionToIR}{\textit{condition}, \textit{program}, \textit{context}}
          \State \textit{instrs} += \texttt{branch <\textrm{\textit{endLabel}}> if \textrm{\textit{condition}} == 0}
          \State \textit{instrs} += \Call{ConvertStatementToIR}{\textit{body}, \textit{program}, \textit{context}}
          \State \textit{instrs} += \texttt{branch <\textrm{\textit{startLabel}}>}
          \State \textit{instrs} += \texttt{label <\textrm{\textit{endLabel}}>}
          \State Pop loop context from Context object
        \EndMatchCase
        \State \ldots\ other AST statement nodes \ldots
      \EndMatch
      \State \Return \textit{instrs}
    \EndFunction
  \end{PseudocodeListing}
  \caption{Pseudocode for the \RustInline{convert_statement_to_ir()} function.}
  \label{lst:convert while stmt to IR pseudocode}
\end{listing}

Some statements took a little more care to ensure the semantic meaning of the program is preserved.
For example, \CInline{switch} statements can contain \CInline{case} blocks in any order.
Some blocks may fall-through to the next block, and there may be a \CInline{default} block.
I handled this by first generating instructions for each case block, and storing them in a switch context until all blocks have been seen.
We then put conditional branches to each case block at the start of the switch statement, followed by each of the block instructions.
By doing this, there is no direct fall-through between any blocks; instead, a block that falls through to the next block will end in a branch instruction to the start of that block.
This also allows \CInline{default} blocks to be easily handled; we just add an unconditional branch to the default block after all the conditional branches.
If there is no \CInline{default} block, the conditional branches are followed by an unconditional branch to the end of the \CInline{switch} statement.
\Ccref{fig:instr structure for switch statements} shows the structure of the generated instructions for \CInline{switch} statements.

\begin{figure}[ht]
  \centering
  \scalebox{0.8}{\tikzfig{70-figures/05-switch-case-handling}}
  \caption{\gls{ir} instructions generated for \CInline{switch} statements.}
  \label{fig:instr structure for switch statements}
\end{figure}

\begin{mrwComment}
TODO talk about the more complex cases, eg. switch statements, variable declarations, function declarations
\end{mrwComment}

Declaration statements have a lot of different cases, each of which I had to handle separately.
For example, variables may be declared without being initialised with a value.

Function declarations only differ syntactically from other declarations by the type of the identifier; so we have to handle them in the same place.
For function definitions (i.e.\ declarations plus body), we transform the body code and add a new function to the \gls{ir}.

Arrays are complicated because there are multiple ways their length can be specified.
It can be given explicitly in the declaration, or implicitly inferred from the length of the initialiser.
To further complicate them, an explicit size can either be a static value or a variable that is only known at runtime (creating a variable-length array).
To handle variable length arrays, an instruction is inserted to allocate space on the stack for it at runtime.
Array and struct initialisers are handled by first allocating memory for the variable, then storing the value of each of the inner members.


Some expressions can be evaluated by the compiler ahead-of-time, for example array length expressions.
I implemented a compile-time expression evaluation function that can handle arithmetic expressions and ternaries.
Expressions are converted according to their structure, recursing into sub-expressions.


\subsection{Context Object Design Pattern}
\label{sec:impl:context object design pattern}

Throughout the middle and back ends, I used a design pattern of passing a context object through all the function calls.
For example, when traversing the \gls{ast} to generate \gls{ir} code, the \RustInline{Context} struct in \ccref{lst:AST to IR context struct} is used to track information about the current context we are in with respect to the source program.
For example, it tracks the stack of nested loops and switch statements, so that when we convert a \CInline{break} or \CInline{continue} statement, we know where to branch to.

\begin{listing}[!ht]
  \begin{RustListing}
    pub struct Context {
        loop_stack: Vec<LoopOrSwitchContext>,
        scope_stack: Vec<Scope>,
        pub in_function_name_expr: bool,
        function_names: HashMap<String, FunId>,
        pub directly_on_lhs_of_assignment: bool,
    }
  \end{RustListing}
  \caption{The context data structure used when converting the \gls{ast} to \gls{ir} code.}
  \label{lst:AST to IR context struct}
\end{listing}

In an object-oriented language, this would often be achieved by encapsulating the methods in an object and using private state inside the object.
Rust, however, is not object oriented, and I found this approach to offer more modularity and flexibility.
Firstly, the context information itself is encapsulated inside its own data structure, allowing methods to be implemented on it that gives calling functions access to exactly the context information they need.
It also allows creating different context objects for different purposes.
In the target code generation stage, the \RustInline{ModuleContext} stores information about the entire module being generated, whereas the \RustInline{FunctionContext} is used for each individual function being converted.
The \RustInline{FunctionContext} lives for a shorter lifetime than the \RustInline{ModuleContext}, so being able to separate the data structures is ideal.


\subsection{Types}

\begin{mrwComment}
- handled type information - created data structure to represent possible types

- making sure instructions are type-safe, type converting where necessary - talk about unary/binary conversions, cite the C reference book
\end{mrwComment}

The following types are supported by the \gls{ir}, mirroring the types supported by the C language \ccite[Chapter 5]{c-reference-manual}.
\IrType{Ux} and \IrType{Ix} types represent unsigned and signed $x$-bit integers, respectively.
Enumeration types (enums) are supported; their values are encoded as \IrType{U64}s.
I followed the standard implementation convention for the bit size of \CInline{char}s, \CInline{short}s, \CInline{int}s, and \CInline{long}s; 8, 16, 32, and 64 bits respectively\footnote{The C specification doesn't specify the exact bit widths, only the minimum size.}.
%
\begin{align*}
\textit{T} &= \IrType{I8} \vertpad \IrType{U8} \vertpad \IrType{I16} \vertpad \IrType{U16} \vertpad \IrType{I32} \vertpad \IrType{U32} \vertpad \IrType{I64} \vertpad \IrType{U64} \vertpad \IrType{F32} \vertpad \IrType{F64} \vertpad \IrType{Void} \\
&\ \vertpad \IrType{Struct}(T[]) \vertpad \IrType{Union}(T[]) \\
&\ \vertpad \IrType{Pointer}(T) \vertpad \IrType{Array}(T, size) \\
&\ \vertpad \IrType{Function}(T, T[], is\_variadic) \\
\end{align*}
%
I created a data structure to represent these types, along with methods for operations on those types.

I implemented the ISO C unary and binary conversions for types \ccite[pp.\ 174--176]{c-reference-manual}.
They are applied before unary and binary operations respectively.
Unary conversion reduces the number of types an operand can be. Smaller integer types are promoted to \IrType{I32}/\IrType{U32} appropriately, and $\IrType{Array}(T, \_)$ types are converted to $\IrType{Pointer}(T)$.
Binary conversions make sure that both operands to a binary operation are of the same type.
First, the unary conversions are applied to each operand individually.
Then, if both operands are an arithmetic type, and one operand is a smaller type than the other, the smaller type is converted to the larger type.
This includes integer types being promoted to float types.

\subsection{The Relooper Algorithm}

\section{Back End: Target Code Generation}


\section{Runtime Environment}

\begin{mrwComment}
%TC:ignore
- Instantiating wasm module

- stdlib functions skeleton implementation

- arg passing + memory initialisation
%TC:endignore
\end{mrwComment}

\section{Optimisations}

\subsection{Unreachable Procedure Elimination}

\subsection{Tail-Call Optimisation}

\begin{mrwComment}
%TC:ignore
Defn of tail-call optimisation

Why do the optimisation
%TC:endignore
\end{mrwComment}

\section{Summary}

\end{document}
