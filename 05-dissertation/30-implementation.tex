\documentclass[00-main.tex]{subfiles}

\begin{document}

\chapter{Implementation}

This chapter describes the implementation of each stage of the compiler.
\Ccref{sec:impl:system architecture} gives an overview of the project structure, and subsequent sections explain in more detail.

Code snippets and figures are presented to supplement my explanations.
Where code is given, it is simplified to highlight the implementation details being explained.
This includes removing boilerplate code, error handling, and other features that are necessary for implementation but unhelpful for clarity.

\section{System Architecture}\label{sec:impl:system architecture}

\Ccref{fig:project flowchart} describes the high-level structure of the project.

\begin{figure}[!t]
  \centering
  \scalebox{0.8}{\tikzfig{70-figures/01-overview-flowchart}}
  \caption{Project structure, highlighting the preprocessor, front end, middle end, and back end. Each solid box represents a module of the project, transforming the input data representation into the output representation. The data representations are shown as dashed boxes. Where no new data representation is shown after a module, the output format is the same as the input format (i.e.~in the preprocessor).}
  \label{fig:project flowchart} % chktex 24
\end{figure}

I created my own \gls{ast} representation and \gls{ir}, which are used as the main data representations in the compiler.

% The optimisations I implemented in the middle end were unreachable procedure elimination, and tail-call optimisation.
% As an extension to the project, I implemented a more optimal stack allocation policy, as part of the target code generation module.

\section{Front End}

% \begin{figure}[H]
%   \centering
%   \tikzfig{70-figures/02-front-end-overview-flowchart}
%   \caption{Structure of the compiler front end.}
%   \label{fig:front end flowchart}
% \end{figure}

The front end of the compiler consists of the lexer and the parser; it takes C source code as input and outputs an abstract syntax tree.
I wrote a custom lexer, because this is necessary to support \CInline{typedef} definitions in C\@.
I used the LALRPOP parser generator to convert the tokens emitted by the lexer into an \gls{ast}~\ccite{lalrpop-docs}.

\subsection{Preprocessor}

I used the GNU C preprocessor (\mintinline{bash}{cpp}) to handle any preprocessor directives in the source code, for example macro definitions~\ccite{gnu-c-preprocessor}.
However, since I do not support linking, I removed any \CInline{#include} directives before running the preprocessor and handled them myself.

For each \CInline{#include <name.h>} directive that is removed, if it is one of the standard library headers that I support, the appropriate library code is copied into the source code from \Filename{headers/<name>.h}.
One exception is when the name of the header file matches the name of the source program, in which case the contents of the program's header file are inserted into the source code, rather than finding a matching library.

After processing \CInline{#include} directives, the compiler spawns \mintinline{bash}{cpp} as a child process, writes the source code to its stdin, and reads the processed source code back from its stdout.

\subsection{Lexer}

The grammar of the C language is mostly context-free, however the presence of \CInline{typedef} definitions makes it context-sensitive~\ccite[sec.~5.10.3]{c-reference-manual}.
For example, the statement \CInline{foo (bar);} can be interpreted in two ways:
\begin{itemize}
\item As a variable declaration, if \CInline{foo} has previously been defined as a type name\footnote{The brackets will be ignored.}; or
\item As a function call, if \CInline{foo} is the name of a function.
\end{itemize}

This ambiguity is impossible to resolve with the language grammar. The solution is to preprocess the \CInline{typedef} names during the lexing stage, and emit distinct type name and identifier tokens to the parser.
Therefore, I implemented a custom lexer that is aware of the type names already defined at the current program point.

The lexer is implemented as a \gls{fsm}. \Ccref{fig:lexer fsm} highlight portions of the machine; the remaining state transition diagrams can be found in \ccref{app:lexer fsm}.
The \gls{fsm} consumes the source code one character at a time, until the end of the token is reached (i.e.\ there is no transition for the next input character).
Then, the token corresponding to the current state is emitted to the parser (as the next token in the token stream).
Some states have no corresponding token to emit because they occur part-way through parsing a token; if the machine finishes in one of these states, this raises a lexing error.
(Every state labelled with a token is an accepting state of the machine.)
For tokens such as number literals and identifiers, the lexer appends the input character to a string buffer on each transition and when the token is complete, the string is stored inside the emitted token.
This gives the parser access to the necessary data about the token, for example the name of the literal.

If, when starting to lex a new token, there is no initial transition corresponding to the input character, then there is no valid token for this input. This raises a lex error and the compiler will exit.

\Ccref{subfig:lexer fsm numbers} shows the \gls{fsm} for lexing number literals. This handles all the different number formats that C supports: decimal, binary, octal, hexadecimal, and floating point.

\begin{figure}[p]
  \begin{subfigure}[t]{\textwidth}
    \centering
    \scalebox{0.85}{\tikzfig{70-figures/10-lexer-fsm/00-numbers}}
    \caption{\Gls{fsm} for lexing number literals.}%
    \label{subfig:lexer fsm numbers}
  \end{subfigure}
  \par\vspace{2\bigskipamount}
  \begin{subfigure}[t]{\textwidth}
    \centering
    \scalebox{0.85}{\tikzfig{70-figures/10-lexer-fsm/01-identifiers}}
    \caption{\Gls{fsm} for lexing identifiers.}%
    \label{subfig:lexer fsm identifiers}
  \end{subfigure}
  \caption{
    \glsreset{fsm}Sections of the lexer \gls{fsm}.
    Named states represent a valid token, and blank states are invalid.
    Only valid transitions are shown, using regular expressions; if no transition exists for an input character, the end of the current token has been reached.
    Transitions without a prior state are the initial transitions for the first input character.
    The token is emitted if the machine stops in a valid (named) state; otherwise a lexing error is raised.
    In a slight abuse of regular expression notation, `\texttt{.}' represents a literal full stop character.
    No input is consumed along an $\epsilon$ transition; the transition is taken if the condition is true. \\
    Tokens: \emph{Dec}:~decimal literal; \emph{FP}:~floating point literal; \emph{Oct}:~octal literal; \emph{Bin}:~binary literal; \emph{Hex}:~hexadecimal literal; \emph{Iden}:~identifier; \emph{Keyword}:~a C keyword; \emph{Typedef}\!:~an identifier defined as a type name. \\
    The states leading to the ellipsis token are shown for completeness since they share the starting dot state, even though the token is not a number literal.
  }%
  \label{fig:lexer fsm}
\end{figure}

\Ccref{subfig:lexer fsm identifiers} shows the \gls{fsm} for lexing identifiers and \CInline{typedef} names.
This is where the ambiguity discussed above is handled.
  Every time another character of an identifier is consumed, the name is compared to language keywords and \CInline{typedef} names we have encountered so far\footnote{Keywords are matched with higher priority.}.
  If a match is found, the machine transitions to the corresponding state, along an $\epsilon$ transition (no input is consumed).
The lexer stores each \CInline{typedef} name that is declared so that it can emit the correct token for future names.


\subsection{Parser}

I used the LALRPOP parser generator to generate parsing code from the input grammar that I wrote~\ccite{lalrpop-docs}.
It generates an \gls{ast} for which I defined the structure.
Microsoft's C Language Syntax Summary and C: A Reference Manual were very useful references to ensure I captured the subtleties of C's syntax when writing my grammar~\ccite{c-language-grammar-microsoft,c-reference-manual}. % chktex 13
My grammar is able to parse all core features of the C language, whilst omitting some of the recent additions. I chose to make my parser handle a larger subset of C than the compiler as a whole supports; the middle end rejects or ignores nodes of the \gls{ast} that it doesn't handle. For example, the parser handles storage class specifiers (e.g.~\CInline{static}) and type qualifiers (e.g.~\CInline{const}), and the middle end simply ignores them.

\subsubsection{Dangling Else Ambiguity}

A naive grammar for C (\ccref{lst:ambiguous if-else grammar}) contains ambiguity around \CInline{if}/\CInline{else} statements~\ccite[sec.~8.5.2]{c-reference-manual}.
C permits the bodies of conditional statements to be written without curly brackets if the body is a single statement.
If we have nested \CInline{if}/\CInline{else} statements that do not use curly brackets, it can be ambiguous which \CInline{if} an \CInline{else} belongs to. This is known as the dangling else problem~\ccite{dangling-else-wiki}.

\begin{listing}[t]
  \begin{GrammarListing}
    if-stmt      ::= "if" "(" expr ")" stmt
    if-else-stmt ::= "if" "(" expr ")" stmt "else" stmt
  \end{GrammarListing}
  \caption{
    Ambigious \CInline{if}/\CInline{else} grammar.
    Terminals are represented in double quotes, and non-terminals in bold.
    The non-terminals \GrammarInline{stmt} and \GrammarInline{expr} represent statements and expressions respectively, for which the rules are not shown.
  }
  \label{lst:ambiguous if-else grammar}
\end{listing}

An example of the dangling else problem is shown in \ccref{lst:dangling else}.
According to the grammar in \ccref{lst:ambiguous if-else grammar}, there are two possible parses of this program.
Either the \CInline{else} belongs to the inner or the outer \CInline{if} (\ccref{lst:dangling else:belonging to inner if} and \ccref{lst:dangling else:belonging to outer if}, respectively).

\begin{listing}[t]
  \begin{sublisting}[b]{0.25\textwidth}
    \begin{CListing}
      if (x)
        if (y)
          stmt1;
      else
        stmt2;
    \end{CListing}
    \caption{Ambiguous source code.}
    \label[listing]{lst:dangling else:ambiguous source}
  \end{sublisting}
  \hfill
  \begin{sublisting}[b]{0.35\textwidth}
    \begin{CListing}
      if (x) {
        if (y) {
          stmt1;
        } else {
          stmt2;
        }
      }
    \end{CListing}
    \caption{Parse: \CInline{else} belongs to inner \CInline{if}.}
    \label[listing]{lst:dangling else:belonging to inner if}
  \end{sublisting}
  \hfill
  \begin{sublisting}[b]{0.35\textwidth}
    \begin{CListing}
      if (x) {
        if (y) {
          stmt1;
        }
      } else {
        stmt2;
      }
    \end{CListing}
    \caption{Parse: \CInline{else} belongs to outer \CInline{if}.}
    \label[listing]{lst:dangling else:belonging to outer if}
  \end{sublisting}
  \caption{Example of the dangling else problem in C. The C source code (\subref{lst:dangling else:ambiguous source}) could have two potential parse trees (\subref{lst:dangling else:belonging to inner if}, \subref{lst:dangling else:belonging to outer if}) when using the grammar in \ccref{lst:ambiguous if-else grammar}.}
  \label{lst:dangling else}
\end{listing}

C resolves the ambiguity by always associating the \CInline{else} with the closest possible \CInline{if}.
We can encode this into the grammar with the concept of `open' and `closed' statements~\ccite{final-solution-to-dangling-else}.
\Ccref{lst:open/closed statement grammar} shows how I introduce this into my grammar for \CInline{if}/\CInline{else} statements.
All other forms of statement must also be converted to the new structure.
Any basic statements, i.e.\ statements that have no sub-statements, are added to \GrammarInline{closed-stmt}.
All other statements that have sub-statements, such as \CInline{while}, \CInline{for}, and \CInline{switch} statements, must be duplicated to both \GrammarInline{open-stmt} and \GrammarInline{closed-stmt}.


\begin{listing}[t]
  \begin{GrammarListing}
    stmt        ::= open-stmt | closed-stmt

    open-stmt   ::= "if" "(" expr ")" stmt
                  | "if" "(" expr ")" closed-stmt "else" open-stmt
                  | ...

    closed-stmt ::= "if" "(" expr ")" closed-stmt "else" closed-stmt
                  | ...
  \end{GrammarListing}
  \caption{Using open and closed statements to solve the dangling else problem. Other statement rules with no sub-statements are added to \GrammarInline{closed-stmt}. Rules with sub-statements, e.g.~rules for \CInline{for} and \CInline{while}, are duplicated across \GrammarInline{open-stmt} and \GrammarInline{closed-stmt}, having open and closed sub-statements, respectively.\medskip}
  \label{lst:open/closed statement grammar}
\end{listing}

A closed statement always has the same number of \CInline{if} and \CInline{else} keywords (excluding anything between a pair of brackets, because bracket matching is unambiguous).
Thus, in the second alternative of an \GrammarInline{open-stmt}, the \CInline{else} terminal can be found by counting the number of \CInline{if}s and \CInline{else}s we encounter since the start of the \GrammarInline{closed-stmt}; the \CInline{else} belonging to the \GrammarInline{open-stmt} is the first \CInline{else} once we have more \CInline{else}s than \CInline{if}s.

If open statements were allowed inside the \CInline{if} clause of an \GrammarInline{open-stmt}, then \GrammarInline{open-stmt} and \GrammarInline{closed-stmt} would no longer be disjoint, and the grammar would be ambiguous. The above method for finding the matching \CInline{else} would not work.

\subsubsection{LALRPOP Parser Generator}

\Ccref{lst:AST generation code example} is an example of the LALRPOP syntax for addition expressions.
The left-hand side of the~\RustInline{=>} describes the grammar rule, and the right-hand side is the code to generate an \RustInline{Expression} node.
Terminals are represented in double quotes; these are defined to map to the tokens emitted by the lexer.
Non-terminals are represented inside angle brackets, with their type and a variable name to use in the \gls{ast} generation code.

\begin{listing}[t]
  \begin{sublisting}[t]{\textwidth}
    \begin{GrammarListing}
      additive-expression ::= additive-expression "+" multiplicative-expression
    \end{GrammarListing}
    \caption{The grammar rule for addition expressions.}
    \label{sublst:addition expr grammar rule}
  \end{sublisting}
  \par\medskip % vertical space between subfigures
  \begin{sublisting}[t]{\textwidth}
    \begin{RustListing}
      AdditiveExpression: ast::Expression = {
          <e1:AdditiveExpression> "+" <e2:MultiplicativeExpression>
              => ast::Expression::BinaryOp(
                  ast::BinaryOperator::Add,
                  e1,
                  e2
              ),
          ...
      };
    \end{RustListing}
    \caption{The LALRPOP syntax for the addition grammar rule.}
    \label{sublst:lalrpop syntax addition rule}
  \end{sublisting}
  \caption{
    Example of the LALRPOP syntax for grammar rules.
    (\subref{sublst:addition expr grammar rule}) is the rule for an addition expression, and (\subref{sublst:lalrpop syntax addition rule}) is the Rust code to represent it.
    In LALRPOP, the grammar parsing and \gls{ast} generation code are combined.
  }
  \label{lst:AST generation code example}
\end{listing}

LALRPOP also allows macros to be defined, which allow the grammar to be written in a more intuitive way.
For example, I defined a macro to represent a comma-separated list of non-terminals (\ccref{lst:parser macro for comma-separated list}).
The macro has a generic type \RustInline{T}, and returns the list items as a \RustInline{Vec<T>}.

\begin{listing}[t]
  \begin{RustListing}
    CommaSepList<T>: Vec<T> = {
        <mut v:(<T> ",")*> <e:T> => {
            v.push(e);
            v
        }
    };
  \end{RustListing}
  \caption{LALRPOP macro to parse a comma-separated list of non-terminals.}
  \label{lst:parser macro for comma-separated list}
\end{listing}

\subsubsection{String Escape Sequences}

The parser has to handle escape sequences in strings.
I implemented this by first creating an iterator over the characters of a string, which replaces escape sequences by the character they represent as it emits each character.
When the current character is a backslash, instead of emitting it straight away, the iterator consumes the next character and emits the character corresponding to the escape sequence.
I wrapped this in an \RustInline{interpret_string} function that internally creates an instance of the iterator and collects the emitted characters back to a string.

\subsubsection{Parsing Type Specifiers}

Another feature of the C language is that type specifiers (\CInline{int}, \CInline{signed}, etc.) can appear in any order before a declaration.
For example, \CInline{signed int x} and \CInline{int signed x} are equivalent declarations.
To handle this, my parser first consumes all type specifier tokens of a declaration, then constructs an \RustInline{ArithmeticType} \gls{ast} node from them.
It uses a bitfield where each bit represents the presence of one of the type specifiers in the type.
The bitfield is the normalised representation of a type; every possible declaration that is equivalent to a type will have the same bitfield.
The declarations above would construct the bitfield \RustInline{0b00010100}, where the two bits set represent \CInline{signed} and \CInline{int} respectively.
For each type specifier, the corresponding bit is set.
Then, the bitfield is matched against the possible valid types, to assign the type to the \gls{ast} node.


\section{Middle End}

The middle end takes an \gls{ast} as input, and transforms it to intermediate code.
It also runs the Relooper algorithm on the \gls{ir}.
Optimisations are run on the \gls{ir} in this stage; they are described in \ccref{sec:impl:optimisations}.

\subsection{Intermediate Code Generation}

I defined a custom three-address code \gls{ir} (the instructions are listed in \ccref{app:intermediate code}).
I decided to create a custom instruction set instead of using an existing one as it allowed me to tailor my compiler with exactly the features I support. For example, the \gls{ir} does not include indirect function calls.
I also added instructions specific to the Relooper algorithm.
The \gls{ir} contains both the program instructions and necessary metadata, such as variable type information, the mapping of variable and function names to their IDs, etc.
The instructions and metadata are contained in separate sub-structs within the main \gls{ir} struct, which enables the metadata to be carried forwards through stages of the compiler pipeline while the instructions are transformed at each stage.
In the stage of converting the \gls{ast} to \gls{ir} code, the instructions and metadata are encapsulated in the \RustInline{Program} struct.

Many objects in the \gls{ir} require unique IDs, such as variables and labels.
I created an \RustInline{Id} trait to abstract this concept, together with a generic \RustInline{IdGenerator} struct (\ccref{lst:Id and IdGenerator implementation}).
The ID generator internally tracks the highest ID that has been generated so far, so that the \gls{ir} can create as many IDs as necessary without needing to know anything about their implementation.
IDs are generated inductively: each ID knows how to generate the next one.

\begin{listing}[t]
  \begin{RustListing}
    trait Id {
        fn initial_id() -> Self;   // Base case
        fn next_id(&self) -> Self; // Generate next ID inductively
    }

    struct IdGenerator<T: Id + Clone> {
        max_id: Option<T>,         // Internally track the highest ID used so far
    }

    impl<T: Id + Clone> IdGenerator<T> {
        // Static method to initialise an IdGenerator
        fn new() -> Self {
            IdGenerator { max_id: None }
        }
        // Use the IdGenerator to get a new ID
        fn new_id(&mut self) -> T {
            let new_id = match &self.max_id {
                None => T::initial_id(),
                Some(id) => id.next_id(),
            };
            self.max_id = Some(new_id.to_owned()); // Update the stored highest ID
            new_id
        }
    }
  \end{RustListing}
  \caption{Implementation of the \RustInline{Id} trait and \RustInline{IdGenerator} struct, used to inductively generate IDs for objects in the \acrlong{ir}.}
  \label{lst:Id and IdGenerator implementation}
\end{listing}


To convert the \gls{ast} to \gls{ir} code, the compiler recursively traverses the tree, generating three-address code instructions and metadata as it does so.
At the highest level, the \gls{ast} contains a list of statements.
For each of these, the compiler calls \RustInline{convert_statement_to_ir(stmt, program, context)}~\ccref{lst:convert while stmt to IR}. % chktex 36
\RustInline{program} is the mutable intermediate representation, to which instructions and metadata are added as the \gls{ast} is traversed.
\RustInline{context} is the context object described in \ccref{sec:impl:context object design pattern}, which passes relevant contextual information through the functions recursively.

The core of converting a statement or expressions to \gls{ir} code is pattern matching the \gls{ast} node, and generating \gls{ir} instructions according to its structure, recursing into sub-statements and expressions.
The case for a \CInline{while} statement is shown in \ccref{lst:convert while stmt to IR}; the labels and branches to execute a \CInline{while} loop are added, and the instructions to evaluate the condition and body are generated recursively.
Other control-flow structures are similar.

\begin{listing}[t]
  \begin{RustListing}
    fn convert_statement_to_ir(stmt: Statement, prog: &mut Program, context: &mut Context)
            -> Vec<Instruction> {
        let mut instrs = Vec::new();
        match stmt {
            Statement::While(cond, body) => {
                let loop_start_label = prog.new_label(); // Create labels for start/end of loop
                let loop_end_label = prog.new_label();
                // Create a new loop context
                context.push_loop(LoopContext::while_loop(loop_start_label, loop_end_label));
                instrs.push(Instruction::Label(loop_start_label));
                // Evaluate loop condition
                let (cond_instrs, cond_var) = convert_expression_to_ir(cond, prog, context);
                instrs.append(cond_instrs);
                // Branch out of loop if condition is false
                instrs.push(Instruction::BrIfEq(cond_var, Constant::Int(0), loop_end_label));
                // Recursively convert loop body
                instrs.append(convert_statement_to_ir(body, prog, context));
                // Branch back to the start of the loop
                instrs.push(Instruction::Br(loop_start_label));
                instrs.push(Instruction::Label(loop_end_label));
                // Remove the loop context we added
                context.pop_loop();
            }
            // ... other AST statement nodes ...
        }
        instrs
    }
  \end{RustListing}
  \caption{Generating \gls{ir} code for a \CInline{while} statement. The \gls{ast} is pattern matched to handle each type of statement separately. The other cases are not shown here.}
  \label{lst:convert while stmt to IR}
\end{listing}

Some statements required care to ensure the semantic meaning of the program was preserved.
For example, \CInline{switch} statements can contain \CInline{case} blocks in any order.
Some blocks may fall-through to the next block, and there may be a \CInline{default} block.
I handled this by first generating instructions for each \CInline{case} block, and storing them in a switch context until all blocks have been seen.
At the start of the \CInline{switch} statement, conditional branches to each \CInline{case} block are inserted.
After these come the blocks themselves.
By doing this, there was no direct fall-through between any blocks; instead, such a block will end in a branch instruction to the start of the next block.
This also allows \CInline{default} blocks to be easily handled; we just add an unconditional branch to the default block after all the conditional branches.
If there is no \CInline{default} block, the conditional branches are followed by an unconditional branch to the end of the \CInline{switch} statement.
\Ccref{fig:instr structure for switch statements} shows the structure of the generated instructions for \CInline{switch} statements.

\begin{figure}[t]
  \centering
  \scalebox{0.8}{\tikzfig{70-figures/05-switch-case-handling}}
  \caption{\gls{ir} instructions generated for \CInline{switch} statements.}
  \label{fig:instr structure for switch statements} % chktex 24
\end{figure}

Declaration statements have many different formats, each of which I had to handle separately; e.g.~variables may be declared without being initialised with a value.

Function declarations only differ syntactically from other declarations by the type of the identifier; therefore they are handled in the same place.
For function definitions (i.e.~declarations plus body), the body code is converted and the function is added to the \gls{ir}.

Arrays are complicated because there are multiple ways their length can be specified.
It can be given explicitly in the declaration, or implicitly inferred from the length of the initialiser.
Furthermore, an explicit size can either be a static value or a variable that is only known at run time (creating a variable-length array).
To handle variable-length arrays, an instruction is inserted to allocate space on the stack for it at run time.
Array and struct initialisers are handled by first allocating memory for the variable, then storing the value of each of the inner members.

Some expressions can be evaluated by the compiler ahead-of-time, e.g.~array length expressions.
I implemented a compile-time expression evaluator that can handle arithmetic expressions and ternary statements.
Expressions are converted according to their structure, recursing into sub-expressions.


\subsection{Context Object Design Pattern}\label{sec:impl:context object design pattern}

Throughout the middle and back ends, I used a design pattern of passing a context object through all function calls.
For example, when traversing the \gls{ast} to generate \gls{ir} code, the \RustInline{Context} struct in \ccref{lst:AST to IR context struct} was used to track information about the current context with respect to the source program.
It tracks the stack of nested loops and switch statements, so that when a \CInline{break} or \CInline{continue} statement is converted, the branch target is known.

\begin{listing}[t]
  \begin{RustListing}
    struct Context {
        loop_stack: Vec<LoopOrSwitchContext>,
        scope_stack: Vec<Scope>,
        in_function_name_expr: bool,
        function_names: HashMap<String, FunId>,
        directly_on_lhs_of_assignment: bool,
    }
  \end{RustListing}
  \caption{The context data structure used when converting the \gls{ast} to \gls{ir} code.}
  \label{lst:AST to IR context struct}
\end{listing}

In an object-oriented language, this would often be achieved by encapsulating the methods in an object and using private state inside the object.
Rust, however, is not object-oriented, and I found this approach to offer more modularity and flexibility.
Firstly, the context information is encapsulated inside its own data structure, allowing methods to be implemented on it that give calling functions access to the exact context information needed.
It also enables creating different context objects for different purposes.
In the target code generation stage, the \RustInline{ModuleContext} stores information about the entire module being generated, whereas the \RustInline{FunctionContext} is used for each individual function being converted.
The \RustInline{FunctionContext} has a shorter lifetime than the \RustInline{ModuleContext}, hence separating the data structures is ideal.


\subsection{Types}

\Ccref{fig:ir supported types} outlines the types supported by the \gls{ir}, mirroring the types supported by the C language~\ccite[ch.~5]{c-reference-manual}.
\IrType{Ux} and \IrType{Ix} types represent unsigned and signed $x$-bit integers, respectively.
Enumeration types (enums) are supported; their values are encoded as \IrType{U64}s.
I followed the standard implementation convention for the bit size of \CInline{char}s, \CInline{short}s, \CInline{int}s, and \CInline{long}s; 8, 16, 32, and 64 bits respectively\footnote{The C specification doesn't define the exact bit widths, only the minimum size.}.

\begin{figure}[t]
  \setlength{\abovedisplayskip}{-6pt}
  \setlength{\belowdisplayskip}{-20pt}
  \begin{align*}
    \textit{T} ={} &\IrType{I8} \vertpad \IrType{U8} \vertpad \IrType{I16} \vertpad \IrType{U16} \vertpad \IrType{I32} \vertpad \IrType{U32} \vertpad \IrType{I64} \vertpad \IrType{U64} \vertpad \IrType{F32} \vertpad \IrType{F64} \vertpad \IrType{Void} \\
    \vertpad &\IrType{Struct}(T[]) \vertpad \IrType{Union}(T[]) \\
    \vertpad &\IrType{Pointer}(T) \vertpad \IrType{Array}(T, size) \\
    \vertpad &\IrType{Function}(T, T[], is\_variadic) \\
  \end{align*}
  \caption{Supported types in the \gls{ir}. \IrType{Ix} and \IrType{Ux} types represent signed and unsigned integers, of bit-width $x$.}%
  \label{fig:ir supported types}
\end{figure}

I implemented the ISO C unary and binary conversions for types~\ccite[pp.~174--176]{c-reference-manual}.
They are applied before unary and binary operations, respectively.
Unary conversion constrains the possible types an operand can have. Smaller integer types are promoted to \IrType{I32}/\IrType{U32} appropriately, and $\IrType{Array}(T, \_)$ types are converted to $\IrType{Pointer}(T)$.
Binary conversions make sure that both operands are of the same type.
Firstly, the unary conversions are applied to each operand individually.
Then, if both operands are arithmetic, and one operand has a smaller type than the other, the smaller is converted to the larger type.
This includes integer promotion to float types.

When transforming each \gls{ast} node to intermediate code, the compiler checks that the types of the operands are valid (after unary/binary conversion) and stores the corresponding type of the result variable.
For example, before generating the instruction \IrCode{t = a < b}, the compiler checks whether \IrCode{a} and \IrCode{b} are comparable arithmetic types---if not, a compile error is thrown---and sets the type of \IrCode{t} to \IrType{I32}\footnote{\IrType{I32} is used to represent booleans.}.
This ensures that the \gls{ir} passed to the back end is type safe.

\subsection{The Relooper Algorithm}\label{sec:impl:relooper algorithm}

The Relooper algorithm was described in \ccref{sec:prep:relooper}.
Here, I describe its implementation.
The Relooper stage takes the \gls{ir} as input, and transforming the instructions into Relooper blocks whilst preserving program metadata.

Firstly, the intermediate code is `soupified'.
This is the process of taking the linear sequence of instructions and producing a set of \emph{labels}\footnote{The term `label' is overloaded here, though the two concepts are related; a `label instruction' refers to the intermediate code instruction, whereas a `label' is a basic block that starts with a label instruction.} (basic blocks), which are the input to the Relooper algorithm itself.
Each label starts with a \IrCode{label} instruction and ends in a \IrCode{branch}.
The process of soupifying is \ccref{alg:soupify}:

\begin{Algorithm}{`Soupifying' the intermediate code. The input is a linear instruction sequence, and a set of label blocks is produced.}{alg:soupify}
\begin{EnumerateAlgorithm}
\item\label[algstep]{alg:soupify:remove label fall-through}
Remove any label fall-through.
By this I mean any label instruction that is not preceded by a branch in the linear instruction sequence, to which control will flow directly from the previous instruction.
A branch instruction is inserted before each label instruction, if one does not already exist, branching to that label.
This ensures that a label instruction is always preceded by a branch instruction along every control flow path.

This will generate many redundant branch instructions, however they are removed when the Relooper blocks are transformed to target code.

\item
Insert an unconditional branch after each conditional branch, to ensure that conditional branches do not occur in the middle of a block.
A new label instruction is created for the unconditional branch, if necessary.

\begin{center}
\small
\pbox[t]{.5\textwidth}{
\IrCode{br <l1> if ...} \\ % chktex 26 chktex 11
\IrCode{br <l2>} \\
\IrCode{label <l2>} \\
\IrCode{...} % chktex 11
}
\hspace{1em}
\pbox[t]{.5\textwidth}{
\CodeComment{Original conditional branch} \\
\CodeComment{New unconditional branch} \\
\CodeComment{New label \IrCode{<l2>}}
}
\end{center}

\item
If there is no label instruction at the very start of the instructions, add one.

\item\label[algstep]{alg:soupify:merge consecutive labels}
Merge any consecutive label instructions into a single instruction, updating branches to the labels accordingly.

\item
Finally, divide the instructions into blocks by passing through the instructions sequentially and starting a new block at each label instruction.
\end{EnumerateAlgorithm}
\end{Algorithm}

\Ccrefrange{alg:soupify:remove label fall-through}{alg:soupify:merge consecutive labels} transform the instruction sequence until it can be directly split into label blocks.
All control flow between labels is made explicit.

For every function in the \gls{ir}, the instructions are soupified and then \ccref{alg:relooper} (\ccref{sec:prep:relooper algorithm}) is used to create a Relooper block.
The \RustInline{IdGenerator} struct (\ccref{lst:Id and IdGenerator implementation}) is used to assign each loop and multiple block a unique ID\@.

In \ccref{alg:relooper}, the \emph{reachability} set of a label is used to determine which type of block to generate.
The reachability of each label is calculated by taking the transitive closure of its possible branch targets.
Starting with a copy of the set of possible branch targets of a label, we iteratively add the possible branch targets of each label in the set, until there are no more changes.
(The reachability is a set so that no duplicates are added.)

% As it creates blocks, the algorithm replaces branch instructions.
% Once an inner block has been created, branches inside it are replaced recursively
\begin{mrwComment}
  Maybe talk a bit more about the actual relooper implementation?

  - why do we need the reachability set that we've just mentioned?
\end{mrwComment}

\section{Back End: Target Code Generation}

In the back end, I defined data structures to directly represent a WebAssembly module with its constituent sections.
I also defined a \RustInline{WasmInstruction} enum to represent all possible WebAssembly instructions, and data structures to represent value types.
The back end generates a \RustInline{WasmModule} containing \RustInline{WasmInstruction}s, and its byte representation is written to a binary file as output.

I defined a \RustInline{ToBytes} trait that is implemented by all data structures that are written to the binary.
This provides a layer of abstraction between the program data structures and the actual byte values in the binary; each structure only needs to know the byte values specific to itself and nothing more.

The core function of the back end is to transform \gls{ir} instructions to WebAssembly instructions.
Since WebAssembly has a stack-based architecture, the general pattern for converting an instruction is:
\begin{itemize}[nosep]
\item Push the value of each operand onto the stack (loading any variables from memory).
\item Perform the operation, which leaves the result on top of the stack.
\item Store the result from the stack back to a variable in memory.
\end{itemize}
Most instructions are a variation of this pattern.
\Ccref{lst:converting add instr to wasm code} shows the code generated for an add instruction.
One subtlety is that \WasmInstr{store} instructions take the memory address as their first operand, and the value to store as their second operand.
This means the address of the destination variable needs to be pushed onto the stack \emph{before} the source operands are loaded and the operation is performed.

\begin{listing}[t]
  \begin{sublisting}[b]{0.28\textwidth}
    \begin{TextListing}
      |\IrCode{c = a + b}|
    \end{TextListing}
    \caption{Intermediate code.}
  \end{sublisting}
  \hfill
  \begin{sublisting}[b]{0.7\textwidth}
    \begin{WasmListing}
      i32.const |<addr of c>|  ;; address for the store instruction
      i32.const |<addr of a>|
      i64.load               ;; load a onto the stack
      i32.const |<addr of b>|
      i64.load               ;; load b onto the stack
      i64.add                ;; perform operation on a and b
      i64.store              ;; store result from top of stack
    \end{WasmListing}
    \caption{Generated WebAssembly code.}
  \end{sublisting}
  \caption{\Gls{ir} code and generated target code for transforming an add instruction, assuming \IrCode{a} and \IrCode{b} are variables of type \IrType{I64}.}
  \label{lst:converting add instr to wasm code}
\end{listing}

I defined \RustInline{load} and \RustInline{store} functions that take the \gls{ir} type of a variable and return correctly typed load and store instructions respectively.
The \RustInline{store} function encapsulates the fact that the address operand has to come before the value to store; this helps to ensure correctness as it is only defined in one place.

I used the same ID generator pattern as in the middle end, to generate unique indexes for items in the WebAssembly module, such as functions and types.

\subsection{Memory Layout}

One of the other main functions of the back end is to generate code that manages the memory layout.
A large part of this is the function call stack; pushing new stack frames when functions are called, and popping them when functions return.
\Ccref{fig:memory structure} shows the memory layout I defined for the compiler.

\begin{figure}[t]
  \centering
  \scalebox{0.9}{\tikzfig{70-figures/06-memory-layout}}
  \caption{Memory structure, with addresses increasing to the right.}
  \label{fig:memory structure} % chktex 24
\end{figure}

The first section of memory contains the \gls{fp} and \gls{sp}, as well as a temporary pointer storage location which is used in intermediate steps of manipulating stack frames.
I chose to call this the `temporary \gls{fp}', because it is mainly used to hold the new value that the \gls{fp} will be set to once we have finished setting up a new stack frame.
In a register architecture, these pointers would be stored in registers, however WebAssembly does not have any registers, so instead I chose to allocate them at known locations in memory.

The next section of memory contains any string literals that are defined in the program.
In C, a string literal is simply a null-terminated array of characters, which a variable accesses via a \CInline{char*}.
My \gls{ir} has a dedicated \RustInline{PointerToStringLiteral} instruction which, in the back end, gets converted to an instruction that pushes the address of the corresponding string onto the stack.
All string literals are allocated at compile-time and have compile-time known addresses.

Space is allocated for all global variables at compile-time.
These are allocated in the same way as local variables, described below.

Programs may accept command-line arguments, which are stored into memory by the runtime environment.
This is described in more detail in \ccref{sec:impl:runtime} below.

The function call stack grows upwards dynamically from this point.
I did not implement heap storage, so memory only increases from one end, unlike in standard C compilers such as GCC\@.

\subsection{Stack Frame Operations}

Whenever a function is called, a new stack frame is constructed at the top of the stack in memory.
\Ccref{fig:stack frame layout} shows the structure of each stack frame.

\begin{figure}[t]
  \centering
  \scalebox{0.9}{\tikzfig{70-figures/07-stack-frame-layout}}
  \caption{Stack frame layout, with addresses increasing upwards.}
  \label{fig:stack frame layout} % chktex 24
\end{figure}

I defined callee/caller conventions for pushing and popping stack frames.
The caller is responsible for constructing the frame with the previous \gls{fp} value, space for the return address, and any function parameters.
The caller also deallocates the stack frame once the function returns.
The callee is responsible for allocating space for its local variables on the stack (discussed in \ccref{sec:impl:local variable allocation} below).

The procedure for pushing a new stack frame is \ccref{alg:pushing new stack frame}:

\begin{Algorithm}{Pushing a new stack frame.}{alg:pushing new stack frame}
\begin{EnumerateAlgorithm}
\item
Store the current \gls{fp} at the top of the stack.
\item
Copy the current \gls{sp} to Temp \gls{fp}: the address of the start of the stack frame.
\item
Allocate space for the return value on top of the stack, according to the return type.
\item
Store each function parameter on top of the stack.
This is either copying a variable from the stack frame below or storing a constant.
\item
Set the \gls{fp} to the value held in Temp \gls{fp}.
\end{EnumerateAlgorithm}
\end{Algorithm}

Using the temporary \gls{fp} here is necessary because if we directly wrote to the \gls{fp}, we would not be able to copy any variables in as parameters.
(Local variables have \gls{fp}-relative addresses.)
We also cannot wait to save the new \gls{fp} address until after we have copied the variables, because by then the \gls{sp} has been moved and no longer points to the start of the stack frame.
(The \gls{sp} is updated every time a value is stored on top of the stack.)

Popping a stack frame is simpler:

\begin{Algorithm}{Popping a stack frame.}{alg:popping stack frame}
\begin{EnumerateAlgorithm}
\item
Set the \gls{sp} to the current value of the \gls{fp}.
The new top of the stack is the top of the previous stack frame.
\item
Restore the previous value of the \gls{fp}.
\item
If the function returns a result to a variable, copy the return value from the stack frame to the destination variable.
\end{EnumerateAlgorithm}
\end{Algorithm}

We can pop the frame simply by moving the \gls{sp} because the data above the \gls{sp} will never be read; when the stack grows again, it will be overwritten.

\subsection{Local Variable Allocation}\label{sec:impl:local variable allocation}

Initially I implemented a naive variable allocation strategy.
As an extension, I implemented a more optimal allocation strategy, described in \ccref{sec:impl:optimised stack allocation}.

Variable allocation is done at compile time, so that variable accesses can be static \gls{fp} offsets rather than requiring a run time address table.
Only in specific cases---variable-length arrays, for example---are variables allocated at run time.
My \gls{ir} has a dedicated \IrCode{AllocateVariable} instruction which takes the number of bytes to allocate as an operand.

Each variable, including temporary variables, is allocated a dedicated byte range in memory; none of the variables overlap.
This safely maintains the programmer's memory model, however it is an inefficient use of memory, hence the later optimisation.

Local variables are stored as part of the function stack frame, the address of which is run time dependent.
Therefore, variables are allocated as offsets from the \gls{fp}.
At run time, a variable is accessed by first loading the \gls{fp}, then adding the variable offset to get the variable's memory address.

\section{Runtime Environment}\label{sec:impl:runtime}

The purpose of the runtime environment is to provide an interface between the WebAssembly binary and the system, allowing it to be executed.
I chose to use Node.js, because this allows the binary to be run locally~\ccite{nodejs}.
The other option would be to use JavaScript within a web browser, which would be the primary reason to use WebAssembly; however, for the purposes of this project that approach would have added unnecessary complexity.

\begin{mrwComment}
Reason for using Node.js maybe should go in preparation chapter
\end{mrwComment}

The runtime is responsible for instantiating the WebAssembly module.
A module must be instantiated before any functions it contains can be executed.
One of the main functions of the instantiation is to pass imports into the WebAssembly module.
The runtime creates a new linear memory for the module to use, and also imports my skeleton library functions.
The memory is created by the runtime so that the standard library functions can have access to it.
If the memory were created by the WebAssembly module, which is the other option, then the imported functions would not be able to reference it.

Once the module has been instantiated, the runtime stores the command-line program arguments into memory.
As is standard for C programs, the first argument is always the name of the program being run (i.e.\ the name of the WebAssembly binary).
Arguments are passed to the \CInline{main} function as an array of \CInline{char} pointers (\CInline{argv}), plus the argument count (\CInline{argc}).
To facilitate this, the runtime first allocates space for the array of pointers, at the compile-time known address immediately after the global variables.
After the pointer array, the actual argument values are stored (as strings), and each corresponding pointer is set (\ccref{fig:program args memory structure}).
The runtime then sets the \gls{sp} to immediately after the last argument.

\begin{figure}[t]
  \centering
  \scalebox{0.9}{\tikzfig{70-figures/08-program-args-memory-layout}}
  \caption{Memory structure of program arguments, with addresses increasing to the right. Here, \CInline{argc}${} = 2$.}
  \label{fig:program args memory structure} % chktex 24
\end{figure}

Finally, the runtime calls the exported \JSInline{main} function, with the values of \JSInline{argc} and \JSInline{argv} as parameters.

\section{Optimisations}\label{sec:impl:optimisations}

After completing the main compiler pipeline, I implemented optimisations in the middle and back ends.
I performed unreachable procedure elimination and tail-call optimisation in the middle end, and in the back end I created a more optimal stack allocation policy for local variables.

\subsection{Unreachable Procedure Elimination}\label{sec:impl:unreachable procedure elimination}

Unreachable procedure elimination removes all functions that are never called.
I do not support function pointers; all function calls are direct.
Therefore, if no \IrCode{call} instruction references a particular function, that function is guaranteed to never be called.

First, the call graph is generated.
This is a directed graph where each node is a function in the program and each edge represents a syntactic function call.
To maintain correctness, the constructed graph is a superset of the \emph{semantic} call graph, which contains all the function calls that can happen in actual execution.
The semantic graph is undecidable at compile time\footnote{Due to the undecidability of arithmetic, it is not possible to decide in general which control flow path will be taken.}, so we calculate the syntactic call graph.

I used an adjacency list to store the call graph, since the graph is likely to be sparse.
(An adjacency matrix would be more suitable for densely connected graphs.)
The call graph data structure also has a set of entry nodes, which are functions that are called from the global scope of the program (e.g.~\CInline{main()}). % chktex 36

To generate the graph, all functions in the \gls{ir} are added as nodes, then an edge is added for every \IrCode{call} instruction.
For a \IrCode{call y} instruction found inside function \IrCode{x}, a directed edge is added from node \IrCode{x} to node \IrCode{y}.
Any functions that are called from the global scope, as well as the \CInline{main} function, are added to the set of entry nodes.

Once the call graph is constructed, it is used to find functions that are never called.
First mark every function as unused.
I then implemented a \gls{bfs} over the graph, starting from each entry node.
Each node we reach is marked as used.
Once the search returns, all nodes still marked as unused can be safely removed from the program.

The benefit of using a \gls{bfs} rather than simply looking for nodes with no incoming edges is that it can handle cycles.
A program may contain cycles of functions that call each other but that are never called from the rest of the program, so the cycle is never entered.
In this case, the nodes would have incoming edges, but would not be reached in a \gls{bfs}, allowing them to be removed.


\subsection{Tail-Call Optimisation}

When a function is called recursively, a new stack frame is pushed onto the call stack.
A recursive tail-call is when the recursive call is the last operation of the calling function, and the result from the recursive call is directly returned from the caller.
In these cases, the caller's stack frame is unneeded as soon as the recursive call is made; when the recursive call returns, the return value is copied to the stack frame below, but nothing more is done.
Therefore, tail-call optimisation aims to remove the unnecessary stack frames, thereby reducing the amount of stack memory used.
The new stack frame replaces the caller's stack frame, instead of being pushed on top of it.

This can have a dramatic impact on memory usage; it turns a function's memory usage from $\bigo{n}$ to $\bigo{1}$ (where $n$ is the recursion depth).
Not only does this make a program much more efficient, but for deep enough recursion it allows programs to run that would otherwise crash due to running out of memory.

There is a distinction between general tail-calls and recursive tail-calls. In general, a tail-call is any function call that is the last action of a function\footnote{A tail-call necessarily has the same return type as the calling function, because otherwise an explicit type conversion instruction would already have been inserted before the \IrCode{return} instruction.}, whereas a recursive tail-call is specifically a recursive call.
Recursive tail-calls are easier to optimise because the function signature is the same, and also have the greatest performance impact because many recursive calls may be made.

Initially I implemented this optimisation by replacing the stack frame in the target code generation stage, however this did not actually work properly.
Function calls are implemented using WebAssembly functions and the \WasmInstr{call} instruction, which means that the WebAssembly virtual machine has its own call stack as well as the call stack I manage in memory.
At large recursion depths, the WebAssembly stack was still running out of memory.
Instead, I implemented an approach of transforming the recursion into iteration in the \gls{ir} stage.
This was much more successful.
However, I kept the old optimisation for the cases of non-recursive tail-calls, because it does still reduce memory usage there.
I will describe both approaches in turn below.

\subsubsection{Approach 1: Replacing the Caller's Stack Frame at Runtime}

My first approach was to replace the caller's stack frame at run time rather than push a new stack frame on top whenever a tail-call is made.

To do this, in the middle end we find all \IrCode{call} instructions that are directly returned from the function, and replace them with a \IrCode{tail-call} instruction.

In the target code generation stage, \IrCode{tail-call} instructions are handled by re-using the current stack frame instead of pushing a new one.
The construction of this is very similar to how a stack frame is normally pushed.
The frame pointer and space for the return value are left as they are; they don't need to be changed for the new stack frame.

To store the parameters in the new stack frame, first they are copied to a region of unused memory above the stack, then copied to their respective positions in the stack frame. This ensures the needed values in the old stack frame are not overwritten by the new stack frame before they are used.

This approach works for both recursive and non-recursive tail-calls.
However, as described above, it is not ideal for recursive tail-calls, because of WebAssembly's own function call stack.
Therefore I implemented the second approach below.

\subsubsection{Approach 2: Transforming Recursion to Iteration}

The second approach is entirely contained in the middle end.
It only targets recursive tail-calls; these are where the majority of the performance gains are to be found.
It removes the recursive calls altogether, and replaces them with iteration back to the start of the function.
Therefore, no new stack frame will be allocated.

Similarly to above, we start by finding all \IrCode{call} instructions that are both recursive and are tail-calls.
In the current function, there is a variable holding each parameter.
For each argument to the recursive call, we copy it to the variable holding the corresponding parameter in the current function.
Once we have done this assignment for each parameter, we add a branch instruction to the start of the function.
\Ccref{lst:tail-call optimisation example} shows an example of this transformation.
(C code is shown for clarity, however the actual optimisation happens on the intermediate code.)

This approach solves the problem of the WebAssembly virtual machine's call stack running out of memory, because the function calls have been entirely removed.
The recursion is now contained within a single function using iteration.

\begin{listing}[t]
  \begin{sublisting}[b]{0.49\textwidth}
    \begin{CListing}
      long sum(long n, long acc) {
          if (n == 0) {
              return acc;
          }
          return sum(n - 1, acc + n);
      }
    \end{CListing}
    \caption{Original function code.}
  \end{sublisting}
  \hfill
  \begin{sublisting}[b]{0.49\textwidth}
    \begin{CListing}
      long sum(long n, long acc) {
      start:
          if (n == 0) {
              return acc;
          }
          // Copy recursive arguments to param vars
          long t0 = n - 1;
          long t1 = acc + n;
          n = t0;
          acc = t1;
          // Loop back to start of function
          goto start;
      }
    \end{CListing}
    \caption{Tail-call optimised function.}
  \end{sublisting}
  \caption{Example of transforming tail-recursion to iteration.}
  \label{lst:tail-call optimisation example}
\end{listing}


\subsection{Stack Allocation Policy}\label{sec:impl:optimised stack allocation}

My initial variable allocation policy is described in \ccref{sec:impl:local variable allocation} above.
Each variable is allocated a disjoint address range in memory.
However, this is very inefficient since most variables are temporary variables that are only used once, and many of them have non-overlapping define-to-use ranges (i.e.\ they never \emph{clash} in the context of liveness analysis).

Variables that never clash can safely be allocated to the same address range.
The challenge of this optimisation was to find a more optimal way of allocating variables such that as little memory is used as possible.

The optimised allocation is performed in the following steps:

\begin{Algorithm}{Optimised variable allocation.}{alg:optimised variable allocation}
\begin{EnumerateAlgorithm}
\item Remove dead variables.
\item Generate the instruction flowgraph.
\item Perform \acrlong{lva}.
\item Generate the clash graph.
\item Allocate variables from the clash graph.
\end{EnumerateAlgorithm}
\end{Algorithm}

\subsubsection{Live Variable Analysis}

\newcommand{\lvadef}{\ensuremath{\mathit{def}}}
\newcommand{\lvaref}{\ensuremath{\mathit{ref}}}

An instruction flowgraph is similar to the function call graph used in \ccref{sec:impl:unreachable procedure elimination} above, but on the level of individual instructions within a function.
Each node in the graph is a single instruction, and successors of a node are any instructions that can be executed as the next instruction along some execution path.
For example, branch instructions will have multiple successors.

The flowgraph is generated by recursing through the program blocks (the output of the Relooper algorithm, \ccref{sec:impl:relooper algorithm}), creating a node for each instruction, and creating edges along all possible control flow paths.

\Gls{lva} is run on the flowgraph to find where each variable is \emph{live}.
A variable is said to be \emph{live} (syntactically\footnote{Compared to \emph{semantic} liveness, which refers to actual possible execution behaviour, but is undecidable at compile time. Syntactic liveness is a safe overapproximation of semantic liveness.}) at an instruction if the value of the variable is used along any path in the flowgraph before it is redefined.

\Gls{lva} is a backwards analysis, which means that liveness information is propagated backwards through the flowgraph.
First, we define \lvadef\ and \lvaref\ sets for each instruction: $\lvadef\,(i)$ is the set of all variables defined by instruction $n$, and $\lvaref\,(n)$ is the set of all variables referenced by $n$.
We then define $\mathit{live}\,(n)$, the set of variables that are live immediately before instruction $n$, as follows:
\begin{align*}
  \mathit{live}\,(n) &= \left( \left( \bigcup_{s \,\in\, \mathit{succ}\,(n)} \mathit{live}\,(s) \right) \setminus \lvadef\,(n) \right) \cup \lvaref\,(n)
\end{align*}
We start with all variables live at successors of $n$, remove all variables that $n$ assigns to, and add any variables that $n$ references.
When $n$ assigns to a variable, it is no longer live for previous instructions, because any previous value has been overwritten.
When $n$ references a variable, it becomes live for previous instructions, until an instruction assigns to that variable.

To implement \gls{lva}, we keep iterating over every instruction in the flowgraph, applying the above equation to update the set of live variables, until there are no more changes (see \Ccref{lst:lva implementation}).
Initially the set of variables live at each instruction is the empty set.
The algorithm is guaranteed to find the smallest set of live variables (it does not add unnecessary overapproximations).

\begin{listing}[!t]
  \begin{RustListing*}{texcomments=true}
    // For every instr, which vars are live at that point
    type LiveVariableMap = HashMap<InstructionId, HashSet<VarId>>;
    fn live_variable_analysis(flowgraph: &Flowgraph) -> LiveVariableMap {
        let mut live: LiveVariableMap = LiveVariableMap::new();
        let mut changes = true;
        while changes {
            changes = false;
            for (instr_id, instr) in &flowgraph.instrs {
                // $\bigcup_{s \,\in\, \mathit{succ}\,(n)} \mathit{live}\,(s)$
                let mut out_live: HashSet<VarId> = HashSet::new();
                for successor in flowgraph.successors.get(instr_id) {
                    out_live.extend(live.get(successor).unwrap_or(&HashSet::new());
                }
                for def_var in def_set(instr) { // ${} \setminus \lvadef\,(n)$
                    out_live.remove(&def_var);
                }
                for ref_var in ref_set(instr) { // ${} \cup \lvaref\,(n)$
                    out_live.insert(ref_var);
                }
                // Update live variable set, and compare to previous value for changes
                let prev_live = live.insert(instr_id, out_live);
                match prev_live {
                    None => {
                        changes = true;
                    }
                    Some(prev_live_vars) => {
                        if prev_live_vars != out_live {
                            changes = true
                        }
                    }
                }
            }
        }
        live // Return the sets of live variables
    }
  \end{RustListing*}
  \caption{\Acrlong{lva} implementation.}%
  \label{lst:lva implementation}
\end{listing}

One complication of \gls{lva} I had to solve was that if a variable is assigned to but never subsequently referenced, it will never be marked as live by the analysis.
The next stage of the optimisation then marks it as having no clashes with any other variables.
However, the write to the variable may occur while other variable are live; and the compiler would happily allocate this variable in an overlapping location, producing incorrect results.
To solve this, I added an optimisation to remove dead variables before running \gls{lva}.
I made sure to keep any side-effect producing instructions (e.g.~function calls) while removing assignments to variables that are not subsequently accessed.

% Stop the LVA code float being pushed down
% \FloatBarrier % chktex 1

\subsubsection{Generating the Clash Graph}

The clash graph is generated from the results of \gls{lva}.
Any variables that are simultaneously live \emph{clash}; i.e.\ they cannot be allocated to overlapping addresses, because they are in use at the same time.
\Ccref{lst:clash graph generation} describes how the clash graph is generated.

The clash graph is an approximation whenever variable pointers are present.
When a variable has its address taken, the pointer may be passed around the program as a value, so it is no longer possible to track exactly where the variable is accessed.
Therefore, to ensure safety of the optimisation, we make each address-taken variable clash with every other variable\footnote{Pointer analysis may be able to improve upon this to reduce the set of clashes for address-taken variables, but cannot solve the problem entirely.}.
I implemented this by storing a set of `universal clashes' in the clash graph; when checking if two variables clash, if one of them is a `universal clash', they clash even if they would otherwise not.

\begin{listing}[t]
  \begin{RustListing}
    let live_vars = live_variable_analysis(flowgraph);
    for (_instr_id, vars_live_at_instr) in live_vars {
        // Add a clash between all vars simultaneously live
        while let Some(var) = vars_live_at_instr.pop() {
            for other_var in vars_live_at_instr {
                clash_graph.add_clash(var, other_var);
            }
        }
    }
    for instr in flowgraph.instrs {
        if let Instruction::AddressOf(_, _, var) = instr {
            clash_graph.add_universal_clash(var); // Over-approximate address-taken variables
        }
    }
  \end{RustListing}
  \caption{Algorithm to generate the clash graph from \gls{lva}.}
  \label{lst:clash graph generation}
\end{listing}

\subsubsection{Allocating Variables from the Clash Graph}

In the Part II Optimising Compilers Course, a register allocation heuristic was described which allocates variables in order of most clashes to least clashes.
When each variable is allocated, it is allocated a register that maximally overlaps with already allocated variables, while avoiding registers containing variables it clashes with.

My variable allocation problem is similar in many regards, however it has some key differences.
I am allocating variables in memory rather than to registers, so each variable occupies a byte \emph{range} rather than a single location.
Variables can have different sizes, and are not required to be aligned, so it is possible that variables may partially overlap (e.g.~$\texttt{t0} \mapsto [0, 4), \texttt{t1} \mapsto [2, 6)$). % chktex 9
There is also (effectively) no limit to the amount of memory available, compared to the very limited set of registers most compilers target.
The goal of my optimisation is to use as little memory as possible, whereas the goal of register allocation is to most efficiently use the constrained set of registers.

I modified the register allocation method to the following heuristic for variable allocation:

\begin{Algorithm}{Variable allocation heuristic.}{alg:optimised variable allocation heuristic}
\begin{EnumerateAlgorithm}
\item\label[algstep]{alg:allocate vars:start} Choose a variable with the least number of clashes. Break ties by choosing smaller variables.
\item Remove the variable and its edges from the clash graph.
\item\label[algstep]{alg:allocate vars:allocate} Allocate the variable to the lowest memory address where it doesn't clash with already allocated variables.
\item Repeat from \ccref{alg:allocate vars:start} until the clash graph is empty.
\end{EnumerateAlgorithm}
\end{Algorithm}

In \ccref{alg:allocate vars:allocate}, variables are always allocated to the lowest position on the stack possible (satisfying clash constraints).
This prioritises overlapping non-clashing variables at lower addresses; only variables that clash lots will be pushed to higher addresses.
We want to do this to keep the stack size as small as possible.

I tested allocating variables in both orders: least clashes first and most clashes first.
I found that contrary to register allocation, allocating variables with least clashes first resulted in more efficient memory use.
This is because of the preference to always allocate to lower addresses; this priority does not exist in the register allocation algorithm, since registers are all equal.
In my heuristic, it is beneficial to have variables with fewer clashes at lower addresses, because this allows more variables to overlap there, and fewer variables will be pushed to higher addresses.

\begin{figure}[!t]
  \begin{sublisting}[b]{0.5\textwidth}
    \begin{TextListing}
      |\IrCode{t0 = 3}|                 |$\{ \}$|
      |\IrCode{t1 = 2}|                 |$\{ \IrCode{t0} \}$|
      |\IrCode{t2 = t0 + t1}|           |$\{ \IrCode{t0}, \IrCode{t1} \}$|
      |\IrCode{t3 = t2 + t1}|           |$\{ \IrCode{t2}, \IrCode{t1} \}$|
    \end{TextListing}
    \caption{Intermediate code, with \acrlong{lva}.}
  \end{sublisting}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \scalebox{0.9}{\tikzfig{70-figures/11-clash-graph}}
    \caption{Clash graph.}
  \end{subfigure}
  \par\vspace{3ex}
  \begin{subfigure}[b]{0.5\textwidth}
    \ctikzfig{70-figures/09-naive-variable-allocation}
    \caption{Naive allocation policy.}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \ctikzfig{70-figures/10-optimised-variable-allocation}
    \caption{Optimised allocation policy.}
  \end{subfigure}
  \caption{Example of optimised stack allocation policy.}
  \label{fig:optimised var allocation example} % chktex 24
\end{figure}

\Ccref{fig:optimised var allocation example} shows an example of the impact this optimisation has.
In the top left is the intermediate code for which memory is being allocated.
The live variables at each instruction---calculated by \gls{lva}---are shows to the right of each line.
In the top right is the clash graph, showing which variables cannot be allocated to the same memory.
The bottom left shows how the naive allocation policy would allocate the variables in memory; sequentially and non-overlapping.
In the bottom right is the result of the optimised allocation policy: variables \texttt{t0}, \texttt{t2}, and \texttt{t3} all use the same memory location because they are never live at the same time.
Notice how all three variables use the lowest location, rather than, for example, \texttt{t3} overlapping with \texttt{t1}.
This is due to the heuristic of always allocating to the lowest valid address.


\section{Repository Overview}

I developed my project in a Git repository, ensuring to regularly push to the cloud for backup purposes.
The high-level structure of the codebase is shown below.
All the code for the compiler is in the \Dirname{src} directory.
The other directories contain the runtime environment code, skeleton standard library implementation, and tests and other tools.
All code was written by me.

\newlength\IndentWidth\setlength\IndentWidth{1em}
\NewDocumentCommand{\Indent}{m}{\hspace{#1\IndentWidth}}
\definecolor{TableHlineColour}{HTML}{ababab}

% Use `tabularx` when table doesn't span across pages, `xltabular` when it does
\begin{tabularx}{\textwidth}{lX}
\arrayrulecolor{TableHlineColour}
\hline % chktex 44
\Dirname{src} & Compiler source code. \\\hline % chktex 44
\Indent{1}\Dirname{program_config} & Compiler constants and run time options data structures. \\\hline % chktex 44
\Indent{1}\Dirname{front_end} & Lexer, parser grammar, \gls{ast} data structure. \\\hline % chktex 44
\Indent{1}\Dirname{middle_end} & \gls{ir} data structures, definition of intermediate instructions. Converting \gls{ast} to \gls{ir}. \\\hline % chktex 44
\Indent{2}\Dirname{middle_end_optimiser} & Tail-call optimisation and unreachable procedure elimination. \\\hline % chktex 44
\Indent{1}\Dirname{relooper} & Relooper algorithm. \\\hline % chktex 44
\Indent{1}\Dirname{back_end} & Target code-generation stage. \\\hline % chktex 44
\Indent{2}\Dirname{wasm_module} & Data structures to represent a WebAssembly module. \\\hline % chktex 44
\Indent{2}\Dirname{dataflow_analysis} & Flowgraph generation, dead code analysis, live variable analysis, clash graph. \\\hline % chktex 44
\Indent{2}\Dirname{stack_allocation} & Different stack allocation policies. \\\hline % chktex 44
% \Indent{1}\Dirname{data_structures} & Interval tree implementation that I ended up not using. \\\hline % chktex 44
\Indent{1}\Filename{preprocessor.rs} & C preprocessor. \\\hline % chktex 44
\Indent{1}\Filename{id.rs} & Trait for generating IDs used across the compiler. \\\hline % chktex 44
\Indent{1}\Filename{lib.rs} & Contains the main \texttt{run} function. \\\hline % chktex 44
\Dirname{runtime} & Node.js runtime environment. \\\hline % chktex 44
\Dirname{headers} & Header files for the parts of the standard library I implemented. \\\hline % chktex 44
\Dirname{tools} & Auxiliary scripts used for testing and generating graphs. \\\hline % chktex 44
\Indent{1}\Filename{profiler.py} & Plot stack usage profiles. \\\hline % chktex 44
\Indent{1}\Filename{testsuite.py} & Test runner script. \\\hline % chktex 44
\Dirname{tests} & Automated test specifications. \\\hline % chktex 44
% Reset the rule colour for future tables
\arrayrulecolor{black}
\end{tabularx}


\section{Summary}

The compiler pipeline is split into the front end, middle end, and back end.
The front end takes C source code, lexes it to a token stream, and parses it to generate an \gls{ast}.
The middle end uses a custom \gls{ir} to represent the program, performing the Relooper algorithm as well as doing tail-call optimisation and unreachable procedure elimination.
The back end transforms the \gls{ir} to a WebAssembly module, along with performing the stack allocation policy optimisation.
The WebAssembly binary is executed by a Node.js runtime environment, which provides an interface to the system and to my skeleton implementation of the C standard library.

\end{document} % chktex 17
