\documentclass[00-main.tex]{subfiles}

\begin{document}

\chapter{Implementation}
%\glsresetall % chktex 1

This chapter describes the implementation of each stage of the compiler.
\Ccref{fig:project flowchart} describes the high-level structure of the project, and subsequent sections explain in more detail.

Code snippets and figures are presented to supplement my explanations.
Where code is given, it is simplified to highlight the implementation details being explained.
This includes removing boilerplate code, error handling, and other features necessary for implementation but unhelpful for clarity.

\begin{figure}[!t]
  \centering
  \scalebox{0.8}{\tikzfig{70-figures/01-overview-flowchart}}
  \caption{Project structure, highlighting the preprocessor, front end, middle end, and back end. Each solid box represents a module of the project, transforming the input data representation into the output representation. The data representations are shown as dashed boxes. Where no new data representation is shown after a module, the output format is the same as the input format (i.e.~in the preprocessor). \emph{AST}: \acrlong{ast}; \emph{IR}: \acrlong{ir}; \emph{Wasm}: WebAssembly; \textit{\texttt{cpp}}: the GNU C Preprocessor.}%
  \label{fig:project flowchart}
\end{figure}


% The optimisations I implemented in the middle end were unreachable procedure elimination, and tail-call optimisation.
% As an extension to the project, I implemented a more optimal stack allocation policy, as part of the target code generation module.

\section{Front End}\label{sec:impl:front end}

% \begin{figure}[H]
%   \centering
%   \tikzfig{70-figures/02-front-end-overview-flowchart}
%   \caption{Structure of the compiler front end.}
%   \label{fig:front end flowchart}
% \end{figure}

The front end of the compiler consists of the preprocessor, the lexer, and the parser; it takes C source code as input and outputs an \gls{ast}.
I wrote a custom lexer, because this is necessary to support \CInline{typedef} definitions.
I used the LALRPOP parser generator to convert the tokens emitted by the lexer into an \gls{ast}, using an \gls{ast} representation I created~\ccite{lalrpop-docs}.

\subsection{Preprocessor}

I used the GNU C preprocessor (\mintinline{bash}{cpp}) to handle preprocessor directives in the source code, e.g.~macro definitions~\ccite{gnu-c-preprocessor}.
However, since I do not support linking, the compiler removes any \CInline{#include} directives before running the preprocessor, and handles them separately.

For each \CInline{#include <name.h>} directive that is removed, if it is one of the standard library headers that I support, the appropriate library code is copied into the source code from \Filename{headers/<name>.h}.
If the name of the header file matches the name of the source program, the contents of the program's own header file are inserted into the source code instead.

After processing \CInline{#include} directives, the compiler uses \mintinline{bash}{cpp} to handle remaining directives.

\subsection{Lexer}\label{sec:impl:lexer}

The grammar of the C language is mostly context-free, however the presence of \CInline{typedef} definitions makes it context-sensitive~\ccite{c-reference-manual-typedef-ambiguity}.
For example, the statement \CInline{foo (bar);} can be interpreted in two ways:
\begin{itemize}[nosep]
\item As a variable declaration, if \CInline{foo} has previously been defined as a type name\footnote{The brackets will be ignored.}.
\item As a function call, if \CInline{foo} is the name of a function.
\end{itemize}

This ambiguity is impossible to resolve with a context-free grammar. The solution is to process \CInline{typedef} names during the lexing stage, and emit distinct type name and identifier tokens to the parser.
Therefore, I created a custom lexer that stores the type names already defined at the current program point.

\begin{figure}[p]
  \hspace{2ex}
  \begin{subfigure}[t]{\textwidth-4ex}
    \centering
    \scalebox{0.85}{\tikzfig{70-figures/10-lexer-fsm/00-numbers}}
    \caption{\Gls{fsm} for lexing number literals, handling all number formats that C supports: decimal, floating point, binary, octal, and hexadecimal.}%
    \label{subfig:lexer fsm numbers}
  \end{subfigure}
  \par\vspace{2\bigskipamount}
  \begin{subfigure}[t]{\textwidth}
    \centering
    \scalebox{0.85}{\tikzfig{70-figures/10-lexer-fsm/01-identifiers}}
    \caption{\Gls{fsm} for lexing identifiers, handling language keywords and user-defined type names.}%
    \label{subfig:lexer fsm identifiers}
  \end{subfigure}
  \caption{
    \protect\glsreset{fsm}Excerpts from the lexer's \gls{fsm}.
    Named states represent a valid token, and blank states are invalid.
    Only valid transitions are shown, using regular expressions; if no transition exists for an input character, the end of the current token has been reached.
    Transitions without a prior state are the initial transitions for the first input character.
    The token is emitted if the machine stops in a valid (named) state; otherwise a lexing error is raised.
    In a slight abuse of regular expression notation, `\texttt{.}' represents a literal dot character.
    No input is consumed along an $\epsilon$~transition; the transition is taken if the condition is true. \\
    Tokens: \emph{Dec}:~decimal literal; \emph{FP}:~floating point literal; \emph{Bin}:~binary literal; \emph{Oct}:~octal literal; \emph{Hex}:~hexadecimal literal; \emph{Iden}:~identifier; \emph{Keyword}:~a C keyword; \emph{Typedef}\!:~a type name identifier. \\
    The states leading to the ellipsis token are shown for completeness since they share the starting dot state.
  }%
  \label{fig:lexer fsm}
\end{figure}


The lexer is implemented as an \gls{fsm}. \Ccref{fig:lexer fsm} highlights portions of the machine; the remaining state transition diagrams can be found in \ccref{app:lexer fsm}.
The \gls{fsm} consumes source code one character at a time, until the end of the current token is reached (i.e.\ there is no transition for the next input character).
Then, the token corresponding to the current state is emitted to the parser, and the machine starts lexing the next token.
% Some states have no corresponding token to emit because they occur part-way through parsing a token; if the machine finishes in one of these states, this raises a lexing error.
% (Every state labelled with a token is an accepting state of the machine.)
For tokens such as number literals and identifiers, the input characters are stored in the emitted token, giving the parser access to the necessary data about the token (e.g.~the value of a number literal).
% For tokens such as number literals and identifiers, the lexer appends the input character to a string buffer on each transition and when the token is complete, the string is stored inside the emitted token.
% This gives the parser access to the necessary data about the token, for example the name of the literal.

If, when starting to lex a new token, there is no initial transition matching the input character, then there is no valid token for this input. This raises a lexing error and the compiler exits.

% \Ccref{subfig:lexer fsm numbers} shows the transition diagram for lexing number literals, handling all the number formats that C supports: decimal, floating point, binary, octal, and hexadecimal.

\Ccref{subfig:lexer fsm identifiers} describes the transitions for lexing identifiers and \CInline{typedef} names.
This is where the ambiguity discussed above is handled.
  Every time another character of an identifier is consumed, the name is compared to language keywords and \CInline{typedef} names we have encountered so far\footnote{Keywords are matched with higher priority.}.
  If a match is found, the machine transitions to the corresponding state, along an $\epsilon$ transition\footnote{$\epsilon$ transitions consume no input.}.


\subsection{Parser}

I wrote an abstract grammar for C, and used the LALRPOP parser generator to generate parsing code from it~\ccite{lalrpop-docs}.
It generates an \gls{ast} using the representation I created.
Microsoft's \emph{C Language Syntax Summary} and \emph{C: A Reference Manual} were useful references to ensure I captured the subtleties of C's syntax~\ccite{c-language-grammar-microsoft,c-reference-manual}. % chktex 13
My grammar is able to parse all core features of the C language, whilst omitting some of the recent additions.
I chose to make my parser handle a larger subset of C than the compiler as a whole supports; the middle end rejects or ignores nodes of the \gls{ast} that it does not handle, such as storage class specifiers (\CInline{static}, etc.).
% For example, the parser handles storage class specifiers (e.g.~\CInline{static}) and type qualifiers (e.g.~\CInline{const}), and the middle end simply ignores them.

\subsubsection{Dangling Else Ambiguity}

A na\"{\i}ve grammar for C (\ccref{lst:ambiguous if-else grammar}) contains ambiguity around \CInline{if}/\CInline{else} statements~\ccite{c-reference-manual-dangling-else}.
C permits them to be written without curly brackets if the body is a single statement.
If we have nested \CInline{if}/\CInline{else} statements that do not use curly brackets, it can be ambiguous which \CInline{if} an \CInline{else} belongs to. This is known as the dangling else problem~\ccite{dangling-else-wiki}.

\ccref{lst:dangling else} shows an example of the dangling else problem; the \CInline{else} could either belong to the inner or the outer \CInline{if}.
% According to the grammar in \subref{lst:ambiguous if-else grammar}, there are two possible parses of this program.
% Either the \CInline{else} belongs to the inner or the outer \CInline{if} (\ccref{lst:dangling else:belonging to inner if} and \ccref{lst:dangling else:belonging to outer if}, respectively).


\begin{listing}[t]
  \begin{sublisting}[t]{\textwidth}
    \begin{GrammarListing}
      if-stmt      ::= "if" "(" expr ")" stmt
      if-else-stmt ::= "if" "(" expr ")" stmt "else" stmt
    \end{GrammarListing}
    \caption{%
      Ambigious \CInline{if}/\CInline{else} grammar.
      Terminals are represented in double quotes, and non-terminals in bold.
      The non-terminals \GrammarInline{stmt} and \GrammarInline{expr} represent statements and expressions, respectively, for which the rules are not shown.
    }
    \label[listing]{lst:ambiguous if-else grammar}
  \end{sublisting}
  \par\vspace{\bigskipamount}
  \begin{sublisting}[b]{0.25\textwidth}
    \begin{CListing}
      if (x)
        if (y)
          stmt1;
      else
        stmt2;
    \end{CListing}
    \caption{Ambiguous source code.}
    \label[listing]{lst:dangling else:ambiguous source}
  \end{sublisting}
  \hfill
  \begin{sublisting}[b]{0.35\textwidth}
    \begin{CListing}
      if (x) {
        if (y) {
          stmt1;
        } else {
          stmt2;
        }
      }
    \end{CListing}
    \caption{Parse: \CInline{else} belongs to inner \CInline{if}.}
    \label[listing]{lst:dangling else:belonging to inner if}
  \end{sublisting}
  \hfill
  \begin{sublisting}[b]{0.35\textwidth}
    \begin{CListing}
      if (x) {
        if (y) {
          stmt1;
        }
      } else {
        stmt2;
      }
    \end{CListing}
    \caption{Parse: \CInline{else} belongs to outer \CInline{if}.}
    \label[listing]{lst:dangling else:belonging to outer if}
  \end{sublisting}
  \par\smallskip
  \caption{Example of the dangling else problem in C. The C source code \subref{lst:dangling else:ambiguous source} could have two potential parse trees (\subref{lst:dangling else:belonging to inner if} and \subref{lst:dangling else:belonging to outer if}) when using the grammar in \subref{lst:ambiguous if-else grammar}.\bigskip}
  \label{lst:dangling else}
\end{listing}

C resolves the ambiguity by always associating the \CInline{else} with the closest possible \CInline{if}.
This can be encoded into the grammar using `open' and `closed' statements~\ccite{final-solution-to-dangling-else}.
\Ccref{lst:open/closed statement grammar} shows my modified grammar for \CInline{if}/\CInline{else} statements.
All other forms of statement must also be converted to the new structure.
% Basic statements (i.e.~with no sub-statements) are added to \GrammarInline{closed-stmt}.
% Statements that have sub-statements, e.g.~\CInline{while} statements, must be duplicated to both \GrammarInline{open-stmt} and \GrammarInline{closed-stmt}.

A closed statement always has the same number of \CInline{if} and \CInline{else} keywords (ignoring anything between a pair of brackets, since bracket matching is unambiguous).
Thus, in the second alternative of an \GrammarInline{open-stmt} in \ccref{lst:open/closed statement grammar}, the \CInline{else} terminal can be found by counting the number of \CInline{if}s and \CInline{else}s we encounter since the start of the \GrammarInline{closed-stmt}; the \CInline{else} belonging to the \GrammarInline{open-stmt} is the first \CInline{else} once we have more \CInline{else}s than \CInline{if}s.
If open statements were allowed inside the \CInline{if} clause of an \GrammarInline{open-stmt}, then the grammar would be ambiguous, because this method for finding the matching \CInline{else} would not work.


\begin{listing}[t]
  \begin{GrammarListing}
    stmt        ::= open-stmt | closed-stmt

    open-stmt   ::= "if" "(" expr ")" stmt
                  | "if" "(" expr ")" closed-stmt "else" open-stmt
                  | ...

    closed-stmt ::= "if" "(" expr ")" closed-stmt "else" closed-stmt
                  | ...
  \end{GrammarListing}
  \caption{Using open and closed statements to solve the dangling else problem. Other statement rules with no sub-statements are added to \GrammarInline{closed-stmt}. Rules with sub-statements, e.g.~rules for \CInline{for} and \CInline{while}, are duplicated across \GrammarInline{open-stmt} and \GrammarInline{closed-stmt}, having open and closed sub-statements, respectively.\medskip}
  \label{lst:open/closed statement grammar}
\end{listing}

\subsubsection{LALRPOP Parser Generator}

\Ccref{lst:AST generation code example} is an example of the LALRPOP syntax for addition expressions.
The left-hand side of the~\RustInline{=>} describes the grammar rule, and the right-hand side generates the corresponding \gls{ast} node.
Terminals are represented in double quotes; these map to the tokens emitted by the lexer.
Non-terminals are represented inside angle brackets, containing their type and a variable name to bind to in \gls{ast} generation code.

\begin{listing}[t]
  \begin{sublisting}[t]{\textwidth}
    \begin{GrammarListing}
      additive-expression ::= additive-expression "+" multiplicative-expression
    \end{GrammarListing}
    \caption{The grammar rule for addition expressions.}
    \label{sublst:addition expr grammar rule}
  \end{sublisting}
  \par\medskip % vertical space between subfigures
  \begin{sublisting}[t]{\textwidth}
    \begin{RustListing}
      AdditiveExpression: ast::Expression = {
          <e1:AdditiveExpression> "+" <e2:MultiplicativeExpression>
              => ast::Expression::BinaryOp(
                  ast::BinaryOperator::Add,
                  e1,
                  e2
              ),
          ...
      };
    \end{RustListing}
    \caption{The LALRPOP syntax for the addition grammar rule.}
    \label{sublst:lalrpop syntax addition rule}
  \end{sublisting}
  \caption{%
    Example of the LALRPOP syntax for grammar rules.
    \subref{sublst:addition expr grammar rule} is the rule for an addition expression, and~\subref{sublst:lalrpop syntax addition rule} is the Rust code to represent it.
    In LALRPOP, the grammar parsing and \gls{ast} generation code are combined.\bigskip
  }
  \label{lst:AST generation code example}
\end{listing}

LALRPOP allows macros to be defined, which allow the grammar to be written more intuitively and concisely.
For example, I defined a macro to represent a comma-separated list of non-terminals~(\ccref{lst:parser macro for comma-separated list}).

\begin{listing}[t]
  \begin{RustListing}
    CommaSepList<T>: Vec<T> = {
        <mut v:(<T> ",")*> <e:T> => {
            v.push(e);
            v
        }
    };
  \end{RustListing}
  \caption{LALRPOP macro to parse a comma-separated list of non-terminals. It takes a generic type \RustInline{T} for the type of the non-terminals, and returns the list items as a \RustInline{Vec<T>}.}
  \label{lst:parser macro for comma-separated list}
\end{listing}

% \subsubsection{String Escape Sequences}
%
% The parser has to handle escape sequences in strings.
% I implemented this by first creating an iterator over the characters of a string, which replaces escape sequences by the character they represent as it emits each character.
% When the current character is a backslash, instead of emitting it straight away, the iterator consumes the next character and emits the character corresponding to the escape sequence.
% I wrapped this in an \RustInline{interpret_string} function that internally creates an instance of the iterator and collects the emitted characters back to a string.

% \subsubsection{Parsing Type Specifiers}
%
% Another feature of the C language is that type specifiers (\CInline{int}, \CInline{signed}, etc.) can appear in any order before a declaration.
% For example, \CInline{signed int x} and \CInline{int signed x} are equivalent declarations.
% To handle this, my parser first consumes all type specifier tokens of a declaration, then constructs an \RustInline{ArithmeticType} \gls{ast} node from them.
% It uses a bitfield where each bit represents the presence of one of the type specifiers in the type.
% The bitfield is the normalised representation of a type; every possible declaration that is equivalent to a type will have the same bitfield.
% The declarations above would construct the bitfield \RustInline{0b00010100}, where the two bits set represent \CInline{signed} and \CInline{int} respectively.
% For each type specifier, the corresponding bit is set.
% Then, the bitfield is matched against the possible valid types, to assign the type to the \gls{ast} node.

\subsubsection{Additional Parser Details}

The parser contains other features that warrant discussion and more code snippets.
The handling of escape sequences in strings is described in~\ccref{app:sec:parsing string escape sequences}, and subtleties around parsing type specifiers are discussed in~\ccref{app:sec:parsing type specifiers}.

\section{Middle End}\label{sec:impl:middle end}

The middle end takes the \gls{ast} as input and transforms it to intermediate code, in the custom \gls{ir} I created.
It also runs the Relooper algorithm on the \gls{ir}.
The optimisations performed in this stage are described in \ccref{sec:impl:optimisations}.
The middle and back ends make use of a context object design pattern, discussed in \ccref{app:sec:context object design pattern}.

\subsection{Intermediate Code Generation}

I defined a custom three-address code \gls{ir} (the instructions are listed in \ccref{app:intermediate code}).
Creating a custom instruction set instead of using an existing one allowed me to tailor my compiler with exactly the features I support, such as not including indirect function calls.
I also added instructions specific to the Relooper algorithm.
The \gls{ir} contains both the program instructions and necessary metadata, such as type information, the mapping between variable names and their IDs in the \gls{ir},~etc.
The instructions and metadata are contained in separate substructures within the main \gls{ir} data structure, enabling the metadata to be carried forwards through stages of the compiler pipeline while the instructions are transformed at each stage.
% In the stage of converting the \gls{ast} to \gls{ir} code, the instructions and metadata are encapsulated in the \RustInline{Program} struct.

Many objects in the middle end require unique IDs; generating these is discussed in~\ccref{app:sec:generating IDs}.

To convert the \gls{ast} to \gls{ir} code, the compiler recursively traverses the tree, generating three-address code instructions and metadata as it does so.
At the highest level, the \gls{ast} contains a list of statements.
Each statement is pattern-matched to handle it according to its structure, recursing into sub-statements and expressions.
The case for a \CInline{while} statement is shown in \ccref{lst:convert while stmt to IR}; labels and branch instructions are added, and the instructions to evaluate the condition and body are generated recursively.

% For each of these, the compiler calls \RustInline{convert_statement_to_ir(stmt, program, context)}, where \RustInline{program} is the mutable \gls{ir}, to which instructions and metadata are added, and \RustInline{context} is the context object described in \ccref{app:sec:context object design pattern}, containing relevant contextual information~(\ccref{lst:convert while stmt to IR}).

% The core of converting a statement or expressions to \gls{ir} code is pattern matching the \gls{ast} node, and generating \gls{ir} instructions according to its structure, recursing into sub-statements and expressions.
% Other control-flow structures are similar.



\begin{listing}[t]
  \begin{RustListing}
    fn convert_statement_to_ir(stmt: Statement, prog: &mut Program, context: &mut Context)
            -> Vec<Instruction> {
        let mut instrs = Vec::new();
        match stmt {
            Statement::While(cond, body) => {
                let loop_start_label = prog.new_label(); // Create labels for start/end of loop
                let loop_end_label = prog.new_label();
                // Create a new loop context
                context.push_loop(LoopContext::while_loop(loop_start_label, loop_end_label));
                instrs.push(Instruction::Label(loop_start_label));
                // Evaluate loop condition
                let (cond_instrs, cond_var) = convert_expression_to_ir(cond, prog, context);
                instrs.append(cond_instrs);
                // Branch out of loop if condition is false
                instrs.push(Instruction::BrIfEq(cond_var, Constant::Int(0), loop_end_label));
                // Recursively convert loop body
                instrs.append(convert_statement_to_ir(body, prog, context));
                // Branch back to the start of the loop
                instrs.push(Instruction::Br(loop_start_label));
                instrs.push(Instruction::Label(loop_end_label));
                // Remove the loop context we added
                context.pop_loop();
            }
            // ... other AST statement nodes ...
        }
        instrs
    }
  \end{RustListing}
  \caption{\protect\glsreset{ast}%
    Generating intermediate code for a \CInline{while} statement. The \gls{ast} is pattern matched to handle each type of statement separately. The other cases are not shown here.
  }
  \label{lst:convert while stmt to IR}
\end{listing}

Some statements required care to ensure the semantic meaning of the program was preserved.
For example, \CInline{switch} statements can contain \CInline{case} blocks in any order.
Some blocks may ``fall through'' to the next block implicitly, and there may be a \CInline{default} block.
I handled this by first generating instructions for each \CInline{case} block, and storing them in a `switch context' data structure.
Once all \CInline{case} statements have been seen, the \gls{ir} code is generated, according to the structure in~\ccref{fig:instr structure for switch statements}.

% At the start of the \CInline{switch} statement, conditional branches to each \CInline{case} block are inserted.
% If a \CInline{default} block is present, an unconditional branch to it is added after the branches to the \CInline{case} blocks (otherwise, a branch to the end of the \CInline{switch} statement is added, for if no cases match).
% After these come the blocks themselves.
% This ensures there is no direct fall-through between any blocks; instead, all blocks end in a branch instruction either to the start of the next block or the end of the \CInline{switch} statement.
% \Ccref{fig:instr structure for switch statements} shows the structure of the generated instructions for \CInline{switch} statements.

\begin{figure}[t]
  \centering
  \scalebox{0.8}{\tikzfig{70-figures/05-switch-case-handling}}
  \caption{%
    \gls{ir} instructions generated for \CInline{switch} statements.
    Firstly, conditional branches to each \CInline{case} block are inserted.
    This is followed by an unconditional branch to the \CInline{default} block, if present; if not, it branches to the end of the \CInline{switch} statement.
    Following the branches are all the \CInline{case} blocks themselves. \\
    This structure ensures there is no direct fall-through between any \CInline{case} blocks; instead, all blocks end in a branch instruction either to the start of another block or the end of the \CInline{switch} statement.
  }
  \label{fig:instr structure for switch statements} % chktex 24
\end{figure}

Declaration statements have many different formats, each of which I had to handle separately; e.g.~variables may be declared with or without being initialised with a value.
Function declarations are also a type of declaration handled here, unlike in some languages where they differ syntactically.

% Syntactically speaking, function declarations (without a body) are identical to variable declarations; the only difference being that they declare the identifier as a function type.
% Therefore they are handled in the same place.
% For function definitions (i.e.~declarations plus body), the body code is converted to intermediate code and a new function object is added to the \gls{ir}.

Array declarations are complicated because their length can be specified in multiple ways.
It can be explicitly specified in the declaration, or implicitly inferred from the length of the initialiser.
A variable-length array is when the array length is set at run-time by a variable; to handle these, an instruction is inserted to allocate memory for it at run-time.
Array and struct initialisers are handled by first allocating memory for the variable, then storing the value of each of the inner members.

Some expressions are evaluated at compile-time, including array length expressions.
My implementation of this is described in~\ccref{app:sec:compile time expression evaluation}.

% \subsection{Context Object Design Pattern}
%
% Throughout the middle and back ends, I used a design pattern of passing a context object through all function calls.
% For example, when traversing the \gls{ast} to generate \gls{ir} code, the \RustInline{Context} struct in \ccref{lst:AST to IR context struct} was used to track information about the current context with respect to the source program.
% It tracks the stack of nested loops and switch statements, so that when a \CInline{break} or \CInline{continue} statement is converted, the branch target is known.
%
% \begin{listing}[t]
%   \begin{RustListing}
%     struct Context {
%         loop_stack: Vec<LoopOrSwitchContext>,
%         scope_stack: Vec<Scope>,
%         in_function_name_expr: bool,
%         function_names: HashMap<String, FunId>,
%         directly_on_lhs_of_assignment: bool,
%     }
%   \end{RustListing}
%   \caption{The context data structure used when converting the \gls{ast} to \gls{ir} code.}
%   \label{lst:AST to IR context struct}
% \end{listing}
%
% In an object-oriented language, this would often be achieved by encapsulating the methods in an object and using private state inside the object.
% Rust, however, is not object-oriented, and I found this approach offered more modularity and flexibility.
% Firstly, the context information is encapsulated inside a separate data structure, allowing methods to be implemented on it that give calling functions access to the exact context information needed.
% It also enables creating different context objects for different purposes.
% In the target code generation stage, the \RustInline{ModuleContext} stores information about the entire module being generated, whereas the \RustInline{FunctionContext} is used for each individual function being converted.
% The \RustInline{FunctionContext} has a shorter lifetime than the \RustInline{ModuleContext}, hence separating the data structures is ideal.

\subsection{Types}

\Ccref{fig:ir supported types} outlines the types supported by the \gls{ir}, mirroring the types supported by the C language~\ccite{c-reference-manual-types}.
% \IrType{Ux} and \IrType{Ix} types represent unsigned and signed $x$-bit integers, respectively.
Enumeration types (enums) are supported; their values are encoded as \IrType{U64}s.
I followed the standard implementation convention for the bit size of \CInline{char}s, \CInline{short}s, \CInline{int}s, and \CInline{long}s; 8, 16, 32, and 64 bits, respectively\footnote{The C specification does not define the exact bit widths, only the minimum size.}.

\begin{figure}[t]
  \setlength{\abovedisplayskip}{-6pt}
  \setlength{\belowdisplayskip}{-20pt}
  \begin{align*}
    \textit{T} ={} &\IrType{I8} \vertpad \IrType{U8} \vertpad \IrType{I16} \vertpad \IrType{U16} \vertpad \IrType{I32} \vertpad \IrType{U32} \vertpad \IrType{I64} \vertpad \IrType{U64} \vertpad \IrType{F32} \vertpad \IrType{F64} \vertpad \IrType{Void} \\
    \vertpad &\IrType{Struct}(T[]) \vertpad \IrType{Union}(T[]) \\
    \vertpad &\IrType{Pointer}(T) \vertpad \IrType{Array}(T, size) \\
    \vertpad &\IrType{Function}(T, T[], is\_variadic) \\
  \end{align*}
  \caption{Supported types in the \gls{ir}. \IrType{Ix} and \IrType{Ux} types represent signed and unsigned integers, respectively, of bit-width $x$. \IrType{Fx} types represent floating point types of bit-width $x$.}%
  \label{fig:ir supported types}
\end{figure}

I implemented the standard C unary and binary conversions for types~\ccite{c-reference-manual-usual-conversions}.
They are applied before unary and binary operations, respectively.
Unary conversion constrains the possible types that an operand can have. Smaller integer types are widened to 32 bits, and $\IrType{Array}(T, \_)$ types are converted to $\IrType{Pointer}(T)$.
After applying the unary conversions to each operand individually, binary conversions ensure that both operands have the same type.
If both operands are arithmetic, and one operand has a smaller type than the other, the smaller is converted to the larger type.
This includes promoting integers to float types.

When transforming each \gls{ast} node to intermediate code, the compiler checks that the types of the operands are valid (after unary/binary conversion) and stores the corresponding type of the result variable.
For example, before generating the instruction \IrCode{t = a < b}, the compiler checks that \IrCode{a} and~\IrCode{b} are comparable arithmetic types---if not, a compile error is thrown---and sets the type of \IrCode{t} to~\IrType{I32}\footnote{\IrType{I32} is used to represent booleans.}.
This guarantees that the \gls{ir} passed to the back end is type safe.

\subsection{The Relooper Algorithm}\label{sec:impl:relooper algorithm}

I now explain implementation-specific details of the Relooper algorithm, described in \ccref{sec:prep:relooper}.
It takes the \gls{ir} as input, and transforms the instructions into Relooper blocks whilst preserving program metadata.

Firstly, the intermediate code is `soupified': splitting it up into a `soup of blocks'.
The linear sequence of instructions is transformed into a set of \emph{labels}\footnote{The term `label' is overloaded: a `label instruction' refers a single instruction marking a point in the code, whereas a `label' is a basic block that starts with a label instruction.} (basic blocks), which are the input to the actual Relooper algorithm.
Each label starts with a \IrCode{label} instruction and ends in a \IrCode{branch}.
\ccref{alg:soupify} shows the process.
\Ccrefrange{alg:soupify:remove label fall-through}{alg:soupify:merge consecutive labels} transform the instruction sequence until it can be directly split into labels.
All control flow between labels is made explicit.

\begin{AlgorithmFloat}[t]
  \addtolength{\belowcaptionskip}{\medskipamount}
  \begin{Algorithm}
    \begin{EnumerateAlgorithm}
    \item\label[algstep]{alg:soupify:remove label fall-through}
      Make all transitions between blocks explicit.
      If a label instruction is not preceded by a branch instruction, a new branch instruction is inserted before it that branches to that label.
      % By this I mean any label instruction that is not preceded by a branch in the linear instruction sequence, to which control will flow directly from the previous instruction.
      % A branch instruction is inserted before each label instruction, if one does not already exist, branching to that label.
      % This ensures that a label instruction is always preceded by a branch instruction along every control flow path.

      This will generate many redundant branch instructions, however they are removed when the Relooper blocks are transformed to target code.

    \item
      Insert an unconditional branch after each conditional branch, to ensure that conditional branches do not occur in the middle of a block.
      A new label instruction is created for the unconditional branch, if necessary.

      \begin{center}
        \small
        \pbox[t]{.5\textwidth}{
          \IrCode{br <l1> if ...} \\ % chktex 26 chktex 11
          \IrCode{br <l2>} \\
          \IrCode{label <l2>} \\
          \IrCode{...} % chktex 11
          }
          \hspace{1em}
          \pbox[t]{.5\textwidth}{
            \CodeComment{Original conditional branch} \\
            \CodeComment{New unconditional branch} \\
            \CodeComment{New label \IrCode{<l2>}}
            }
      \end{center}

    \item
      If there is no label instruction at the very start of the instructions, add one.

    \item\label[algstep]{alg:soupify:merge consecutive labels}
      Merge any consecutive label instructions into a single instruction, updating branches to the labels accordingly.

    \item
      Finally, divide the instructions into blocks by passing through the instructions sequentially and starting a new block at each label instruction.
    \end{EnumerateAlgorithm}
  \end{Algorithm}
  \caption{`Soupifying' the intermediate code. The input is a linear instruction sequence, and a set of label blocks is produced.\medskip}%
  \label[algorithm]{alg:soupify}
\end{AlgorithmFloat}

After soupifying, \ccref{alg:relooper} (\ccref{sec:prep:relooper}) is used to create a Relooper block.
In the algorithm, the \emph{reachability} set of a label is used to determine which type of block to generate.
The reachability of each label is calculated as the transitive closure of its possible branch targets (\ccref{alg:reachability}).
Subsequent steps of the algorithm are implemented as described earlier.

\begin{AlgorithmFloat}[!t]
  \begin{PseudocodeListing}
    \Function{CalculateReachability}{\textit{label}}
      \State \textit{reachability} $\gets$ possible branch targets of \textit{label}
      \While{\textit{reachability} changes}
        \For{$\ell$ in \textit{reachability}}
          \State \textit{reachability} += possible branch targets of $\ell$
        \EndFor
      \EndWhile
    \EndFunction
  \end{PseudocodeListing}
  \caption{Calculating the reachability of a label. Reachability is stored as a set so that no duplicates are added.}%
  \label[algorithm]{alg:reachability}
\end{AlgorithmFloat}

\FloatBarrier % chktex 1

\section{Back End: Target Code Generation}\label{sec:impl:back end}

In the back end, I defined data structures to directly represent a WebAssembly module with its sections, all possible WebAssembly instructions, and value types.
% I also defined a \RustInline{WasmInstruction} enum to represent all possible WebAssembly instructions, and data structures to represent value types.
The back end generates a Web\-Assembly module, and its byte representation is written to a binary file as output.
% The back end generates a \RustInline{WasmModule} containing \RustInline{WasmInstruction}s, and its byte representation is written to a binary file as output.
I defined a \RustInline{ToBytes} trait that is implemented by all these data structures, returning a vector of bytes.
It provides a layer of abstraction between the back end transformations and the actual byte values in the binary.

\par\nobreak\vfil\penalty0\vfilneg % chktex 1
   \vtop\bgroup % chktex 1
The core function of the back end is to transform \gls{ir} instructions to WebAssembly instructions.
Since WebAssembly is a stack-based architecture, the general pattern for converting an instruction is:
\begin{itemize}[nosep]
\item Push the value of each operand onto the stack (loading variables from memory).
\item Perform the operation, leaving the result on the stack.
\item Store the result back to a variable in memory.
\end{itemize}
\par\xdef\tpd{\the\prevdepth}\egroup % chktex 1
   \prevdepth=\tpd % chktex 1
Most instructions are a variation of this pattern.
\Ccref{lst:converting add instr to wasm code} shows the code generated for an add instruction.
One subtlety is that \WasmInstr{store} instructions take the memory address as their first operand, and the value to store as their second operand.
This means that the address of the destination variable needs to be pushed onto the stack \emph{before} the source operands are loaded and the operation is performed.

\begin{listing}[t]
  \begin{sublisting}[b]{0.28\textwidth}
    \begin{TextListing}
      |\IrCode{c = a + b}|
    \end{TextListing}
    \caption{Intermediate code.}
  \end{sublisting}
  \hfill
  \begin{sublisting}[b]{0.7\textwidth}
    \begin{WasmListing}
      i32.const |<addr of c>|  ;; address for the store instruction
      i32.const |<addr of a>|
      i64.load               ;; load a onto the stack
      i32.const |<addr of b>|
      i64.load               ;; load b onto the stack
      i64.add                ;; perform operation on a and b
      i64.store              ;; store result from top of stack
    \end{WasmListing}
    \caption{Generated WebAssembly code.}
  \end{sublisting}
  \caption{\Gls{ir} code and generated target code for transforming an add instruction, assuming \IrCode{a} and \IrCode{b} are variables of type \IrType{I64}.}
  \label{lst:converting add instr to wasm code}
\end{listing}

I defined \RustInline{load} and \RustInline{store} helper functions that take the \gls{ir} type of a variable and return correctly typed load and store instructions, respectively.
This significantly reduces the amount of code repetition in the back end.
% The \RustInline{store} function encapsulates the fact that the address operand has to come before the value to store; this helps to ensure correctness as it is only defined in one place.

% I used the same ID generator pattern as in the middle end, to generate unique indexes for items in the WebAssembly module, such as functions and types.

\vspace{-6pt}
\subsection{Memory Layout}

The back end generates code that manages the memory layout, including the function call stack: pushing new stack frames when functions are called, and popping them when functions return.
\Ccref{fig:memory structure} shows the memory layout I defined.

\glsreset{fp}\glsreset{sp}

The first section of memory contains the \gls{fp} and \gls{sp}.
It also contains a `temporary frame pointer', which is used in intermediate steps of manipulating stack frames to hold the \gls{fp}.
In a register architecture, these pointers would be stored in registers, however WebAssembly has no registers, so instead I allocate them at known memory locations.

The next section of memory contains any string literals used in the program.
In C, a string literal is a null-terminated character array.
My \gls{ir} has an instruction that returns a pointer to the given string literal (\IrCode{t = \&<str_id>}).
% , in the back end, is converted to an instruction that pushes the address of the corresponding string onto the stack.
All string literals are allocated at compile-time.

Global variables and program arguments are stored in the next region of memory, before the call stack.
Global variables are allocated in the same way as local variables (see \ccref{sec:impl:local variable allocation}).
Program arguments are stored by the runtime environment~(see \ccref{sec:impl:runtime}).

The function call stack grows upwards dynamically from this point.
I did not implement heap storage, so memory only increases from one end, unlike in standard C compilers such as GCC~\ccite{gcc}.

\begin{figure}[t]
  \centering
  \scalebox{0.9}{\tikzfig{70-figures/06-memory-layout}}
  \caption{\protect\glsreset{fp}\protect\glsreset{sp}%
    Memory structure, containing the \gls{fp}, temporary \acrlong{fp}, and \gls{sp}, followed by other sections of memory.
    Addresses increase to the right; compile-time known memory addresses are labelled above.
    \bigskip
  }%
  \label{fig:memory structure}
\end{figure}

\subsection{Stack Frame Operations}

Whenever a function is called, a new stack frame is constructed at the top of the stack in memory.
\Ccref{fig:stack frame layout} shows the frame structure.

In a WebAssembly module, the code is split up into functions.
Since there is no arbitrary jump instruction~(c.f.~\ccref{sec:prep:relooper}), function calls must be implemented using the WebAssembly \WasmInstr{call} instruction.
This contrasts to other instruction sets, where function calls are implemented as jumps within a single linear instruction sequence.
At run-time, WebAssembly has its own call stack, as well as the call stack maintained by the compiler.
Arguments and return values are passed via the compiler's call stack.

\begin{figure}[!t]
  \centering
  \scalebox{0.9}{\tikzfig{70-figures/07-stack-frame-layout}}
  \caption{\protect\glsreset{fp}\protect\glsreset{sp}%
    Memory layout of a stack frame, showing where the \gls{fp} and \gls{sp} point to.
    Addresses increase upwards; compile-time known offsets from the \gls{fp} are labelled.
  }%
  \label{fig:stack frame layout}
\end{figure}

The procedure for pushing a new stack frame is \ccref{alg:pushing new stack frame}:

\begin{AlgorithmFloat}[H]
  \begin{Algorithm}
    \begin{EnumerateAlgorithm}
    \item
    Store the current \gls{fp} at the top of the stack.
    \item
    Copy the current \gls{sp} to Temp \gls{fp}: the address of the start of the stack frame.
    \item
    Allocate space for the return value on top of the stack, according to the return type.
    \item
    Store each function parameter on top of the stack.
    This is either copying a variable from the stack frame below or storing a constant.
    \item
    Set the \gls{fp} to the value held in Temp \gls{fp}.
    \end{EnumerateAlgorithm}
  \end{Algorithm}
  \caption{Pushing a new stack frame.}%
  \label[algorithm]{alg:pushing new stack frame}
\end{AlgorithmFloat}

Using the temporary \gls{fp} here is necessary since overwriting the \gls{fp} directly would prevent copying local variables in as parameters.
(Local variables have \gls{fp}-relative addresses).
We also cannot wait to save the new \gls{fp} address until after we have copied the variables, because by then the \gls{sp} has been moved and no longer points to the start of the stack frame.
(The \gls{sp} is updated every time a value is stored on top of the stack).

Popping a stack frame is simpler:

\begin{AlgorithmFloat}[H]
  \begin{Algorithm}
    \begin{EnumerateAlgorithm}
    \item
    Set the \gls{sp} to the current value of the \gls{fp}.
    The new top of the stack is the top of the previous stack frame.
    \item
    Restore the previous value of the \gls{fp}.
    \item
    If the function returns a result to a variable, copy the return value from the stack frame to the destination variable.
    \end{EnumerateAlgorithm}
  \end{Algorithm}
  \caption{Popping a stack frame.}%
  \label[algorithm]{alg:popping stack frame}
\end{AlgorithmFloat}

We can pop the frame simply by moving the \gls{sp} because the data above the \gls{sp} will never be read; when the stack grows again, it will be overwritten.

I defined caller/callee conventions for managing the stack.
The caller is responsible for constructing the new frame, and deallocating it once the function returns.
% with the previous \gls{fp} value, space for the return address, and any function parameters.
% The caller also deallocates the stack frame once the function returns.
The callee is responsible for allocating space for its local variables on the stack.


\subsection{Local Variable Allocation}\label{sec:impl:local variable allocation}

Initially, I implemented a naive variable allocation strategy.
As an extension, I created a more optimal allocation strategy, described in \ccref{sec:impl:optimised stack allocation}.

Local variables are stored as part of the function stack frame, the address of which is run-time dependent.
Therefore, variables are accessed as offsets from the \gls{fp}, since we know their position within the stack frame.
% At run-time, a variable is accessed by first loading the \gls{fp}, then adding the variable offset to get the variable's memory address.

Variable allocation is performed at compile-time so that variable accesses can be static \gls{fp} offsets rather than requiring a run-time address table.
Only in specific cases---variable-length arrays, for example---are variables allocated at run-time.
My \gls{ir} has a dedicated \IrCode{AllocateVariable} instruction that takes the number of bytes to allocate as an operand, and returns a pointer to the allocated memory.
This is allocated at the top of the stack, above all compile-time allocated variables.

Each variable, including temporary variables, is allocated a dedicated byte range in memory; no variables overlap.
This safely maintains the programmer's memory model, but is an inefficient use of memory, hence the later optimisation.

\section{Runtime Environment}\label{sec:impl:runtime}

The purpose of the runtime environment is to provide an interface between the WebAssembly binary and the system, thus allowing it to be executed.
I chose to use Node.js, which allows the binary to run locally~\ccite{nodejs}.
Another option was to use JavaScript within a web browser; however, for the purposes of this project it would have added unnecessary complexity.

The runtime is responsible for instantiating the WebAssembly module, which includes passing in imports.
This must happen before any of its functions can be executed.
The runtime creates a new linear memory for the module to use, and also imports my skeleton library functions.
The memory is created by the runtime so that the standard library functions can access it.
If the memory were created by the WebAssembly module, the imported functions would not be able to reference it.

Once the module has been instantiated, the runtime stores the command-line program arguments into memory.
Following the C standard, the first argument is always the name of the program being run.
Arguments are passed to the \CInline{main} function as an array of \CInline{char} pointers (\CInline{argv}), plus the argument count (\CInline{argc}).
To facilitate this, the runtime first allocates space for the array of pointers, after the global variables.
After the pointer array, the actual argument values are stored (as strings), and each corresponding pointer is set (\ccref{fig:program args memory structure}).
The runtime then sets the \gls{sp} to immediately after the last argument.

\begin{figure}[t]
  \centering
  \scalebox{0.9}{\tikzfig{70-figures/08-program-args-memory-layout}}
  \caption{\protect\glsreset{sp}%
    Memory structure of program arguments, with addresses increasing to the right. Here, the argument count (\CInline{argc}) is 2.
    The pointer to the start of the array (\CInline{argv}) and the \gls{sp} are labelled.
  }%
  \label{fig:program args memory structure}
\end{figure}

Finally, the runtime calls the exported \JSInline{main} function, with the values of \JSInline{argc} and \JSInline{argv} as parameters.

\section{Optimisations}\label{sec:impl:optimisations}

After completing the main compiler pipeline, I implemented optimisations in the middle and back ends.
I performed unreachable procedure elimination and tail-call optimisation in the middle end, and in the back end I created a more optimal variable allocation policy.

\subsection{Unreachable Procedure Elimination}\label{sec:impl:unreachable procedure elimination}

Unreachable procedure elimination removes all functions that are never called.
I do not support function pointers; all function calls are direct.
Therefore, if a function is never referenced by any \IrCode{call} instruction, it is guaranteed to never be called.

Firstly, the call graph is generated.
This is a directed graph where nodes are functions in the program and edges represent \IrCode{call} instructions.
It also stores a set of \emph{entry} nodes, which are functions that are called from the global scope of the program (e.g.~\CInline{main()}). % chktex 36
To maintain correctness, the constructed graph is a superset of the \emph{semantic} call graph, which contains all function calls that can happen along a real execution path.
The semantic graph is undecidable at compile-time\footnote{Due to the Halting Problem, it is not possible to decide which control flow paths will be taken for every program. However, more sophisticated analyses can be done to improve the approximation.}, so we calculate the syntactic call graph.

% I used an adjacency list to store the call graph, since it is likely to be sparse.
% (An adjacency matrix would be more suitable for densely connected graphs.)

% To generate the graph, all functions in the \gls{ir} are added as nodes, then an edge is added for every \IrCode{call} instruction.
% For a \IrCode{call y} instruction found inside function \IrCode{x}, a directed edge is added from node \IrCode{x} to node \IrCode{y}.
% Any functions that are called from the global scope, as well as the \CInline{main} function, are added to the set of entry nodes.

Once the call graph is constructed, it is used to find functions that are never called.
Firstly, every function is marked as unused.
I then implemented a \gls{bfs} over the graph, starting from each entry node.
All nodes reached are marked as used.
Once the search returns, all nodes still marked as unused can be safely removed from the program.

The benefit of using a \gls{bfs} rather than simply looking for nodes with no incoming edges is that it can handle cycles.
A program may contain cycles of functions that call each other but that are never called from the rest of the program, so the cycle is never entered.
In this case, the nodes would have incoming edges, but would not be reached in a \gls{bfs}, allowing them to be removed.


\subsection{Tail-Call Optimisation}\label{sec:impl:tail-call optimisation}

When a function is called recursively, a new stack frame is pushed onto the call stack.
A recursive tail-call is when the recursive call is the last operation of the calling function, and the result from the recursive call is directly returned from the caller.
In these cases, the caller's stack frame is unneeded as soon as the recursive call is made because when the recursive call returns, the return value is copied to the stack frame below, but nothing more is done.
Therefore, tail-call optimisation removes unnecessary stack frames, reducing the amount of stack memory used.
The new stack frame replaces the caller's stack frame, instead of being pushed on top of it.

This optimisation has a dramatic impact on memory usage; it turns a function's memory usage from $\bigo{n}$ to~$\bigo{1}$ (where $n$ is the recursion depth).
Not only does this make a program much more efficient, but with deep enough recursion, it allows programs to run that would otherwise crash due to lack of memory.

There is a distinction between general tail-calls and recursive tail-calls. In general, a tail-call is any function call that is the last action of a function\footnote{A tail-call necessarily has the same return type as the calling function, because there would otherwise be an explicit type conversion instruction before the \IrCode{return} instruction.}, whereas a recursive tail-call is specifically a call to the same function.
Recursive tail-calls are easier to optimise because the function signature is the same, and they also have the greatest performance impact because many recursive calls may be made.

Initially, I implemented this optimisation by replacing the stack frame in the target code generation stage, however this did not work properly.
Function calls are implemented using WebAssembly functions and the \WasmInstr{call} instruction, which means that the WebAssembly virtual machine has its own call stack as well as the call stack I manage in memory.
At large recursion depths, the WebAssembly stack was still running out of memory, even though mine was not.
Instead, I implemented an approach of transforming the recursion into iteration at the \gls{ir} stage.
This was much more successful.
However, I kept the old optimisation for the cases of non-recursive tail-calls, since it still reduces memory usage there.
I describe both approaches in turn below.

\subsubsection{Approach 1: Replacing the Caller's Stack Frame at Run-time}

My first approach was to replace the caller's stack frame at run-time instead of pushing a new stack frame on top.

Firstly, the middle end finds all \IrCode{call} instructions that are directly returned from the function, and replaces them with a \IrCode{tail-call} instruction.

In the target code generation stage, \IrCode{tail-call} instructions are handled by reusing the current stack frame instead of pushing a new one.
% The construction of this is very similar to how a stack frame is normally pushed.
The frame pointer and space for the return value are left as they are; they do not need to be changed for the new stack frame.
Function parameters are initially copied to a region of unused memory above the stack, then copied to their respective positions in the new stack frame. This ensures the values in the old stack frame are not overwritten by the new stack frame before they are used.

This approach works for both recursive and non-recursive tail-calls.
However, as described above, it is not ideal for recursive tail-calls, because WebAssembly's own function call stack still uses \bigo{n} memory.
Therefore I implemented the second approach below.

\subsubsection{Approach 2: Transforming Recursion to Iteration}

The second approach is entirely contained within the middle end.
It only targets recursive tail-calls; these are where the majority of the performance gains are to be found.
It removes the recursive calls altogether, replacing them with iteration back to the start of the function.
Therefore, no new stack frame will be allocated.

Similarly to the previous approach, first all \IrCode{call} instructions are found that are both recursive and tail-calls.
In the current function, there is a variable for each formal parameter.
Each actual parameter to the recursive call is copied to corresponding formal parameter in the current function.
(A temporary variable is used for each parameter, to avoid overwriting variables that might still be used for other parameters.)
Once done for each parameter, a branch instruction is added to the start of the function.
\Ccref{lst:tail-call optimisation example} shows an example of this transformation.

\begin{listing}[!b]
  \begin{sublisting}[b]{0.49\textwidth}
    \begin{CListing}
      long sum(long n, long acc) {
          if (n == 0) {
              return acc;
          }
          return sum(n - 1, acc + n);
      }
    \end{CListing}
    \caption{Original function code.}
  \end{sublisting}
  \hfill
  \begin{sublisting}[b]{0.49\textwidth}
    \begin{CListing}
      long sum(long n, long acc) {
      start:
          if (n == 0) return acc;
          // Copy recursive arguments to param vars
          long t0 = n - 1;
          long t1 = acc + n;
          n = t0;
          acc = t1;
          // Loop back to start of function
          goto start;
      }
    \end{CListing}
    \caption{Tail-call optimised function.}
  \end{sublisting}
  \caption{Example of transforming tail-recursion to iteration. C code is shown for clarity, however the actual optimisation happens on the intermediate code.}
  \label{lst:tail-call optimisation example}
\end{listing}

This approach solves the problem of the WebAssembly virtual machine's call stack running out of memory, because the function calls have been entirely removed.

\FloatBarrier%

\subsection{Stack Allocation Policy}\label{sec:impl:optimised stack allocation}

My initial variable allocation policy is described in \ccref{sec:impl:local variable allocation}; each variable is allocated a disjoint address range in memory.
However, this is inefficient since most variables are temporary variables that are only used once, and many of them have non-overlapping define-to-use ranges (i.e.\ they never \emph{clash} in the context of liveness analysis).

Variables that never clash can safely be allocated to the same address range.
The challenge was to find a more optimal way of allocating variables to use as little memory as possible.

The optimised allocation is performed with the following steps:

\begin{AlgorithmFloat}[H]
  \begin{Algorithm}
    \begin{EnumerateAlgorithm}
    \item Remove dead variables.
    \item Generate the instruction flowgraph.
    \item Perform \acrlong{lva}.
    \item Generate the clash graph.
    \item Allocate variables from the clash graph.
    \end{EnumerateAlgorithm}
  \end{Algorithm}
  \caption{Optimised variable allocation.}%
  \label[algorithm]{alg:optimised variable allocation}
\end{AlgorithmFloat}

\subsubsection{Live Variable Analysis}

\newcommand{\lvadef}{\ensuremath{\mathit{def}}}
\newcommand{\lvaref}{\ensuremath{\mathit{ref}}}

The instruction flowgraph is similar to the call graph used in the unreachable procedure elimination in \ccref{sec:impl:unreachable procedure elimination}, but at the level of individual instructions.
Each node in the graph is a single instruction, and successors of a node are any instructions that can be executed as the next instruction along some execution path.
For example, branch instructions will have multiple successors.

% The flowgraph is generated by recursing through the program blocks (the output of the Relooper algorithm, \ccref{sec:impl:relooper algorithm}), creating a node for each instruction, and creating edges along all possible control flow paths.

\Gls{lva} is run on the flowgraph to find where each variable is \emph{live}.
A variable is said to be \emph{live} (syntactically\footnote{Compared to \emph{semantic} liveness, which refers to actual possible execution behaviour, but is undecidable at compile-time. Syntactic liveness is a safe overapproximation of semantic liveness.}) at an instruction if the value of the variable is used along any path in the flowgraph before it is redefined.

\Gls{lva} is a backwards analysis, which means that liveness information is propagated backwards through the flowgraph.
Firstly, we define \lvadef\ and \lvaref\ sets for each instruction: $\lvadef\,(i)$ is the set of all variables defined by instruction $n$, and $\lvaref\,(n)$ is the set of all variables referenced by $n$.
\Ccrefeq{eq:live vars} defines $\mathit{live}\,(n)$, the set of variables that are live immediately before instruction $n$:
\begin{align}\label{eq:live vars}
  \mathit{live}\,(n) &= \left( \left( \bigcup_{s \,\in\, \mathit{succ}\,(n)} \mathit{live}\,(s) \right) \setminus \lvadef\,(n) \right) \cup \lvaref\,(n)
  \tag{Equation~3.1}
\end{align}
Starting with all variables live at successors of $n$, all variables that $n$ assigns to are removed, then any variables that $n$ references are added.
When $n$ assigns to a variable, it is no longer live for previous instructions, because any previous value has been overwritten.
When $n$ references a variable, it becomes live for previous instructions, until an instruction assigns to that variable.

\gls{lva} is implemented by iterating over every instruction in the flowgraph, applying \ccrefeq{eq:live vars} to update the set of live variables, until there are no more changes (see \ccref{lst:lva implementation}).
Initially the set of variables live at each instruction is the empty set.
The algorithm is guaranteed to find the smallest set of live variables (it does not add unnecessary overapproximations).

\begin{listing}[!t]
  \begin{RustListing*}{texcomments=true}
    // For every instr, which vars are live at that point
    type LiveVariableMap = HashMap<InstructionId, HashSet<VarId>>;
    fn live_variable_analysis(flowgraph: &Flowgraph) -> LiveVariableMap {
        let mut live: LiveVariableMap = LiveVariableMap::new();
        let mut changes = true;
        while changes {
            changes = false;
            for (instr_id, instr) in &flowgraph.instrs {
                // $\bigcup_{s \,\in\, \mathit{succ}\,(n)} \mathit{live}\,(s)$
                let mut out_live: HashSet<VarId> = HashSet::new();
                for successor in flowgraph.successors.get(instr_id) {
                    out_live.extend(live.get(successor).unwrap_or(&HashSet::new());
                }
                for def_var in def_set(instr) { // ${} \setminus \lvadef\,(n)$
                    out_live.remove(&def_var);
                }
                for ref_var in ref_set(instr) { // ${} \cup \lvaref\,(n)$
                    out_live.insert(ref_var);
                }
                // Update live variable set, and compare to previous value for changes
                let prev_live = live.insert(instr_id, out_live);
                match prev_live {
                    None => {
                        changes = true;
                    }
                    Some(prev_live_vars) => {
                        if prev_live_vars != out_live {
                            changes = true
                        }
                    }
                }
            }
        }
        live // Return the sets of live variables
    }
  \end{RustListing*}
  \caption{\Acrlong{lva} implementation, iteratively applying \ccrefeq{eq:live vars}.}%
  \label{lst:lva implementation}
\end{listing}

One complication of \gls{lva} which had to be solved was that if a variable is assigned to but is never subsequently referenced, it will never be marked as live by the analysis.
The next stage of the optimisation then marks it as having no clashes with any other variables.
However, writing to the variable may occur while other variables are live; and the compiler would happily allocate this variable in an overlapping location, producing incorrect results.
To solve this, I added an optimisation to remove writes to dead variables before running \gls{lva}.
This optimisation also removes the redundancy introduced earlier by the Relooper algorithm, which inserted unnecessary writes to the label variable~(\ccref{sec:prep:relooper}).

I handled side-effecting instructions (e.g.~function calls) that write to an unused variable by creating a `null destination' variable.
The instruction in the \gls{ir} writes to the null variable.
When transforming to target code, no write instruction is generated if the destination variable is the null variable.


\subsubsection{Generating the Clash Graph}\label{sec:impl:generating clash graph}

The clash graph is generated from the results of \gls{lva}.
Any variables that are simultaneously live \emph{clash}; i.e.\ they cannot be allocated to overlapping addresses, because they are in use at the same time.
\Ccref{lst:clash graph generation} describes how the clash graph is generated.

The clash graph is an approximation whenever variable pointers are present.
When a variable has its address taken, the pointer may be passed around the program as a value, so it is no longer possible to track exactly where the variable is accessed.
Therefore, to ensure safety of the optimisation, we make each address-taken variable clash with every other variable\footnote{Pointer analysis is able to reduce the set of clashes for address-taken variables, but cannot solve the problem entirely.}.
I implemented this by storing a set of `universal clashes' in the clash graph; when checking if two variables clash, if one of them is a `universal clash', they clash even if they would otherwise not.

\begin{listing}[t]
  \begin{RustListing}
    let live_vars = live_variable_analysis(flowgraph);
    for (_instr_id, vars_live_at_instr) in live_vars {
        // Add a clash between all vars simultaneously live
        while let Some(var) = vars_live_at_instr.pop() {
            for other_var in vars_live_at_instr {
                clash_graph.add_clash(var, other_var);
            }
        }
    }
    for instr in flowgraph.instrs {
        if let Instruction::AddressOf(_, _, var) = instr {
            clash_graph.add_universal_clash(var); // Over-approximate address-taken variables
        }
    }
  \end{RustListing}
  \caption{Algorithm to generate the clash graph from the results of \acrlong{lva}.}
  \label{lst:clash graph generation}
\end{listing}

\subsubsection{Allocating Variables from the Clash Graph}

In the Part II \emph{Optimising Compilers} course, a register allocation heuristic was described which allocates variables in order of most to least clashes.
When each variable is allocated, it is allocated to a register that maximally overlaps with already allocated variables, while avoiding those containing variables that it clashes with.

My variable allocation problem is similar in many regards, however it has some key differences.
I am allocating variables in memory rather than to registers, so each variable occupies a byte \emph{range} rather than a single location.
Variables can have different sizes, and are not required to be aligned, so it is possible that variables may partially overlap (e.g.~$\texttt{t0} \mapsto [0, 4), \texttt{t1} \mapsto [2, 6)$). % chktex 9
There is also (effectively) no limit to the amount of memory available, compared to the very limited set of registers most compilers target.
The goal of my optimisation is to use as little memory as possible, whereas the goal of register allocation is to most efficiently use the constrained set of registers.

I modified the register allocation method to the following heuristic for variable allocation:
\nopagebreak
\begin{AlgorithmFloat}[H]
  \begin{Algorithm}
    \begin{EnumerateAlgorithm}
    \item\label[algstep]{alg:allocate vars:start} Choose a variable with the least number of clashes. Break ties by choosing smaller variables.
    \item Remove the variable and its edges from the clash graph.
    \item\label[algstep]{alg:allocate vars:allocate} Allocate the variable to the lowest memory address where it does not clash with already allocated variables.
    \item Repeat from \ccref{alg:allocate vars:start} until the clash graph is empty.
    \end{EnumerateAlgorithm}
  \end{Algorithm}
  \caption{Variable allocation heuristic.}%
  \label[algorithm]{alg:optimised variable allocation heuristic}
\end{AlgorithmFloat}

In \ccref{alg:allocate vars:allocate}, variables are always allocated to the lowest possible position on the stack (whilst satisfying clash constraints).
This prioritises overlapping non-clashing variables at lower addresses; only variables that clash often will be pushed to higher addresses.
This keeps the stack size as small as possible.
\Ccref{fig:optimised var allocation example} shows an example of the impact this optimisation has.

\begin{figure}[!b]
  \hspace{2ex}
  \begin{sublisting}[b]{0.5\textwidth}
    \begin{TextListing}
      |\IrCode{t0 = 3}|                 |$\{ \}$|
      |\IrCode{t1 = 2}|                 |$\{ \IrCode{t0} \}$|
      |\IrCode{t2 = t0 + t1}|           |$\{ \IrCode{t0}, \IrCode{t1} \}$|
      |\IrCode{t3 = t2 + t1}|           |$\{ \IrCode{t2}, \IrCode{t1} \}$|
    \end{TextListing}
    \caption{Intermediate code, with \acrlong{lva}.}%
    \label{subfig:optimised var allocation source}
  \end{sublisting}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \scalebox{0.9}{\tikzfig{70-figures/11-clash-graph}}
    \caption{Clash graph.}%
    \label{subfig:optimised var allocation clash graph}
  \end{subfigure}
  \par\vspace{3ex}
  \begin{subfigure}[b]{0.5\textwidth}
    \ctikzfig{70-figures/09-naive-variable-allocation}
    \caption{Memory allocation (naive policy).}%
    \label{subfig:optimised var allocation naive policy}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \ctikzfig{70-figures/10-optimised-variable-allocation}
    \caption{Memory allocation (optimised policy).}%
    \label{subfig:optimised var allocation optimised policy}
  \end{subfigure}
  \caption{
    Example of the optimised stack allocation policy.
    \subref{subfig:optimised var allocation source}~is the intermediate code for which memory is being allocated, showing the set of live variables at each instruction.
    \subref{subfig:optimised var allocation clash graph}~is the clash graph generated after running \acrlong{lva}.
    \subref{subfig:optimised var allocation naive policy}~and~\subref{subfig:optimised var allocation optimised policy} show the allocations made by the naive and optimised policies, respectively.
    Memory addresses increase to the right; variables overlapping vertically, separated by dashed lines, are allocated to the same address.
    Notice how the optimised policy allocates \texttt{t0}, \texttt{t2}, and \texttt{t3} all to the same location, due to the heuristic of always allocating to the lowest valid address.
  }
  \label{fig:optimised var allocation example} % chktex 24
\end{figure}

I tested allocating variables in both orders: least clashes or most clashes first.
I found that contrary to register allocation, allocating variables with least clashes first resulted in more efficient memory use.
This is because of the preference to always allocate to lower addresses; this priority does not exist in the register allocation algorithm, since registers are all equal.
In my heuristic, it is beneficial to have variables with fewer clashes at lower addresses, because this allows more variables to overlap there, and fewer variables will be pushed to higher addresses.


% In the top left is the intermediate code for which memory is being allocated.
% The live variables at each instruction---calculated by \gls{lva}---are shown to the right of each line.
% In the top right is the clash graph, showing which variables cannot be allocated to the same memory.
% The bottom left shows how the naive allocation policy would allocate the variables in memory; sequentially and non-overlapping.
% In the bottom right is the result of the optimised allocation policy: variables \texttt{t0}, \texttt{t2}, and \texttt{t3} all use the same memory location because they are never live at the same time.
% Notice how all three variables use the lowest location, rather than, for example, \texttt{t3} overlapping with \texttt{t1}.
% This is due to the heuristic of always allocating to the lowest valid address.

\FloatBarrier % chktex 1

\vspace{-2ex} % Make the summary fit on the bottom of the page below the table
\section{Repository Overview}
\vspace{-1ex}

I developed my project in a Git repository, ensuring to regularly push to the cloud for backup purposes.
The high-level structure of the codebase is shown in \ccref{tab:repository structure}.
All the code for the compiler is in the \Dirname{src} directory.
The other directories contain the runtime environment code, skeleton standard library implementation, and tests and other tools.
All code was written by myself.
Throughout the project I learnt idiomatic ways of writing and structuring Rust code, to produce an organised and maintainable codebase.

\newlength\IndentWidth\setlength\IndentWidth{1em}
\NewDocumentCommand{\Indent}{m}{\hspace{#1\IndentWidth}}
\definecolor{TableHlineColour}{HTML}{ababab}

% Use `tabularx` when table doesn't span across pages, `xltabular` when it does
\begingroup
% \smallskip
\begin{xltabular}{\textwidth}{lX}
\arrayrulecolor{TableHlineColour}
\specialrule{\heavyrulewidth}{0pt}{0pt}
\Dirname{src} & Compiler source code \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{1}\Dirname{program_config} & Compiler constants and run-time options data structures \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{1}\Dirname{front_end} & Lexer, parser grammar, \gls{ast} data structure \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{1}\Dirname{middle_end} & \gls{ir} data structures, definition of intermediate instructions, and converting \gls{ast} to \gls{ir} \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{2}\Dirname{middle_end_optimiser} & Tail-call optimisation and unreachable procedure elimination \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{1}\Dirname{relooper} & Relooper algorithm \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{1}\Dirname{back_end} & Target code-generation stage \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{2}\Dirname{wasm_module} & Data structures to represent a WebAssembly module \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{2}\Dirname{dataflow_analysis} & Flowgraph generation, dead code analysis, live variable analysis, clash graph \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{2}\Dirname{stack_allocation} & Different stack allocation policies \\\specialrule{\lightrulewidth}{0pt}{0pt}
% \Indent{1}\Dirname{data_structures} & Interval tree implementation that I ended up not using \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{1}\Filename{preprocessor.rs} & C preprocessor \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{1}\Filename{id.rs} & Trait for generating IDs used across the compiler \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{1}\Filename{lib.rs} & Contains the main \texttt{run} function \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Dirname{runtime} & Node.js runtime environment \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Dirname{headers} & Header files for the parts of the standard library I implemented \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Dirname{tools} & Auxiliary scripts used for testing and generating graphs \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{1}\Filename{profiler.py} & Plot stack usage profiles\\\specialrule{\lightrulewidth}{0pt}{0pt}
\Indent{1}\Filename{testsuite.py} & Test runner script \\\specialrule{\lightrulewidth}{0pt}{0pt}
\Dirname{tests} & Automated test specifications \\\specialrule{\heavyrulewidth}{0pt}{0pt}
% Reset the rule colour
\arrayrulecolor{black}
\end{xltabular}
\vspace{-5pt}
\captionof{table}{Repository structure.\protect\label{tab:repository structure}}
\endgroup

\vspace{-5ex} % Make the summary fit on the bottom of the page below the table
\section{Summary}
\vspace{-1ex}

% The compiler pipeline is split into the front, middle, and back ends.
% The front end takes C source code, lexes it to a token stream, and parses it to generate an \gls{ast}.
% The middle end uses a custom \gls{ir} to represent the program, performing the Relooper algorithm as well as doing tail-call optimisation and unreachable procedure elimination.
% The back end transforms the \gls{ir} to a WebAssembly module, along with performing the stack allocation policy optimisation.
% The WebAssembly binary is executed by a Node.js runtime environment, which provides an interface to the system and to my skeleton implementation of the C standard library.

I implemented a complete compiler from C to WebAssembly.
I created my own \gls{ast} to represent the program in the front end, and my own \gls{ir} for the middle end, enabling optimisations.
I successfully implemented the Relooper algorithm to transform the control flow.
I performed tail-call optimisation to significantly reduce memory usage of recursive functions, and I created a novel and more optimal memory allocation algorithm.


\end{document} % chktex 17
