\documentclass[00-main.tex]{subfiles}

\begin{document}

\chapter{Implementation}

\begin{Comment}
%TC:ignore
Word budget: \textasciitilde 4500--5400 words
%TC:endignore
\end{Comment}

\begin{Comment}
%TC:ignore
Describe what was actually produced.

Describe any design strategies that looked ahead to the testing phase, to demonstrate professional approach
%TC:endignore
\end{Comment}
\begin{Comment}
%TC:ignore
Describe high-level structure of codebase.

Say that I wrote it from scratch.

-> mention LALRPOP parser generator used for .lalrpop files
%TC:endignore
\end{Comment}

\section{Repository Overview}

% formatting commands for files/directories in tree
\newcommand{\DTfile}[1]{\texttt{#1}}
\newcommand{\DTdir}[1]{\textbf{\texttt{#1}}/}

I developed my project in a GitHub repository, ensuring to regularly push to the cloud for backup purposes.
This repository is a monorepo containing both my research and documentation along with my source code.

% - horizontal offset of vertical lines to the right
% - width of horizontal lines sticking out
% - separation between horizontal lines and start of text
% - length of horizontal lines
% - size of connecting dots
\DTsetlength{0.2em}{1em}{0.2em}{0.4pt}{2pt}

\makeatletter
\newcommand{\DTdotfill}{\leavevmode \cleaders \hb@xt@ .7em{\hss .\hss }\hfill \kern \z@}
\makeatother
\definecolor{DTdotfillColor}{HTML}{878787}
\renewcommand{\DTcomment}[2][8cm]{%
\textcolor{DTdotfillColor}{\DTdotfill}
\rmfamily
\begin{minipage}[t]{#1}
#2
\end{minipage}
\vspace{0.5ex}
}
% extra space after multi-line comment
\newcommand{\DTcommentMl}[2][8cm]{\DTcomment[#1]{#2}\vspace{0.5ex}}

The high-level structure of the codebase is shown below.
All the code for the compiler is in the \DTdir{src} directory.
The other directories contain the runtime environment code, skeleton standard library implementation, and tests and other tools.

\dirtree{%
.1 .
.2 \DTdir{headers} \DTcomment{Header files for the standard library functions I implemented.}.
.3 \DTfile{stdio.h}.
.3 {...}.
.2 \DTdir{runtime} \DTcomment{NodeJS runtime environment.}.
.3 \DTdir{stdlib} \DTcomment{Implementations of standard library functions in JavaScript.}.
.3 \DTfile{profiler.mjs}.
.3 \DTfile{run.mjs}.
.3 {...}.
.2 \DTdir{src} \DTcomment{The source code for the compiler.}.
.2 \DTdir{tests} \DTcomment{Test specification files.}.
.2 \DTdir{tools}.
.3 \DTfile{profiler.py} \DTcomment{Code to plot stack usage profiles.}.
.3 \DTfile{testsuite.py} \DTcomment{Test runner script.}.
}

The compiler code is mainly split into the front, middle, and back end.
The structure of each stage is described in more detail below.

\dirtree{%
.1 \DTdir{src}.
.2 \DTdir{program\_config}.
.3 \DTfile{program\_constants.rs} \DTcommentMl{Stores global constants, such as the name of imports to the WebAssembly module.}.
.3 \DTfile{enabled\_optimisations.rs} \DTcomment{Data structures to store which optimisations and profiling are enabled, depending on command-line arguments.}.
.3 \DTfile{enabled\_profiling.rs}.
.2 \DTdir{front\_end}.
.2 \DTdir{middle\_end}.
.2 \DTdir{relooper}.
.2 \DTdir{back\_end}.
.2 \DTdir{data\_structures}.
.3 \DTfile{interval\_tree.rs} \DTcommentMl{Interval tree data structure, that turned out to be unsuitable for the task.}.
.2 \DTfile{main.rs} \DTcomment{Entry-point to the program. Calls \RustInline{lib::run()}}.
.2 \DTfile{lib.rs} \DTcommentMl{Top-level \RustInline{run()} function. Defines command-line arguments, and runs each stage of the compiler pipeline.}.
.2 \DTfile{id.rs} \DTcommentMl{Trait for generating IDs in the middle and back ends.}.
.2 \DTfile{fmt\_indented.rs} \DTcomment{Trait for printing data structures with indents.}.
.2 \DTfile{preprocessor.rs} \DTcomment{Preprocessor stage of the compiler.}.
}

The front end contains the lexer and the parser.

\dirtree{%
.1 \DTdir{front\_end}.
.2 \DTfile{lexer.rs}.
.2 \DTfile{c\_parser.lalrpop} \DTcomment{Input grammar to the parser generator.}.
.2 \DTfile{parser.rs} \DTcomment{Wrapper around the parser generator.}.
.2 \DTfile{ast.rs} \DTcomment{Abstract Syntax Tree data structure.}.
.2 \DTfile{interpret\_string.rs} \DTcomment{Function to handle escape characters when parsing string literals.}.
}

The middle end contains all the code to transform the abstract syntax tree to the intermediate representation.
This includes the intermediate representation instructions and program data structure.
The code for the optimisations to the intermediate representation is also here.

\dirtree{%
.1 \DTdir{middle\_end}.
.2 \DTdir{middle\_end\_optimiser}.
.3 \DTfile{ir\_optimiser.rs} \DTcommentMl{Runs the optimisations on the intermediate representation.}.
.3 \DTfile{remove\_redundancy.rs} \DTcommentMl{Function to remove unused labels from the instructions.}.
.3 \DTfile{tail\_call\_optimise.rs} \DTcomment{Tail-call optimisation implementation.}.
.3 \DTfile{unreachable\_procedure\_elimination.rs} \DTcommentMl[6.5cm]{Unreachable procedure elimination implementation.}.
.2 \DTfile{ast\_to\_ir.rs} \DTcommentMl{Code to transform the abstract syntax tree to intermediate code.}.
.2 \DTfile{ir.rs} \DTcomment{Intermediate representation data structure.}.
.2 \DTfile{instructions.rs} \DTcomment{Definition of three-address instructions.}.
.2 \DTfile{ir\_types.rs} \DTcommentMl{Data structures to store type information of variables and functions.}.
.2 \DTfile{ids.rs} \DTcommentMl{Types for IDs across the middle end, such as variable IDs.}.
.2 \DTfile{context.rs} \DTcommentMl{Data structure for sharing contextual information throughout the middle end transformation.}.
.2 \DTfile{compile\_time\_eval.rs} \DTcommentMl{Function to evaluate expressions at compile time.}.
.2 \DTfile{type\_conversions.rs} \DTcommentMl{Functions to generate instructions to convert between types.}.
.2 {...}.
}

The code for the Relooper algorithm is self-contained in its own module.

\dirtree{%
.1 \DTdir{relooper}.
.2 \DTfile{relooper.rs} \DTcomment{The main Relooper algorithm implementation.}.
.2 \DTfile{soupify.rs} \DTcomment{Turns a list of instructions into a `soup of blocks'}.
.2 \DTfile{blocks.rs} \DTcomment{Data structures for Relooper blocks.}.
}

The back end contains data structures representing a WebAssembly module, the code to transform the intermediate representation to target code, and optimisations that apply to the target code.

\dirtree{%
.1 \DTdir{back\_end}.
.2 \DTdir{dataflow\_analysis}.
.3 \DTfile{clash\_graph.rs} \DTcommentMl{Clash graph data structure, and the function to generate it.}.
.3 \DTfile{dead\_code\_analysis.rs} \DTcomment{Removes dead variables from the code.}.
.3 \DTfile{flowgraph.rs} \DTcommentMl{Flow graph data structure, and the functions to generate it.}.
.3 \DTfile{instruction\_def\_ref.rs} \DTcommentMl{Functions to return the $\mathit{def}(n)$ and $\mathit{ref}(n)$ sets for an instruction (used in Live Variable Analysis).}.
.3 \DTfile{live\_variable\_analysis.rs} \DTcomment{Live Variable Analysis implementation.}.
.2 \DTdir{stack\_allocation} \DTcommentMl{Contains the different stack allocation policies.}.
.3 \DTfile{allocate\_vars.rs}.
.3 \DTfile{naive\_allocation.rs} \DTcommentMl{Naive implementation of stack allocation policy.}.
.3 \DTfile{optimised\_allocation.rs} \DTcommentMl{Optimised implementation of stack allocation policy.}.
.3 {...}.
.2 \DTdir{wasm\_module} \DTcommentMl{Contains data structures representing Web\-Assembly modules.}.
.3 \DTfile{module.rs}.
.3 \DTfile{code\_section.rs}.
.3 {...}.
.2 \DTfile{target\_code\_generation.rs} \DTcomment{Code to transform the intermediate representation to target code.}.
.2 \DTfile{wasm\_instructions.rs} \DTcomment{Definition of target code instructions.}.
.2 \DTfile{wasm\_types.rs} \DTcomment{Definition of WebAssembly types.}.
.2 \DTfile{to\_bytes.rs} \DTcomment{Trait to get the byte representation from a data structure.}.
.2 \DTfile{integer\_encoding.rs} \DTcomment{Function to encode integer literals using LEB128 encoding.}.
.2 \DTfile{vector\_encoding.rs} \DTcomment{Function to encode a vector of items.}.
.2 \DTfile{wasm\_indices.rs} \DTcomment{Types for indices used across the back end.}.
.2 \DTfile{initialise\_memory.rs} \DTcomment{Function to initialise the contents of memory.}.
.2 \DTfile{memory\_operations.rs} \DTcomment{Functions to generate instructions to load from and store to memory.}.
.2 \DTfile{stack\_frame\_operations.rs} \DTcomment{Functions to push and pop stack frames.}.
.2 \DTfile{profiler.rs} \DTcomment{Inserts profiling code to the program.}.
.2 {...}.
}

\begin{Comment}
Finish this. Will have to see if it'll be better to have comments on the right of dirs, or to highlight the main structure below
\end{Comment}

\section{System Architecture}

\Ccref{fig:project flowchart} describes the high-level structure of the project. The \textcolor{frontendcolor}{front end}, \textcolor{middleendcolor}{middle end}, and \textcolor{backendcolor}{back end} are denoted by colour.

\begin{figure}[H]
  \centering
  \tikzfig{70-figures/01-overview-flowchart}
  \caption{Project structure, highlighting the \textcolor{frontendcolor}{front end}, \textcolor{middleendcolor}{middle end}, and \textcolor{backendcolor}{back end}.}
  \label{fig:project flowchart}
\end{figure}

Each solid box represents a module of the project, transforming the input data representation into the output representation.
The data representations are shown as dashed boxes.

I created my own Abstract Syntax Tree (AST) representation and Intermediate Representation (IR), which are used as the main data representations in the compiler.

% The optimisations I implemented in the middle end were unreachable procedure elimination, and tail-call optimisation.
% As an extension to the project, I implemented a more optimal stack allocation policy, as part of the target code generation module.

\section{Front End}

% \begin{figure}[H]
%   \centering
%   \tikzfig{70-figures/02-front-end-overview-flowchart}
%   \caption{Structure of the compiler front end.}
%   \label{fig:front end flowchart}
% \end{figure}

The front end of the compiler consists of the lexer and the parser; it takes C source code as input and outputs an abstract syntax tree.
I wrote a custom lexer, because this is necessary to support \CInline{typedef} definitions in C\@.
I used a parser generator to convert the tokens emitted by the lexer into an AST\@.

\subsection{Preprocessor}

I used the GNU C preprocessor (\mintinline{bash}{cpp}) \ccite{gnu-c-preprocessor} to handle any preprocessor directives in the source code, for example macro definitions.
However, since I am not supporting linking, I removed any \CInline{#include} directives before running the preprocessor, and handled them myself.

For each \CInline{#include <name.h>} directive that is removed, if it is one of the standard library headers that I support, the appropriate library code is copied into the source code from \Filename{headers/<name>.h}.
One exception is when the name of the header file matches the name of the source program, in which case the contents of the program's header file are inserted to the source code, rather than finding a matching library.

After processing \CInline{#include} directives, the compiler spawns \mintinline{bash}{cpp} as a child process, writes the source code to its stdin, and reads the processed source code back from its stdout.

\subsection{Lexer}

The grammar of the C language is mostly context-free, however the presence of \CInline{typedef} definitions make it context-sensitive \ccite[Section 5.10.3]{c-reference-manual}.
For example, the statement in \ccref{lst:typedef name ambiguity example} can be interpreted in two ways:
\begin{itemize}
\item As a variable declaration, if \CInline{foo} has previously been defined as a type name\footnote{The brackets will be ignored.}; or
\item As a function call, if \CInline{foo} is the name of a function.
\end{itemize}

\begin{listing}[H]
  \begin{CListing}
  foo (bar);
  \end{CListing}
  \caption{An example of \CInline{typedef} name ambiguity in C.}
  \label{lst:typedef name ambiguity example}
\end{listing}

This ambiguity is impossible to resolve with the language grammar. The solution is to preprocess the \CInline{typedef} names during the lexing stage, and emit distinct type name and identifier tokens to the parser.
Therefore, I implemented a custom lexer that is aware of the type names that have already been defined the current point in the program.

The lexer is implemented as a finite state machine. \Ccref{fig:lexing numbers fsm,fig:lexing identifiers fsm} highlight portions of the machine; the remaining state transition diagrams can be found in \ccref{app:lexer fsm}.
The diagrams show the input character, as a regular expression, along each transition arrow. (Note: in a slight abuse of regular expression notation, the dot character `\texttt{.}' represents a literal full stop character, and the backslash character `\texttt{\char`\\}' represents a literal backslash.)
It is assumed that when no state transition is shown for a particular input, the end of the current token has been reached.
Transition arrows without a prior state are the initial transitions for the first input character.
Node labels represent the token that will be emitted when we finish in that state.

The finite state machine consumes the source code one character at a time, until the end of the token is reached (i.e.\ there is no transition for the next input character).
Then, the token corresponding to the current state is emitted to the parser.
Some states have no corresponding token to emit, because they occur part-way through parsing a token; if the machine finishes in one of these states, this raises a lex error.
(In other words, every state labelled with a token is an accepting state of the machine.)
For tokens such as number literals and identifiers, the lexer appends the input character to a string buffer on each transition, and when the token is complete, the string is stored inside the emitted token.
This gives the parser access to the necessary data about the token, for example the name of the literal.

If, when starting to lex a new token, there is no initial transition corresponding to the input character, then there is no valid token for this input. This raises a lex error, and the compiler will exit.

\Ccref{fig:lexing numbers fsm} shows the finite state machine for lexing number literals. This handles all the different forms of number that C supports: decimal, binary, octal, hexadecimal, and floating point.
(Note: the states leading to the ellipsis token are shown for completeness, even though the token is not a number literal, since they share the starting dot state.)

\begin{figure}[ht]
  \centering
  \tikzfig{70-figures/10-lexer-fsm/00-numbers}
  \caption{Finite state machine for lexing number literals.}
  \label{fig:lexing numbers fsm}
\end{figure}

\Ccref{fig:lexing identifiers fsm} shows the finite state machine for lexing identifiers and \CInline{typedef} names.
This is where we handle the ambiguity introduced into the language.
  Every time we consume another character of an identifier, we check whether the current name (which we have stored in the string buffer) matches either a keyword of the language of a \CInline{typedef} name we have encountered this far.
(Keywords are given a higher priority of matching.)
  If a match is found, we move to the corresponding state, represented by the $\epsilon$ transitions (since no input is consumed along these transitions).
When we reach the end of the token, the three states will emit the corresponding token, either an identifier, keyword, or \CInline{typedef} name token respectively.
When we emit a \CInline{typedef} name token, the lexer pushes it to an array of all the type names that have been declared this far in the program, so we can match future identifiers against it.

\begin{figure}[ht]
  \centering
  \tikzfig{70-figures/10-lexer-fsm/01-identifiers}
  \caption{Finite state machine for lexing identifiers.}
  \label{fig:lexing identifiers fsm}
\end{figure}


\subsection{Parser}

\begin{Comment}
%TC:ignore
Talk about my \texttt{interpret\_string} implementation, to handle string escaping. Implemented using an iterator.

- created AST representation

Talk about structure of my AST

Talk about how I parsed type specifiers into a standard type representation. Used a bitfield to parse arithmetic types, cos they can be declared in any order.
%TC:endignore
\end{Comment}

I used the LALRPOP parser generator \ccite{lalrpop-docs} to generate parsing code from the input grammar I wrote.
The Microsoft's C Language Syntax Summary \ccite{c-language-grammar-microsoft} and C: A Reference Manual \ccite{c-reference-manual} were very useful references to ensure I captured the subtleties of C's syntax when writing my grammar.
My grammar is able to parse all of the core features of the C language, omitting some of the recent language additions. I chose to make my parser handle a larger subset of C than the compiler as a whole supports; the middle end rejects or ignores nodes of the AST that it doesn't handle. For example, the parser handles storage class specifiers (e.g.\ \CInline{static}) and type qualifiers (e.g.\ \CInline{const}), and the middle end simply ignores them.

A naive grammar for C (\ccref{lst:ambiguous if-else grammar}) contains an ambiguity around \CInline{if}/\CInline{else} statements \ccite[Section 8.5.2]{c-reference-manual}.
C permits the bodies of conditional statements to be written without curly brackets if the body is a single statement.
If we have nested \CInline{if}/\CInline{else} statements that don't use curly brackets, if can be ambiguous which \CInline{if} an \CInline{else} belongs to. This is known as the dangling else problem.

\begin{listing}[!ht]
  \begin{GrammarListing}
    if-stmt      ::= "if" "(" expr ")" stmt
    if-else-stmt ::= "if" "(" expr ")" stmt "else" stmt
  \end{GrammarListing}
  \caption{Ambigious \CInline{if}/\CInline{else} grammar.}
  \label{lst:ambiguous if-else grammar}
\end{listing}

An example of the dangling else problem is shown in \ccref{lst:dangling else}.
According to the grammar in \ccref{lst:ambiguous if-else grammar}, there are two possible parses of this program.
Either the \CInline{else} belongs to the inner or the outer \CInline{if} (\ccref{lst:dangling else possible parsings}).

\begin{listing}[!ht]
  \begin{CListing}
    if (x)
      if (y)
        stmt1;
    else
      stmt2;
  \end{CListing}
  \caption{Example of the dangling else problem.}
  \label{lst:dangling else}
\end{listing}

\begin{listing}[!ht]
  \begin{subfigure}[t]{0.49\textwidth}
    \begin{CListing}
      if (x) {
        if (y) {
          stmt1;
        } else {
          stmt2;
        }
      }
    \end{CListing}
    \caption{\CInline{else} belongs to inner \CInline{if}.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\textwidth}
    \begin{CListing}
      if (x) {
        if (y) {
          stmt1;
        }
      } else {
        stmt2;
      }
    \end{CListing}
    \caption{\CInline{else} belongs to outer \CInline{if}.}
  \end{subfigure}
  \caption{Possible parsings of \ccref{lst:dangling else}.}
  \label{lst:dangling else possible parsings}
\end{listing}

C resolves the ambiguity by always associating the \CInline{else} with the closest possible \CInline{if}.
We can encode this into the grammar with the concept of `open' and `closed' statements \ccite{final-solution-to-dangling-else}.
\Ccref{lst:open/closed statement grammar} shows how we introduce this into our grammar for \CInline{if}/\CInline{else} statements.
All other forms of statement must also be converted to the new structure.
Any basic statements, i.e.\ statements that have no sub-statements, are added to \GrammarInline{closed-stmt}.
All other statements that have sub-statements, such as \CInline{while}, \CInline{for}, and \CInline{switch} statements, must be duplicated to both \GrammarInline{open-stmt} and \GrammarInline{closed-stmt}.


\begin{listing}[!ht]
  \begin{GrammarListing}
    stmt        ::= open-stmt | closed-stmt

    open-stmt   ::= "if" "(" expr ")" stmt
                  | "if" "(" expr ")" closed-stmt "else" open-stmt
                  | ...

    closed-stmt ::= "if" "(" expr ")" closed-stmt "else" closed-stmt
                  | ...
  \end{GrammarListing}
  \caption{Using open and closed statements to solve the dangling else problem}
  \label{lst:open/closed statement grammar}
\end{listing}

A closed statement always has the same number of \CInline{if} and \CInline{else} keywords (excluding anything between a pair of brackets, because bracket matching is unambiguous).
Thus, in the second alternative of an \GrammarInline{open-stmt}, the \CInline{else} terminal can be found by counting the number of \CInline{if}s and \CInline{else}s we encounter since the start of the \GrammarInline{closed-stmt}; the \CInline{else} belonging to the \GrammarInline{open-stmt} is the first \CInline{else} once we have more \CInline{else}s than \CInline{if}s.

If we allowed open statements inside the \CInline{if} clause of an \GrammarInline{open-stmt}, then \GrammarInline{open-stmt} and \GrammarInline{closed-stmt} would no longer be disjoint, and the grammar would be ambiguous. This is because we wouldn't be able to use the above method for finding the \CInline{else} that belongs to the outer \GrammarInline{open-stmt}.

I chose the LALRPOP parser generator because it builds up the AST as it parses the grammar.
This is in contrast to some of the other available libraries, which separate the grammar code and the code that generates the AST\@.
LALRPOP provides an intuitive and powerful approach.
Each grammar rule contains both the grammar specification and code to generate the corresponding AST node.

\Ccref{lst:AST generation code example} is an example of the LALRPOP syntax for addition expressions.
The left-hand side of the~\RustInline{=>} describes the grammar rule, and the right-hand side is the code to generate an \RustInline{Expression} node.
Terminals are represented in double quotes; these are defined to map to the tokens emitted by the lexer.
Non-terminals are represented inside angle brackets, with their type and a variable name to use in the AST generation code.

\begin{listing}[!ht]
  \begin{subfigure}[t]{\textwidth}
    \begin{GrammarListing}
      additive-expression ::= additive-expression "+" multiplicative-expression
    \end{GrammarListing}
    \caption{The grammar rule for addition expressions.}
  \end{subfigure}
  \par\medskip % vertical space between subfigures
  \begin{subfigure}[t]{\textwidth}
    \begin{RustListing}
      AdditiveExpression: ast::Expression = {
          <e1:AdditiveExpression> "+" <e2:MultiplicativeExpression>
              => ast::Expression::BinaryOp(
                  ast::BinaryOperator::Add,
                  Box::new(e1),
                  Box::new(e2)
              ),
          ...
      };
    \end{RustListing}
    \caption{The LALRPOP syntax for the addition grammar rule.}
  \end{subfigure}
  \caption{In LALRPOP, the AST generation and grammar code are combined.}
  \label{lst:AST generation code example}
\end{listing}

LALRPOP also allows macros to be defined, which allow the grammar to be written in a more intuitive way.
For example, I defined a macro to represent a comma-separated list of non-terminals (\ccref{lst:parser macro for comma-separated list}).
The macro has a generic type \RustInline{T}, and automatically collects the list items into a \RustInline{Vec<T>}, which can be used by the rules that use the macro.

\begin{listing}[!ht]
  \begin{RustListing}
    CommaSepList<T>: Vec<T> = {
        <mut v:(<T> ",")*> <e:T> => {
            v.push(e);
            v
        }
    };
  \end{RustListing}
  \caption{LALRPOP macro to parse a comma-separated list of non-terminals.}
  \label{lst:parser macro for comma-separated list}
\end{listing}

\section{Middle End}

\begin{Comment}
%TC:ignore
Give an overview of the middle end
%TC:endignore
\end{Comment}

\subsection{Intermediate Code Generation}

\begin{Comment}
%TC:ignore
- Defined my own three-address code representation

- for every ast node, defined transformation to 3AC instructions

- created IR data structure to hold instructions + all necessary metadata

- Talk about auto-incrementing IDs - abstraction of the Id trait and generic IdGenerator struct

- handled type information - created data structure to represent possible types

- making sure instructions are type-safe, type converting where necessary - talk about unary/binary conversions, cite the C reference book

- Compile-time evaluation of expressions, eg. for array sizes

- Talk about the Context design pattern I used throughout -- maybe research this and see if it's been done before?
%TC:endignore
\end{Comment}

I defined a custom three-address code intermediate representation (IR).
The IR contains both the program instructions and necessary metadata, such as variable type information.
The instructions and metadata are contained in separate sub-structs within the main IR struct, which enables the metadata to be carried forwards through stages of the compiler pipeline while the instructions are transformed at each stage.
In the stage of converting the AST to IR code, the instructions and metadata are encapsulated in the \RustInline{Program} struct.

Many objects in the IR require unique IDs, such as variables and labels.
I created a \RustInline{Id} trait to abstract this concept, together with a generic \RustInline{IdGenerator} struct (\ccref{lst:Id and IdGenerator implementation}).
The ID generator internally tracks the highest ID that has been generated so far, so that the IR can create IDs as necessary without needing to know anything about their implementation.
IDs are generated inductively: each ID knows how to generate the next one.

\begin{listing}[!ht]
  \begin{RustListing}
    pub trait Id {
        fn initial_id() -> Self;
        fn next_id(&self) -> Self;
    }

    pub struct IdGenerator<T: Id + Clone> {
        max_id: Option<T>,
    }

    impl<T: Id + Clone> IdGenerator<T> {
        pub fn new() -> Self {
            IdGenerator { max_id: None }
        }

        pub fn new_id(&mut self) -> T {
            let new_id = match &self.max_id {
                None => T::initial_id(),
                Some(id) => id.next_id(),
            };
            self.max_id = Some(new_id.to_owned());
            new_id
        }
    }
  \end{RustListing}
  \caption{Implementation of the \RustInline{Id} trait and \RustInline{IdGenerator}.}
  \label{lst:Id and IdGenerator implementation}
\end{listing}

Throughout the middle and back ends, I used a design pattern of passing a context object through all the function calls.
For example, when traversing the AST to generate IR code, the \RustInline{Context} struct in \ccref{lst:AST to IR context struct} is used to track information about the current context we are in with respect to the source program.
For example, it tracks the stack of nested loops and switch statements, so that when we convert a \CInline{break} or \CInline{continue} statement, we know where to branch to.

\begin{listing}[!ht]
  \begin{RustListing}
    pub struct Context {
        loop_stack: Vec<LoopOrSwitchContext>,
        scope_stack: Vec<Scope>,
        pub in_function_name_expr: bool,
        function_names: HashMap<String, FunId>,
        pub directly_on_lhs_of_assignment: bool,
    }
  \end{RustListing}
  \caption{The context datatype used when converting the AST to IR code.}
  \label{lst:AST to IR context struct}
\end{listing}

In an object-oriented language, this would be achieved by encapsulating the methods in an object and using private state inside the object.
Rust, however, is not object oriented, and I believe this approach also offers more modularity and flexibility.
Firstly, the context information itself is encapsulated inside its own data structure, allowing methods to be implemented on it that gives calling functions access to exactly the context information they need.
Also, it allows separation of the different functions in the middle end, rather than constraining them to all needing to be within one class.

To convert the AST to IR code, the compiler recursively traverses the tree, generating three-address code instructions and metadata as it does so.
At the highest level, the AST contains a list of statements.
For each of these, the compiler calls \RustInline{convert_statement_to_ir(stmt, program, context)}.
\RustInline{program} is the mutable intermediate representation, to which instructions and metadata are added as the AST is traversed.
\RustInline{context} is the context object described above, which makes gets passes through the functions recursively so the compiler always has access to relevant contextual information.

The core of converting statements and expressions to IR code is matching the type of AST node, and generating IR instructions according to the structure of the statement, recursing into sub-statements and -expressions.
The case for a \CInline{while} statement is shown in \ccref{lst:convert while stmt to IR pseudocode}; the labels and branches to execute a while loop are added, and the instructions to evaluate the condition and body are generated recursively.
Other control-flow structures are similar.

\begin{listing}[!ht]
  \begin{minted}[escapeinside=||]{text}
    fn |\textbf{convert\_statement\_to\_ir}|(stmt, program, context) {
        instrs = []
        match stmt {
            |\textbf{While}|(condition, body) => {
                Create new labels for start and end of loop
                Push new loop context to |\textbf{Context}| object
                instrs += start of loop label
                instrs += |\textbf{convert\_expression\_to\_ir}|(condition, program, context)
                instrs += |\textbf{BranchIf}|(condition false, branch to end of loop)
                instrs += |\textbf{convert\_statement\_to\_ir}|(body, program, context)
                instrs += |\textbf{Branch}|(start of loop label)
                instrs += end of loop label
                Pop loop context from |\textbf{Context}| object
            }
            ... other AST statement nodes ...
        }
        return instrs
    }
  \end{minted}
  \caption{Pseudocode for the \RustInline{convert_statement_to_ir()} function.}
  \label{lst:convert while stmt to IR pseudocode}
\end{listing}

\begin{Comment}
TODO talk about the more complex cases, eg. switch statements, variable declarations, function declarations
\end{Comment}

\subsection{The Relooper Algorithm}

\begin{Comment}
%TC:ignore
cite Emscripten \cite{emscripten}
%TC:endignore
\end{Comment}

\section{Back End: Target Code Generation}


\section{Runtime Environment}

\begin{Comment}
%TC:ignore
- Instantiating wasm module

- stdlib functions skeleton implementation

- arg passing + memory initialisation
%TC:endignore
\end{Comment}

\section{Optimisations}

\subsection{Unreachable Procedure Elimination}

\subsection{Tail-Call Optimisation}

\begin{Comment}
%TC:ignore
Defn of tail-call optimisation

Why do the optimisation
%TC:endignore
\end{Comment}

\section{Summary}

\end{document}
