\documentclass[00-main.tex]{subfiles}

\begin{document}

\chapter{Implementation}

\begin{Comment}
%TC:ignore
Word budget: \textasciitilde 4500--5400 words
%TC:endignore
\end{Comment}

\begin{Comment}
%TC:ignore
Describe what was actually produced.

Describe any design strategies that looked ahead to the testing phase, to demonstrate professional approach
%TC:endignore
\end{Comment}
\begin{Comment}
%TC:ignore
Describe high-level structure of codebase.

Say that I wrote it from scratch.

-> mention LALRPOP parser generator used for .lalrpop files
%TC:endignore
\end{Comment}

\section{Repository Overview}

% formatting commands for files/directories in tree
\newcommand{\DTfile}[1]{#1}
\newcommand{\DTdir}[1]{\textbf{#1}/}

\begin{Comment}
%TC:ignore
Don't put github repo url -- it's got my name in it
%TC:endignore
\end{Comment}

I developed my project in a GitHub repository\footnote{\url{https://github.com/martin-walls/cam-part-ii-c-webassembly-compiler}}, ensuring to regularly push to the cloud for backup purposes.
This repository is a monorepo containing both my research and documentation along with my source code.

% - horizontal offset of vertical lines to the right
% - width of horizontal lines sticking out
% - separation between horizontal lines and start of text
% - length of horizontal lines
% - size of connecting dots
\DTsetlength{0.2em}{1em}{0.2em}{0.4pt}{2pt}

\makeatletter
\newcommand \Dotfill {\leavevmode \cleaders \hb@xt@ .7em{\hss .\hss }\hfill \kern \z@}
\makeatother
\renewcommand{\DTcomment}[1]{%
\Dotfill
\rmfamily
\begin{minipage}[t]{8cm}
#1
\end{minipage}
}

\dirtree{%
.1 .
.2 \DTdir{headers} \DTcomment{Header files for the standard library functions I implemented}.
.3 \DTfile{stdio.h}.
.3 {...}.
.2 \DTdir{runtime} \DTcomment{NodeJS runtime environment}.
.3 \DTdir{stdlib} \DTcomment{Implementations of standard library functions in JS}.
.3 \DTfile{run.mjs}.
.3 {...}.
.2 \DTdir{src} \DTcomment{The source code for the compiler, explained below}.
.3 {...}.
.2 \DTdir{tests} \DTcomment{Test specification files}.
.3 {...}.
.2 \DTdir{tools}.
.3 \DTfile{profiler.py} \DTcomment{Code to plot stack usage profiles}.
.3 \DTfile{testsuite.py} \DTcomment{Test runner}.
}

\dirtree{%
.1 \DTdir{src}.
.2 \DTdir{back\_end}.
.2 \DTdir{data\_structures}.
.2 \DTdir{front\_end}.
.2 \DTdir{middle\_end}.
.2 \DTdir{program\_config}.
.2 \DTdir{relooper}.
.2 \DTfile{fmt\_indented.rs}.
.2 \DTfile{id.rs}.
.2 \DTfile{lib.rs}.
.2 \DTfile{main.rs}.
.2 \DTfile{preprocessor.rs}.
}

\begin{Comment}
%TC:ignore
Finish this. Will have to see if it'll be better to have comments on the right of dirs, or to highlight the main structure below
%TC:endignore
\end{Comment}

\section{System Architecture}

\Ccref{fig:project flowchart} describes the high-level structure of the project. The \textcolor{frontendcolor}{front end}, \textcolor{middleendcolor}{middle end}, and \textcolor{backendcolor}{back end} are denoted by colour.

\begin{figure}[H]
  \centering
  \tikzfig{70-figures/01-overview-flowchart}
  \caption{Project structure, highlighting the \textcolor{frontendcolor}{front end}, \textcolor{middleendcolor}{middle end}, and \textcolor{backendcolor}{back end}.}
  \label{fig:project flowchart}
\end{figure}

Each solid box represents a module of the project, transforming the input data representation into the output representation.
The data representations are shown as dashed boxes.

I created my own Abstract Syntax Tree (AST) representation and Intermediate Representation (IR), which are used as the main data representations in the compiler.

% The optimisations I implemented in the middle end were unreachable procedure elimination, and tail-call optimisation.
% As an extension to the project, I implemented a more optimal stack allocation policy, as part of the target code generation module.

\section{Front End}

% \begin{figure}[H]
%   \centering
%   \tikzfig{70-figures/02-front-end-overview-flowchart}
%   \caption{Structure of the compiler front end.}
%   \label{fig:front end flowchart}
% \end{figure}

The front end of the compiler consists of the lexer and the parser; it takes C source code as input and outputs an abstract syntax tree.
I wrote a custom lexer, because this is necessary to support \CInline{typedef} definitions in C\@.
I used a parser generator to convert the tokens emitted by the lexer into an AST\@.

\subsection{Preprocessor}

I used the GNU C preprocessor (\mintinline{bash}{cpp}) \ccite{gnu-c-preprocessor} to handle any preprocessor directives in the source code, for example macro definitions.
However, since I am not supporting linking, I removed any \CInline{#include} directives before running the preprocessor, and handled them myself.

For each \CInline{#include <name.h>} directive that is removed, if it is one of the standard library headers that I support, the appropriate library code is copied into the source code from \Filename{headers/<name>.h}.
One exception is when the name of the header file matches the name of the source program, in which case the contents of the program's header file are inserted to the source code, rather than finding a matching library.

After processing \CInline{#include} directives, the compiler spawns \mintinline{bash}{cpp} as a child process, writes the source code to its stdin, and reads the processed source code back from its stdout.

\subsection{Lexer}

The grammar of the C language is mostly context-free, however the presence of \CInline{typedef} definitions make it context-sensitive \ccite[Section 5.10.3]{c-reference-manual}.
For example, the statement in \ccref{lst:typedef name ambiguity example} can be interpreted in two ways:
\begin{itemize}
\item As a variable declaration, if \CInline{foo} has previously been defined as a type name\footnote{The brackets will be ignored.}; or
\item As a function call, if \CInline{foo} is the name of a function.
\end{itemize}

\begin{listing}[H]
  \begin{CListing}
  foo (bar);
  \end{CListing}
  \caption{An example of \CInline{typedef} name ambiguity in C.}
  \label{lst:typedef name ambiguity example}
\end{listing}

This ambiguity is impossible to resolve with the language grammar. The solution is to preprocess the \CInline{typedef} names during the lexing stage, and emit distinct type name and identifier tokens to the parser.
Therefore, I implemented a custom lexer that is aware of the type names that have already been defined the current point in the program.

The lexer is implemented as a finite state machine. \Ccref{fig:lexing numbers fsm,fig:lexing identifiers fsm} highlight portions of the machine; the remaining state transition diagrams can be found in \ccref{app:lexer fsm}.
The diagrams show the input character, as a regular expression, along each transition arrow. (Note: in a slight abuse of regular expression notation, the dot character `\texttt{.}' represents a literal full stop character, and the backslash character `\texttt{\char`\\}' represents a literal backslash.)
It is assumed that when no state transition is shown for a particular input, the end of the current token has been reached.
Transition arrows without a prior state are the initial transitions for the first input character.
Node labels represent the token that will be emitted when we finish in that state.

The finite state machine consumes the source code one character at a time, until the end of the token is reached (i.e.\ there is no transition for the next input character).
Then, the token corresponding to the current state is emitted to the parser.
Some states have no corresponding token to emit, because they occur part-way through parsing a token; if the machine finishes in one of these states, this raises a lex error.
(In other words, every state labelled with a token is an accepting state of the machine.)
For tokens such as number literals and identifiers, the lexer appends the input character to a string buffer on each transition, and when the token is complete, the string is stored inside the emitted token.
This gives the parser access to the necessary data about the token, for example the name of the literal.

If, when starting to lex a new token, there is no initial transition corresponding to the input character, then there is no valid token for this input. This raises a lex error, and the compiler will exit.

\Ccref{fig:lexing numbers fsm} shows the finite state machine for lexing number literals. This handles all the different forms of number that C supports: decimal, binary, octal, hexadecimal, and floating point.
(Note: the states leading to the ellipsis token are shown for completeness, even though the token is not a number literal, since they share the starting dot state.)

\begin{figure}[ht]
  \centering
  \tikzfig{70-figures/10-lexer-fsm/00-numbers}
  \caption{Finite state machine for lexing number literals.}
  \label{fig:lexing numbers fsm}
\end{figure}

\Ccref{fig:lexing identifiers fsm} shows the finite state machine for lexing identifiers and \CInline{typedef} names.
This is where we handle the ambiguity introduced into the language.
  Every time we consume another character of an identifier, we check whether the current name (which we have stored in the string buffer) matches either a keyword of the language of a \CInline{typedef} name we have encountered this far.
(Keywords are given a higher priority of matching.)
  If a match is found, we move to the corresponding state, represented by the $\epsilon$ transitions (since no input is consumed along these transitions).
When we reach the end of the token, the three states will emit the corresponding token, either an identifier, keyword, or \CInline{typedef} name token respectively.
When we emit a \CInline{typedef} name token, the lexer pushes it to an array of all the type names that have been declared this far in the program, so we can match future identifiers against it.

\begin{figure}[ht]
  \centering
  \tikzfig{70-figures/10-lexer-fsm/01-identifiers}
  \caption{Finite state machine for lexing identifiers.}
  \label{fig:lexing identifiers fsm}
\end{figure}


\subsection{Parser}

\begin{Comment}
%TC:ignore
Talk about my \texttt{interpret\_string} implementation, to handle string escaping. Implemented using an iterator.

- created AST representation

Talk about structure of my AST

Talk about how I parsed type specifiers into a standard type representation. Used a bitfield to parse arithmetic types, cos they can be declared in any order.
%TC:endignore
\end{Comment}

I used the LALRPOP parser generator \ccite{lalrpop-docs} to generate parsing code from the input grammar I wrote.
The Microsoft's C Language Syntax Summary \ccite{c-language-grammar-microsoft} and C: A Reference Manual \ccite{c-reference-manual} were very useful references to ensure I captured the subtleties of C's syntax when writing my grammar.
My grammar is able to parse all of the core features of the C language, omitting some of the recent language additions. I chose to make my parser handle a larger subset of C than the compiler as a whole supports; the middle end rejects or ignores nodes of the AST that it doesn't handle. For example, the parser handles storage class specifiers (e.g.\ \CInline{static}) and type qualifiers (e.g.\ \CInline{const}), and the middle end simply ignores them.

A naive grammar for C (\ccref{lst:ambiguous if-else grammar}) contains an ambiguity around \CInline{if}/\CInline{else} statements \ccite[Section 8.5.2]{c-reference-manual}.
C permits the bodies of conditional statements to be written without curly brackets if the body is a single statement.
If we have nested \CInline{if}/\CInline{else} statements that don't use curly brackets, if can be ambiguous which \CInline{if} an \CInline{else} belongs to. This is known as the dangling else problem.

\begin{listing}[!ht]
  \begin{GrammarListing}
    if-stmt      ::= "if" "(" expr ")" stmt
    if-else-stmt ::= "if" "(" expr ")" stmt "else" stmt
  \end{GrammarListing}
  \caption{Ambigious \CInline{if}/\CInline{else} grammar.}
  \label{lst:ambiguous if-else grammar}
\end{listing}

An example of the dangling else problem is shown in \ccref{lst:dangling else}.
According to the grammar in \ccref{lst:ambiguous if-else grammar}, there are two possible parses of this program.
Either the \CInline{else} belongs to the inner or the outer \CInline{if} (\ccref{lst:dangling else possible parsings}).

\begin{listing}[!ht]
  \begin{CListing}
    if (x)
      if (y)
        stmt1;
    else
      stmt2;
  \end{CListing}
  \caption{Example of the dangling else problem.}
  \label{lst:dangling else}
\end{listing}

\begin{listing}[!ht]
  \begin{subfigure}[t]{0.49\textwidth}
    \begin{CListing}
      if (x) {
        if (y) {
          stmt1;
        } else {
          stmt2;
        }
      }
    \end{CListing}
    \caption{\CInline{else} belongs to inner \CInline{if}.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\textwidth}
    \begin{CListing}
      if (x) {
        if (y) {
          stmt1;
        }
      } else {
        stmt2;
      }
    \end{CListing}
    \caption{\CInline{else} belongs to outer \CInline{if}.}
  \end{subfigure}
  \caption{Possible parsings of \ccref{lst:dangling else}.}
  \label{lst:dangling else possible parsings}
\end{listing}

C resolves the ambiguity by always associating the \CInline{else} with the closest possible \CInline{if}.
We can encode this into the grammar with the concept of `open' and `closed' statements \ccite{final-solution-to-dangling-else}.
\Ccref{lst:open/closed statement grammar} shows how we introduce this into our grammar for \CInline{if}/\CInline{else} statements.
All other forms of statement must also be converted to the new structure.
Any basic statements, i.e.\ statements that have no sub-statements, are added to \GrammarInline{closed-stmt}.
All other statements that have sub-statements, such as \CInline{while}, \CInline{for}, and \CInline{switch} statements, must be duplicated to both \GrammarInline{open-stmt} and \GrammarInline{closed-stmt}.


\begin{listing}[!ht]
  \begin{GrammarListing}
    stmt        ::= open-stmt | closed-stmt

    open-stmt   ::= "if" "(" expr ")" stmt
                  | "if" "(" expr ")" closed-stmt "else" open-stmt
                  | ...

    closed-stmt ::= "if" "(" expr ")" closed-stmt "else" closed-stmt
                  | ...
  \end{GrammarListing}
  \caption{Using open and closed statements to solve the dangling else problem}
  \label{lst:open/closed statement grammar}
\end{listing}

A closed statement always has the same number of \CInline{if} and \CInline{else} keywords (excluding anything between a pair of brackets, because bracket matching is unambiguous).
Thus, in the second alternative of an \GrammarInline{open-stmt}, the \CInline{else} terminal can be found by counting the number of \CInline{if}s and \CInline{else}s we encounter since the start of the \GrammarInline{closed-stmt}; the \CInline{else} belonging to the \GrammarInline{open-stmt} is the first \CInline{else} once we have more \CInline{else}s than \CInline{if}s.

If we allowed open statements inside the \CInline{if} clause of an \GrammarInline{open-stmt}, then \GrammarInline{open-stmt} and \GrammarInline{closed-stmt} would no longer be disjoint, and the grammar would be ambiguous. This is because we wouldn't be able to use the above method for finding the \CInline{else} that belongs to the outer \GrammarInline{open-stmt}.

I chose the LALRPOP parser generator because it builds up the AST as it parses the grammar.
This is in contrast to some of the other available libraries, which separate the grammar code and the code that generates the AST\@.
LALRPOP provides an intuitive and powerful approach.
Each grammar rule contains both the grammar specification and code to generate the corresponding AST node.

\Ccref{lst:AST generation code example} is an example of the LALRPOP syntax for addition expressions.
The left-hand side of the~\RustInline{=>} describes the grammar rule, and the right-hand side is the code to generate an \RustInline{Expression} node.
Terminals are represented in double quotes; these are defined to map to the tokens emitted by the lexer.
Non-terminals are represented inside angle brackets, with their type and a variable name to use in the AST generation code.

\begin{listing}[!ht]
  \begin{subfigure}[t]{\textwidth}
    \begin{GrammarListing}
      additive-expression ::= additive-expression "+" multiplicative-expression
    \end{GrammarListing}
    \caption{The grammar rule for addition expressions.}
  \end{subfigure}
  \par\medskip % vertical space between subfigures
  \begin{subfigure}[t]{\textwidth}
    \begin{RustListing}
      AdditiveExpression: ast::Expression = {
          <e1:AdditiveExpression> "+" <e2:MultiplicativeExpression>
              => ast::Expression::BinaryOp(
                  ast::BinaryOperator::Add,
                  Box::new(e1),
                  Box::new(e2)
              ),
          ...
      };
    \end{RustListing}
    \caption{The LALRPOP syntax for the addition grammar rule.}
  \end{subfigure}
  \caption{In LALRPOP, the AST generation and grammar code are combined.}
  \label{lst:AST generation code example}
\end{listing}

LALRPOP also allows macros to be defined, which allow the grammar to be written in a more intuitive way.
For example, I defined a macro to represent a comma-separated list of non-terminals (\ccref{lst:parser macro for comma-separated list}).
The macro has a generic type \RustInline{T}, and automatically collects the list items into a \RustInline{Vec<T>}, which can be used by the rules that use the macro.

\begin{listing}[!ht]
  \begin{RustListing}
    CommaSepList<T>: Vec<T> = {
        <mut v:(<T> ",")*> <e:T> => {
            v.push(e);
            v
        }
    };
  \end{RustListing}
  \caption{LALRPOP macro to parse a comma-separated list of non-terminals.}
  \label{lst:parser macro for comma-separated list}
\end{listing}

\section{Middle End}

\begin{Comment}
%TC:ignore
Give an overview of the middle end
%TC:endignore
\end{Comment}

\subsection{Intermediate Code Generation}

\begin{Comment}
%TC:ignore
- Defined my own three-address code representation

- for every ast node, defined transformation to 3AC instructions

- created IR data structure to hold instructions + all necessary metadata

- Talk about auto-incrementing IDs - abstraction of the Id trait and generic IdGenerator struct

- handled type information - created data structure to represent possible types

- making sure instructions are type-safe, type converting where necessary - talk about unary/binary conversions, cite the C reference book

- Compile-time evaluation of expressions, eg. for array sizes

- Talk about the Context design pattern I used throughout -- maybe research this and see if it's been done before?
%TC:endignore
\end{Comment}

I defined a custom three-address code intermediate representation (IR).
The IR contains both the program instructions and necessary metadata, such as variable type information.
The instructions and metadata are contained in separate sub-structs within the main IR struct, which enables the metadata to be carried forwards through stages of the compiler pipeline while the instructions are transformed at each stage.

Many objects in the IR require unique IDs, such as variables and labels.
I created a \RustInline{Id} trait to abstract this concept, together with a generic \RustInline{IdGenerator} struct (\ccref{lst:Id and IdGenerator implementation}).
The ID generator internally tracks the highest ID that has been generated so far, so that the IR can create IDs as necessary without needing to know anything about their implementation.
IDs are generated inductively: each ID knows how to generate the next one.

\begin{listing}[!ht]
  \begin{RustListing}
    pub trait Id {
        fn initial_id() -> Self;
        fn next_id(&self) -> Self;
    }

    pub struct IdGenerator<T: Id + Clone> {
        max_id: Option<T>,
    }

    impl<T: Id + Clone> IdGenerator<T> {
        pub fn new() -> Self {
            IdGenerator { max_id: None }
        }

        pub fn new_id(&mut self) -> T {
            let new_id = match &self.max_id {
                None => T::initial_id(),
                Some(id) => id.next_id(),
            };
            self.max_id = Some(new_id.to_owned());
            new_id
        }
    }
  \end{RustListing}
  \caption{Implementation of the \RustInline{Id} trait and \RustInline{IdGenerator}.}
  \label{lst:Id and IdGenerator implementation}
\end{listing}

Throughout the middle and back ends, I used the `context' design pattern.

\subsection{The Relooper Algorithm}

\begin{Comment}
%TC:ignore
cite Emscripten \cite{emscripten}
%TC:endignore
\end{Comment}

\section{Back End: Target Code Generation}


\section{Runtime Environment}

\begin{Comment}
%TC:ignore
- Instantiating wasm module

- stdlib functions skeleton implementation

- arg passing + memory initialisation
%TC:endignore
\end{Comment}

\section{Optimisations}

\subsection{Unreachable Procedure Elimination}

\subsection{Tail-Call Optimisation}

\begin{Comment}
%TC:ignore
Defn of tail-call optimisation

Why do the optimisation
%TC:endignore
\end{Comment}

\section{Summary}

\end{document}
