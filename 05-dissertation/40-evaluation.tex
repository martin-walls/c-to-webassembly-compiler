\documentclass[00-main.tex]{subfiles}

\begin{document}

\chapter{Evaluation}

\newcommand{\IncludeStackPlot}[1]{\resizebox{0.98\textwidth}{!}{\input{71-plots/#1}}}

In this chapter, I evaluate my project against my success criteria (\ccref{sec:eval:success criteria}), showing that all the success criteria were achieved.
\ccref{sec:eval:correctness testing} demonstrates that the compiler is correct using a variety of test programs.
\ccref{sec:eval:optimisations} evaluates the impact of the optimisations I implemented, particularly showing significant improvements in memory usage.


\section{Success Criteria}\label{sec:eval:success criteria}

The success criteria for my project, as defined in my project proposal, are:
\begin{itemize}[nosep, itemsep=2pt]
\item The program generates an abstract syntax tree from C source code.
\item The program transforms the abstract syntax tree into an intermediate representation.
\item The program uses the Relooper algorithm to transform unstructured to structured control
flow.
\item The program generates WebAssembly binary code from the intermediate representation.
\item The compiler generates binary code that produces the same output as the source program.
\end{itemize}

All the success criteria have been met.
The first four correspond to the main compiler pipeline.
The correctness of these stages is verified by the fifth criterion.
\ccref{sec:eval:correctness testing} describes the variety of test programs used to verify the compiler's correctness.


\section{Correctness Testing}\label{sec:eval:correctness testing}

I wrote a suite of test programs in C to evaluate the correctness and performance of my compiler.
There were two types of program:
\begin{itemize}[nosep, itemsep=2pt]
\item 18 `unit test' programs, each testing a specific construct in the C language.
\item 11 `full programs', representing real workloads.
\end{itemize}

Programs of the first type are not strictly unit tests by the standard definition.
Unit tests verify the functionality of individual units of source code, in isolation from the rest of the application.
Each test should test one specific behaviour, and should be independent from the rest of the program's functionality~\ccite{unit-testing}.
My test programs do not test an isolated behaviour of the compiler.
Instead, they test a single behaviour of the C source code that is being compiled (e.g.~pointer dereferencing), verifying that the compiler pipeline maintains the correct behaviour.
This allowed me to find and fix bugs more easily.

Examples of `full programs' include Conway's Game of Life, calculating the Fibonacci numbers, and finding substring occurrences in a string~\ccite{conways-game-of-life,fibonacci-numbers}.
Six of the programs were sourced from Clib, which contains many utility packages for C. Those that I used had no external dependencies, and were useful in verifying that my compiler worked for other people's code as well as my own. Clib is licensed under the MIT license, permitting use ``without restriction''~\ccite{clib}.
The remaining five full programs and all 18 unit test programs were written by myself.

For all my tests, I used GCC\footnote{GCC version 11.3.1.} as the reference for correctness~\ccite{gcc}.
I deemed a program to be compiled correctly if it produced the same output as when compiled with GCC\@.
To facilitate this, I made liberal use of \texttt{printf}, to output the results of computations.

I wrote a test runner script to ensure that I maintained correctness as I continued to develop and extend the compiler.
This script reads a directory of YAML files, each specifying a test program and its arguments~\ccite{yaml}.
It then compiles the programs with both my compiler and GCC, and compares the outputs.
If the outputs are identical, the test passes, otherwise it fails.

I also implemented some convenience features into the test script.
The command-line interface takes an optional `filter' argument, which runs a subset of the tests whose name matches the filter.
The script can also be used to run test programs without comparing to GCC, printing to the standard output for manual testing.

To prevent bugs from accidentally being introduced into my compiler, I set up the test suite to run automatically as a pre-commit hook whenever I committed changes to the Git repository.
This would block the commit if any of the tests failed, alerting me and allowing me to make corrections first.
This ensured the version of my project in source control was always functioning correctly.


\section{Impacts of Optimisations}\label{sec:eval:optimisations}

In the following sections, I evaluate the impacts of my optimisations.
For unreachable procedure elimination, I evaluate the reduction in code size.
For tail-call optimisation and stack allocation optimisation, I evaluate the effectiveness at reducing memory usage.

To evaluate the memory usage optimisations, the compiler inserts profiling code to the programs, which measures the size of the stack throughout program execution.
When generating instructions that move the \acrlong{sp}, the compiler inserts a call to \JSInline{log_stack_ptr}, a function imported from the runtime that appends the current \acrlong{sp} value to a log file.
I wrote a Python script to visualise the resulting data.
The plots show how the stack grows and shrinks throughout the execution of the program.
\Ccref{fig:tail-call optimisation stack use,fig:comparing stack usage optimisation across programs,fig:comparing stack usage for case.c,fig:comparing stack usage for fibonacci.c} are generated using this method.

\subsection{Unreachable Procedure Elimination}\label{sec:eval:unreachable procedure elimination}

In the context of this project, unreachable procedure elimination is mainly beneficial in removing unused standard library functions from the compiled binary.
When a standard library header is included in a program, the preprocessor inserts the entire code of that library\footnote{That is, the entirety of my skeleton implementation of that library.}.
If the program only uses a few of the functions, the rest are redundant.
Unreachable procedure elimination safely removes them, resulting in a smaller binary.

% I only implemented enough of the standard library to allow my test programs to run, so the impact of this optimisation is limited by the number of functions imported.
% If I were to implement more of the standard library, this optimisation would become more important.

The standard library with the most functions that I implemented was \Filename{ctype.h}.
This contains 13 functions, of which a program might normally use two or three.
This is where I saw the biggest improvement from the optimisation.
Programs that used the \CInline{ctype} library saw an average file-size reduction of \SI{4.7}{\kilo\byte}.

The other libraries I implemented contained only a few functions, so the impact of this optimisation was much more limited.
If I implemented more of the standard library, I would see much more improvement.

Due to this difference in the standard library files, and also that source programs can contain arbitrary functions that are never used, it is not meaningful to calculate aggregate metrics across my test programs.
However, testing programs individually verifies that any unused functions are removed from the compiled binary.

\subsection{Tail-Call Optimisation}\label{sec:eval:tail-call optimisation}

My implementation of tail-call optimisation\index{Tail-call optimisation} was successful in reusing the existing function stack frame for tail-recursive calls.
Recursion is converted into iteration within the function, eliminating the need for new stack frame allocations.
Therefore, the stack memory usage remains constant rather than growing linearly with the number of recursive calls (\ccref{fig:tail-call optimisation stack use}).

One of the functions I used to evaluate this optimisation uses tail-recursion to compute the sum of the first $n$ integers~(\ccref{lst:tail-recursive sum}).

\begin{listing}[t]
  \CInputListing[firstline=4, lastline=9]{../02-test-programs/15-tail-call-optimisation/02-tail-call-sum.c}
  \caption{Tail-recursive function to sum the integers 1 to $n$.\medskip}
  \label{lst:tail-recursive sum}
\end{listing}


\Ccref{fig:tail-call optimisation stack use} compares the stack memory usage of this function with and without tail-call optimisation.
Without the optimisation, the stack size grows with \bigo{n}, whereas with the optimisation it is \bigo{1}.
When running the program with $n=500$, a stack size of \SI{46.3}{\kilo\byte} is reached.
When compiled with tail-call optimisation enabled, only 298 bytes of stack space are used; a \SI{99.36}{\percent} reduction in memory usage.
Of course, the reduction depends on how large $n$ is.

\begin{figure}[t]
  \centering
  \IncludeStackPlot{22-tailcall-sum-compare-tailcallopt-without-stackopt.pgf}
  \caption{Stack usage for calling \CInline{sum(500, 0)} (see \ccref{lst:tail-recursive sum})}
  \label{fig:tail-call optimisation stack use} % chktex 24
\end{figure}

When testing with large $n$, the non-optimised version quickly runs out of memory, and throws an exception.
In contrast, the optimised version has no memory constraint on the number of iterations.
It successfully runs with $n = \num{1000000}$, at which point it fails with GCC\@.


\subsection{Optimised Stack Allocation Policy}\label{sec:eval:optimised stack allocation}

My stack allocation policy was successful in reducing the amount of stack memory used.

From the test programs, the largest reduction in memory use was \SI{69.73}{\percent} compared to the unoptimised program.
The average improvement was \SI{50.28}{\percent}.
% with a \SI{95}{\percent}~confidence interval of $[41.37\%, 59.19\%]$.

\Ccref{fig:comparing stack usage optimisation across programs} shows the impact that this optimisation had on the different test programs.
The full-height bars represent the stack usage of the unoptimised program, against which the optimised program was measured.
The darker bars show the stack usage of the optimised program as a percentage of the original stack usage.
Shorter bars represent a greater improvement (less memory is being used).

For programs that benefit from tail-call optimisation, I measured the effect of the optimisation on both the optimised and unoptimised versions.
I did this because tail-call optimisation also affects memory usage, so it may have an impact on the effectiveness of this optimisation.

\begin{figure}[t]
  \centering
  \adjustbox{clip, trim=0cm 1.2cm 0cm 0.3cm}{\IncludeStackPlot{40-stack-use-across-programs.pgf}}
  \caption{Comparing optimised against unoptimised stack memory usage to usage. The shorter the optimised bar, the greater the improvement (less memory is used). \emph{TC opt}: tail-call optimised. \emph{Blinker} and \emph{pulsar} are specific instances of the Game of Life program.}%
  \label{fig:comparing stack usage optimisation across programs}
\end{figure}

One of the main factors influencing the degree of improvement is the number of temporary variables generated.
The more temporary variables generated, the more scope for the compiler to find non-clashing variables to overlap.
Since temporary variables are generated locally for each instruction, they only have short-range dependencies.
Only variables that correspond to user variables have longer-range dependencies.
Therefore, the temporary variables offer the compiler more options of independent variables.

The result of this is that as a function increases its number of operations and hence the number of temporary variables increases, so too does the scope for optimisation that the compiler is able to exploit.

We can see the effect of this directly when we compare the stack usage of the unoptimised and optimised versions of the same program.
\Ccref{fig:comparing stack usage for case.c} shows the size of the stack over the execution of a program that converts strings to upper, lower, or camel case.
Since the main stack allocations and deallocations occur on function calls and returns respectively, each spike on the plot corresponds to a function call.
Hence we can interpret which parts of the plot correspond to which parts of the source program.

\begin{figure}[t]
  \centering
  \IncludeStackPlot{01-case-compare.pgf}
  \caption{Comparing stack usage for \Filename{case.c}.}
  \label{fig:comparing stack usage for case.c} % chktex 24
\end{figure}

The program in turn calls \CInline{case_upper()}, \CInline{case_lower()}, and \CInline{case_camel()}, corresponding to the three distinct sections of the plot.

\CInline{case_upper()} and \CInline{case_lower()} each make repeated calls to \CInline{toupper()} and \CInline{tolower()}, which correspond to the many short spikes on the plot (one for each character in the string).
Other than a \CInline{for} loop, they do not contain many operations, and therefore not many temporary variables are generated.

In contrast, \CInline{case_camel()} performs many more operations iteratively in the body of the function.
\Ccref{lst:section of casecamel body code} shows an extract of its body code.
Even in this short section, more temporary variables are created than in the entire body of \CInline{case_upper()} (\ccref{lst:caseupper body code}).
This results in the larger spike at the end of \ccref{fig:comparing stack usage for case.c}.

\begin{listing}[t]
  \begin{sublisting}[b]{0.49\textwidth}
    \CInputListing[firstline=18, lastline=21]{../02-test-programs/12-single-source-programs/case.c}
    \caption{The entire body of \CInline{case_upper()}.}
    \label{lst:caseupper body code}
  \end{sublisting}
  \hfill
  \begin{sublisting}[b]{0.49\textwidth}
    \CInputListing[firstline=39, lastline=43]{../02-test-programs/12-single-source-programs/case.c}
    \caption{A short section of the body of \CInline{case_camel()}.}
    \label{lst:section of casecamel body code}
  \end{sublisting}
  \caption{TODO write this caption: Comparing case_upper's code against case_camel's}
  \label{lst:caseupper and casecamel body code}
\end{listing}

Due to the differences in temporary variables described above, the compiler is able to optimise \CInline{case_camel()} much more than the other functions.
This parallels the fact that \CInline{case_camel()} had the largest stack frame initially.


Another area in which this optimisation has a large impact is recursive functions.
Since this optimisation reduces the size of each stack frame, there is a large improvement when there are many recursive stack frames.
\Ccref{fig:comparing stack usage for fibonacci.c} shows the size of the stack over the execution of calculating the Fibonacci numbers recursively.
In this instance, the optimised stack allocation policy reduced the stack size of the program by \SI{67.20}{\percent}.


\begin{figure}[t]
  \centering
  \IncludeStackPlot{02-fibonacci-compare.pgf}
  \caption{Comparing stack usage for \Filename{fibonacci.c}.}
  \label{fig:comparing stack usage for fibonacci.c} % chktex 24
\end{figure}



\section{Summary}

The main objective of this project was to produce a compiler that generated correct WebAssembly binary code.
Through the range of testing described above, I have shown that this objective was achieved, with the definition of correctness being that the generated program behaves in the same way as when compiled with GCC\@.

The objective of adding optimisations is to improve the performance of the compiled programs, while maintaining the semantic meaning of the program.
The correctness of my optimisations was verified with the same test suite as was used to test the unoptimised compiler's correctness.
In the previous sections, we have seen measurable evidence that the optimisations did improve performance.
Therefore, the optimisations can be considered successful.


\end{document}
